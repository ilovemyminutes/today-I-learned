{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import transformers\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from networks.Attention import *\n",
    "from dataset import dataset_loader\n",
    "from flags import Flags\n",
    "from utils import get_network\n",
    "\n",
    "def get_train_transforms(height, width):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height, width),\n",
    "            A.Compose([A.HorizontalFlip(p=1), A.VerticalFlip(p=1)], p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms(height, width):\n",
    "    return A.Compose([A.Resize(height, width), ToTensorV2(p=1.0)])\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/Attention-jupyter.yaml\"\n",
    "options = Flags(CONFIG_PATH).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "(\n",
    "    train_data_loader,\n",
    "    validation_data_loader,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    ") = dataset_loader(\n",
    "        options=options,\n",
    "        train_transform=get_train_transforms(\n",
    "            options.input_size.height, options.input_size.width\n",
    "        ),\n",
    "        valid_transform=get_valid_transforms(\n",
    "            options.input_size.height, options.input_size.width\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = options.data.rgb\n",
    "num_classes=len(train_dataset.id_to_token)\n",
    "src_dim = options.Attention.src_dim\n",
    "embedding_dim = options.Attention.embedding_dim\n",
    "hidden_dim = options.Attention.hidden_dim\n",
    "pad_id = train_dataset.token_to_id[PAD]\n",
    "st_id = train_dataset.token_to_id[START]\n",
    "num_layers = options.Attention.layer_num\n",
    "cell_type = options.Attention.cell_type\n",
    "\n",
    "# 인코더\n",
    "encoder = CNN(nc=nc)\n",
    "\n",
    "# 디코더 - AttentionDecoder\n",
    "embedding = nn.Embedding(num_classes + 1, embedding_dim)\n",
    "attention_cell = AttentionCell(\n",
    "            src_dim, hidden_dim, embedding_dim, num_layers, cell_type\n",
    "        )\n",
    "hidden_dim = hidden_dim\n",
    "num_classes = num_classes # 토큰 수\n",
    "num_layers = num_layers # 레이어 수 - AttentionCell 내 RNN 레이어 수를 설정\n",
    "generator = nn.Linear(hidden_dim, num_classes) # hidden_state를 확률화\n",
    "pad_id = pad_id # 패드 토큰 ID\n",
    "st_id = st_id # 시작 토큰 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample batch for implementation beam-search\n",
    "for batch in train_data_loader: break\n",
    "input = batch['image'].float()\n",
    "curr_batch_size = len(input)\n",
    "expected = batch['truth']['encoded']\n",
    "expected[expected == -1] = train_data_loader.dataset.token_to_id[PAD]"
   ]
  },
  {
   "source": [
    "- Attention Input for forwarding\n",
    "    - `input`(torch.Tensor): 전처리와 collate_fn을 거쳐 생성된 이미지 텐서\n",
    "    - `expected`(torch.Tensor): Ground Truth 수식 텍스트\n",
    "    - `is_train`(bool)\n",
    "    - `teacher_forcing`(flaot)\n",
    "\n",
    "- Encoder Input\n",
    "    - input: 전처리와 collate_fn을 거쳐 생성된 이미지 텐서\n",
    "\n",
    "- Decoder Input\n",
    "    - `src`: 인코더로부터 얻은 feature map. 모델 내부에서 다음을 거친 `out`을 받음\n",
    "        ```python\n",
    "        out = self.encoder(input)\n",
    "        b, c, h, w = out.size()\n",
    "        out = out.view(b, c, h * w).transpose(1, 2)  # [b, h x w, c]\n",
    "        ```\n",
    "    - `text`: 생성할 수식의 Ground Truth. 모델 내부에서 `expected`로 받음\n",
    "    - `is_train`:\n",
    "    - `teacher_forcing`: \n",
    "    - `batch_max_length`: 생성할 수식의 최대 길이. 모델 내부에서 `expected.size(1)`로 받음"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 인코더 output 추출\n",
    "encoder_output = encoder(input) # (B, C, H, W) <- conv&pooling 결과\n",
    "b, c, h, w = encoder_output.size()\n",
    "out = encoder_output.view(b, c, h * w).transpose(1, 2)  # [b, h x w, c]\n",
    "\n",
    "src = out # 디코더 input\n",
    "batch_max_length = expected.size(1) # 디코더 최대 생성 길이\n",
    "text = expected # (B, MAX_LEN)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "source": [
    "# AttentionDecoder forwarding 과정\n",
    "batch_size = src.size(0)\n",
    "num_steps = batch_max_length - 1 # 총 생성 횟수\n",
    "\n",
    "output_hiddens = (\n",
    "            torch.FloatTensor(batch_size, num_steps, hidden_dim)\n",
    "            .fill_(0)\n",
    "            .to('cpu')\n",
    "        )\n",
    "print('Output hidden vector size:', output_hiddens.size())\n",
    "\n",
    "# hidden state 초기값 텐서 선언 - (배치사이즈, 임베딩사이즈(=hidden_dim))\n",
    "hidden = (\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0).to(device), # hidden state\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0).to(device) # cell state\n",
    ") "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output hidden vector size: torch.Size([13, 52, 128])\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Beam Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = expected.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder input: out(이미지 feature map)-[B, WxH, C], expected(GT 토큰화 결과)-[B, MAX_LEN]\n",
    "\n",
    "# for single layer\n",
    "hidden = (\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0).to(device),\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0).to(device)\n",
    ")\n"
   ]
  },
  {
   "source": [
    "여기는 문장 단위로 일어나네"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, max_len = expected.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글자 단위\n",
    "# for idx in range(max_len):\n",
    "\n",
    "# 문장 단위\n",
    "# for idx in range(batch_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "topk = 1\n",
    "decoded_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 231, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "src.size() # [B, WxH, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, prevnode, wordid, logprob, length):\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = prevnode\n",
    "        self.wordid = wordid\n",
    "        self.logp = logprob\n",
    "        self.len = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward <- ?\n",
    "\n",
    "        return self.logp / float(self.len - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "source": [
    "## Greedy Decoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(BATCH_SIZE, MAX_LEN) of Text(ground truth) torch.Size([13, 34])\nNUMBER OF STEPS 33\n"
     ]
    }
   ],
   "source": [
    "print('(BATCH_SIZE, MAX_LEN) of Text(ground truth)', text.size()) # (BATCH_SIZE, MAX_LEN)\n",
    "print('NUMBER OF STEPS', num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = src.size(0)\n",
    "num_steps = batch_max_length - 1  # 최대 스텝(=최대 생성 횟수)\n",
    "\n",
    "output_hiddens = (\n",
    "    torch.FloatTensor(batch_size, num_steps, hidden_dim)\n",
    "    .fill_(0)\n",
    "    .to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = embedding(text[:, 0]) # 0번 인덱스에 해당되는 글자\n",
    "hidden, alpha = attention_cell(hidden, src, embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "hidden[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    # one-hot vectors for a i-th char. in a batch\n",
    "    embedd = embedding(text[:, i])\n",
    "    # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
    "    hidden, alpha = attention_cell(hidden, src, embedd)\n",
    "    if num_layers == 1:\n",
    "        output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
    "    else:\n",
    "        output_hiddens[:, i, :] = hidden[-1][0]\n",
    "probs = generator(output_hiddens)"
   ]
  },
  {
   "source": [
    "- AttentionCell forwarding\n",
    "    - `prev_hidden`(torch.Tensor): a"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) torch.Size([13])\ntorch.Size([13, 128])\n"
     ]
    }
   ],
   "source": [
    "# T: 토큰 개수(=단어 개수)\n",
    "# E: 임베딩 사이즈\n",
    "\n",
    "i = 0\n",
    "print(text[:, i], text[:, i].size())\n",
    "print(embedding(text[:, i]).size()) # [T, E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 34])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "text.size()"
   ]
  },
  {
   "source": [
    "## Beam Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = out.size(0)\n",
    "num_steps = expected.size(1)\n",
    "\n",
    "output_hiddens = (\n",
    "            torch.FloatTensor(batch_size, num_steps, hidden_dim)\n",
    "            .fill_(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "hidden = (\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0), # hidden\n",
    "    torch.FloatTensor(batch_size, hidden_dim).fill_(0), # cell\n",
    "    )\n",
    "hidden[0].size() # [B, HIDDEN]"
   ]
  },
  {
   "source": [
    "# no teacher forcing\n",
    "\n",
    "targets = torch.LongTensor(batch_size).fill_(st_id)\n",
    "print(targets.size()) # [B] - 각 샘플에 대한 시작토큰\n",
    "\n",
    "probs = torch.FloatTensor(batch_size, num_steps, num_classes).fill_(0)\n",
    "probs.size() # [B, MAX_LEN, VOCAB_SIZE] - 스텝별 생성 확률이 기재될 확률 테이블"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([13])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 48, 245])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([13, 128])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.1009,  0.2129, -0.0321,  ...,  0.3426, -0.2417,  0.3173],\n",
       "        [ 0.1296,  0.3131, -0.0300,  ..., -0.1156, -0.0877, -0.0591],\n",
       "        [-0.1152,  0.2518, -0.0503,  ...,  0.3893, -0.1282,  0.2493],\n",
       "        ...,\n",
       "        [-0.2687,  0.2140, -0.0347,  ...,  0.3321, -0.1570,  0.3576],\n",
       "        [-0.1026,  0.2222, -0.0024,  ...,  0.4579, -0.2338,  0.3322],\n",
       "        [-0.0978,  0.1827, -0.0255,  ...,  0.4020, -0.2308,  0.2959]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "embedded = embedding(targets)\n",
    "print(embedded.size()) # [B, HIDDEN] - 시작 토큰 벡터의 임베딩 결과\n",
    "\n",
    "hidden, alpha = attention_cell(hidden, out, embedded)\n",
    "hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "class BeamSearchNode:\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "topk = 1\n",
    "decoded_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = out.size(0)\n",
    "num_steps = expected.size(1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 52, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# hidden 만들어놓고\n",
    "output_hiddens = (\n",
    "    torch.FloatTensor(batch_size, num_steps, hidden_dim)\n",
    "    .fill_(0)\n",
    ")\n",
    "output_hiddens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# layers = 1\n"
     ]
    }
   ],
   "source": [
    "# initialize hidden state for LSTM\n",
    "if num_layers == 1:\n",
    "    print('# layers = 1')\n",
    "    hidden = (\n",
    "        torch.FloatTensor(batch_size, hidden_dim)\n",
    "        .fill_(0)\n",
    "        .to(device),  # hidden\n",
    "        torch.FloatTensor(batch_size, hidden_dim)\n",
    "        .fill_(0)\n",
    "        .to(device),  # cell\n",
    "    )\n",
    "else:\n",
    "    print('# layers > 1')\n",
    "    hidden = [\n",
    "        (\n",
    "            torch.FloatTensor(batch_size, hidden_dim)\n",
    "            .fill_(0)\n",
    "            .to(device),  # hidden\n",
    "            torch.FloatTensor(batch_size, hidden_dim)\n",
    "            .fill_(0)\n",
    "            .to(device),  # cell\n",
    "        )\n",
    "        for _ in range(self.num_layers)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(batch_size): break\n",
    "# sample = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "hidden[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(hidden, tuple):  # LSTM case\n",
    "    decoder_hidden = (\n",
    "        hidden[0][i, :].unsqueeze(0), hidden[1][i, :].unsqueeze(0))\n",
    "else:\n",
    "    decoder_hidden = hidden[i, :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_data_loader: break\n",
    "input = batch['image'].float() # [B, C, H, W]\n",
    "encoder_output = encoder(input) # [B, C, H, W]\n",
    "b, c, h, w = encoder_output.size()\n",
    "src = encoder_output.view(b, c, h * w).transpose(1, 2)  # [b, h x w, c]\n",
    "\n",
    "batch_size = len(input)\n",
    "expected = batch['truth']['encoded']\n",
    "expected[expected == -1] = train_data_loader.dataset.token_to_id[PAD]\n",
    "text = expected # [B, MAX_LEN]"
   ]
  },
  {
   "source": [
    "Forwarding in Decoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden/cell state for LSTM\n",
    "hidden = (\n",
    "    torch.FloatTensor(batch_size, hidden_dim)\n",
    "    .fill_(0), # hidden\n",
    "    torch.FloatTensor(batch_size, hidden_dim)\n",
    "    .fill_(0) # cell\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 hidden state 텐서\n",
    "output_hiddens = (\n",
    "    torch.FloatTensor(batch_size, num_steps, hidden_dim)\n",
    "    .fill_(0)\n",
    ") # [B, MAX_LEN, HIDDEN]"
   ]
  },
  {
   "source": [
    "Beam Search\n",
    "- Greedy docoding이랑 달리 문장 단위로 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([13, 512, 7, 33])"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "encoder_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "SOS_TOKEN_ID = train_dataset.token_to_id['<SOS>']\n",
    "decoder_input = torch.LongTensor([SOS_TOKEN_ID])\n",
    "decoder_input.size()"
   ]
  }
 ]
}