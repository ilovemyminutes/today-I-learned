{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('py36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e4583e3e051816b4ac89cf1b6baba6f81196736b754b6f0341f1424c68064e5b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from attention import MultiHeadAttention\n",
    "from encoder import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 6\n",
    "NUM_HEADS = 2\n",
    "MAX_LEN = 4\n",
    "BATCH_SIZE = 5\n",
    "encoder = Encoder(d_model=6, num_heads=2, max_len=4)\n",
    "sample_batch = torch.rand(10, 4, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = MultiHeadAttention(d_model=D_MODEL, num_heads=NUM_HEADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.rand(BATCH_SIZE, MAX_LEN, D_MODEL)\n",
    "key = torch.rand(BATCH_SIZE, MAX_LEN, D_MODEL)\n",
    "value = torch.rand(BATCH_SIZE, MAX_LEN, D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = attn_layer(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(BATCH_SIZE, -1, NUM_HEADS, D_MODEL // NUM_HEADS)\n",
    "key = key.view(BATCH_SIZE, -1, NUM_HEADS, D_MODEL // NUM_HEADS)\n",
    "value = value.view(BATCH_SIZE, -1, NUM_HEADS, D_MODEL // NUM_HEADS)\n",
    "query = query.transpose(1, 2)\n",
    "key = key.transpose(1, 2)\n",
    "value = value.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ddfe6c2130b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m attention_raw = F.softmax(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m \u001b[0mattention_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "attention_raw = F.softmax(\n",
    "    torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(D_MODEL), dim=-1\n",
    ")\n",
    "attention_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(4,4), diagonal=0).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "pad_id = 0\n",
    "vocab_size = 100\n",
    "max_len = 10\n",
    "hidden_dim = 6\n",
    "\n",
    "def padding(data: list, pad_id: int=0) -> (list, int):\n",
    "    max_len = len(max(data, key=len))\n",
    "    output = [sample + [pad_id]*(max_len-len(sample)) for sample in tqdm(data)]\n",
    "    return output, max_len\n",
    "\n",
    "data = [\n",
    "     [62, 13, 47, 39, 78, 33, 56, 13, 39, 29],\n",
    "     [60, 96, 51, 32, 90, 44, 86, 71, 36, 18],\n",
    "     [35, 45, 48, 65, 91, 99, 92, 10, 31, 21],\n",
    "     [75, 51, 45, 48, 65, 91, 99, 11, 13, 28],\n",
    "     [66, 88, 98, 47, 48, 65, 17, 13, 67, 99],\n",
    "     ]\n",
    "\n",
    "# data, max_len = padding(data)\n",
    "data = torch.LongTensor(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weight initialized\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_dim)\n",
    "embedding.weight.data.uniform_(-1, 1)\n",
    "print('Weight initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X_embedded = embedding(data)\n",
    "X_embedded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_query = nn.Linear(in_features=hidden_dim, out_features=hidden_dim) # Query\n",
    "w_key = nn.Linear(in_features=hidden_dim, out_features=hidden_dim) # Key\n",
    "w_value = nn.Linear(in_features=hidden_dim, out_features=hidden_dim) # Value\n",
    "\n",
    "query = w_query(X_embedded)\n",
    "key = w_key(X_embedded)\n",
    "value = w_value(X_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiHeadAttention(hidden_dim=hidden_dim, num_heads=2)\n",
    "attention_mat = attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "attention_mat.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer_norm = nn.LayerNorm(normalized_shape=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "attention_mat.size()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "attn_layer_norm(attention_mat).size()"
   ]
  }
 ]
}