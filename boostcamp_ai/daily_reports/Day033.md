# Day 33. Object Detection, CNN Visualization

##### Object Detection?

- 이미지에 어떤 것들이 있는지 개체를 탐지하는 것
- 물체별 경계를 나타내는 Semantic Segmentation보다 한차원 더 복잡한 문제
- 물체가 (1) 어디에 있고 (2) 어떤 클래스에 해당하는 지 추론해야 하는 Task

##### 전통적 방식
- Gradient-based Detector

  ![trad_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/trad_1.jpg?raw=true)

  - 이미지를 잘게 분할 한 뒤, 각 부분에 대해 SVM 분류기를 활용하여 각 그리드별 그래디언트를 구하여 개체 개체탐지

- Selective Search

  ![trad_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/trad_2.jpg?raw=true)

  - 이미지에 수많은 Segmentation을 지정한 뒤, 유사한 영역끼리 합쳐나가는 방법

- 모두 특정 알고리즘에 기반한 constant한 방식!

##### R-CNN, Fast R-CNN, Faster R-CNN

- 세 모델의 전반적인 개체 탐지 과정은 다음과 같음

  1. 이미지로부터 '개체가 있을 법한' 후보 바운딩 박스를 제공
  2. 후보 바운딩 박스에 어떤 개체가 있는지 분류
  3. 바운딩 박스를 정교하게 재배열

  - 2와 3은 동시에 일어난다고 보면 됨

- R-CNN: 시초

  ![rcnn_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_1.png?raw=true)

  1. 임의 사이즈의 이미지로부터 후보 바운딩 박스를 2000개 수집
     - 후보 바운딩 박스 추출에는 Selective Search를 활용
  2. 후보 바운딩 박스를 모두 같은 사이즈로 warping하고, CNN에 입력
     - CNN은 총 5쌍의 Conv 레이어, Pooling 레이어 쌍으로 구성
     - 마지막 Pooling 레이어의 Feature Map을 FC 레이어에 입력. 최종적으로 얻은 1차원 Feature 벡터로 바운딩 박스 보정과 클래스 분류
       - 바운딩 박스 보정(bounding box regression): (x, y, w, h)를 예측. 각 value에 대해 FC 레이어를 연결하여 linear regression을 진행
       - 클래스 분류(classification): 해당 바운딩 박스에 어떤 개체가 있는지 분류. SVM을 사용

  - 문제점: 2000개의 후보 바운딩 박스에 대한 예측을 모두 진행해 추론 속도가 매우 느림

    ![rcnn_ipad_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_ipad_1.jpg?raw=true)

- Fast R-CNN: 후보 마운딩 박스는 단 한개만!

  ![rcnn_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_3.jpg?raw=true)

  1. 이미지 전체를 Conv 레이어에 입력하여 Feature Map을 구성

  2. ROI를 통해 얻은 후보 바운딩 박스를 ROI 풀링 레이어에 입력하여 일정한 사이즈의 출력을 얻음
     
     - 후보 바운딩 박스는 R-CNN과 같이 Selective Search를 활용
     
  3. ROI 풀링 레이어를 통해 얻은 Feature Map을 몇몇 FC 레이어를 거쳐 1차원 벡터화(flatten)

  4. 1차원 Feauture 벡터를 바탕으로 개체의 카테고리를 분류
     
     - R-CNN의 SVM과 달리, FC 레이어 + Softmax 레이어를 통해 분류
     
  5. 1차원 Feauture 벡터를 바탕으로 해당 바운딩 박스를 보정

     ![rcnn_ipad_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_ipad_2.jpg?raw=true)

- Faster R-CNN: ROI를 학습 가능하게

  ![rcnn_4](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_4.png?raw=true)

  - 모델은 크게 Conv 레이어, RPN, Fast RCNN 모듈로 구성. RPN과 Fast RCNN 모듈은 Conv 레이어를 공유(Shared Conv 레이어)

  1. 이미지를 Shared Conv 레이어에 입력하여 Feature Map을 추출

  2. RPN에 해당 Feature Map을 입력하여 후보 바운딩 박스를 얻음

     - 개체의 존재 여부와 위치에 의해 후보 바운딩 박스가 결정됨

       ![rcnn_5](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/rcnn_5.png?raw=true)

     - RPN에는 후보 바운딩 박스를 얻기 위해 Anchor Box를 활용

     - 슬라이딩 윈도윙 과정에서 윈도우의 중심을 Anchor라고 부르는데, 이 Anchor에 사전에 정의한 Anchor 박스를 위치해본 뒤, 그럴듯한 Anchor 박스를 후보 바운딩 박스로 선별

     - 논문에서 Anchor 박스는 3가지 사이즈와 3가지 비율로 총 9가지의 Anchor 박스를 사용

  3. 얻은 후보 바운딩 박스를 Fast R-CNN 모듈을 통해 분류 및 보정
  
  - [?] RPN이 제대로 이해되지 않음
    - Sliding Window: 3 x 3 필터를 Padding을 2로 설정(입력 shape 유지)
    - 각 Sliding Window의 Center Point에 k개의 Anchor Box를 적용
    - 각 Anchor Box와 Ground Truth Box 간 IoU를 계산, Foreground에 해당하는지 Background에 해당하는지에 대한 레이블링을 진행
      - Positive Label(Foreground): 해당 Anchor Box가 어떤 Ground Truth와도 IoU가 0.7보다 큰 경우, 또는 어떤 Ground Truth와의 IoU가 가장 큰 Anchor Box
      - Negative Label(Background): 해당 Anchor Box가 어떤 Ground Truth와도 IoU가 0.3보다 작은 경우
      - 그 외 Anchor Box는 Loss 계산에서 제외, 즉 학습에서 제외
    - 그래서 이 다음은?
      - Anchor Box를 적용한 뒤의 출력이 어떻게 생긴지 모르겠어
      - 지금 입력 자체가 Conv 레이어를 거친 Feature Map인데, Ground Truth는 어떤 식으로 표시되어 있는것? Ground Truth에 해당하는 픽셀이 Conv 레이어를 거쳐 불명확해졌을텐데
      - Anchor Box를 선별하는 건 어떻게 구현?
      - 어떤 Ground Truth와도 IoU가 0.7보다 큰게 가능?  => Anchor Box가 엄청 크면 가능할지도 모르겠네


> Attitude & Tips

- 헷갈려

