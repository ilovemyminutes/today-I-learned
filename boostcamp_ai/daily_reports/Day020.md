# Day 20. Self-Supervised Pre-Training Models

> GPT-1, BERT

최근 동향

- Transformer와 Self-Attention 메커니즘은 NLP 분야의 큰손으로 자리 잡았음
- 최근 모델의 핵심 키워드는 'BIG'
  - Transformer 블록을 여러 층으로 쌓아올려 성능 향상을 이끌고 있고, 모델의 크기와 성능의 비례관계는 그칠 줄을 모름
  - 특정 태스크에 맞는 모델을 학습하는 것보다, 대용량 데이터를 모조리 활용하여 대형 모델을 학습한 뒤, fine-tuning을 통해 downstream task를 해결
    - 대형 모델의 말단 부분만 문제 목적에 맞게 수정하여 Fine-tuning
    - 새롭게 수정한 부분에는 비교적 높은 learning rate를, 기존 모델 부분에는 비교적 낮은 learning rate를 부여하여 효과적인 학습을 진행!
  - ALBERT, RoBERTa, Reformer, T5, ELECTRA 등!
  - 하지만, Self-attention 구조의 Greedy Encoder는 여전히 문제로 남아있음

GPT-1, OpenAI

![gpt1_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\gpt1_1.jpg)

- Transformer 블록을 12개 쌓아올린 BIG 모델

- 입력 데이터의 구성을 달리하거나 말단 레이어를 조정한 뒤 fine-tuning을 통해 downstream 문제 해결

  ![gpt1_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\gpt1_2.jpg)

- Big model과 Fine-tuning의 힘!

BERT

![bert_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\bert_1.jpg)

- 기존의 GPT-1 모델의 경우 (transformer의 특징과 유사) cheating 방지를 위해 특정 time step의 학습/추론에 이후 time step 정보를 활용할 수 없음

- 근데, 사람은 앞 단어를 못 들었을 때, 뒷 단어를 통해 앞 단어를 유추하기도 하잖아?

  - 이러한 아이디어를 모델에 반영: MLM

- 크게 MLM(Masked Language Model), NSP(Next Sentence Prediction)으로 학습을 진행

- Train Process #1. Masked Language Model, MLM

  1. 전체 입력 단어 중 랜덤으로 15% 단어를 추출

  2. 이 중 80%를 [MASK]로 대체

  3. 10%를 임의의 다른 단어로 대체

  4. 나머지 10%를 있는 그대로 놓아 둠

  - 기존의 joint probability를 고려하여 학습하는 RNN 계열 모델과 달리, 단어 간 독립성을 가정한 뒤 모든 단어를 고려한 학습(위 그림 참조)
  - 문제점: fine-tuning의 과정에서 [MASK]의 토큰은 절대 등장하지 않는다는 점 - 학습과 현실의 괴리

- Train Process #2. Next Sentence Prediction, NSP

  ![nsp](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\nsp.jpg)

  - 한 문자열 내 두 문장을 구성
  - 앞 문장에는 [CLS], 뒷 문장에는 [SEP] 토큰을 포함하여 문장을 구분
  - 실제로 연결된 두 문장(real), 임의로 연결한 두 문장(fake)을 입력으로 binary classification 학습 진행 => 문맥에 대한 학습을 진행

- Pre-trained Model Architecture

  - L: Self-attention 블록 수, H: 인코딩 차원수, A: Attention 헤드 수
  - BERT BASE: L = 12, H = 768, A = 12 
  - BERT LARGE: L = 24, H = 1024, A = 16

- Input Representation

  ![bert_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\bert_2.jpg)

  - 서브 워드 단위의 임베딩(30,000 WordPiece)
  - 사전 학습/최적화한 positional embedding
  - [CLS] - classification embedding
  - [SEP] - packed sequence embedding
  - Segment Embedding: 토큰별 소속 정보에 대한 임베딩 벡터

- BERT와 GPT-1의 차이점?

  - 학습 데이터: BERT는 BookCorpus와 위키피디아의 25억 개 단어, GPT-1은 BookCorpus의 8억 개 단어
  - 스페셜 토큰: Pre-training 과정에서 [CLS], [SEP], NSP과정에서의 sentence embedding
  - 배치 사이즈: BERT는 128000 단어, GPT는 32000 단어
  - 특정 태스크에 대한 fine-tuning
    - GPT: fine-tuning에 5e-5의 lr을 채택
    - BERT: fine-tuning에 따라 다른 lr 요구

BERT의 task-specific fine-tuning

- Machine Reading Comprehension, MRC(QA)

  - SQuAD 1.1: 독해를 통해 문서를 이해하고 주어진 문제에 대한 정답을 문서에서 추출

    ![squad_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\squad_1.jpg)

    - 지문 + [SEP] + 질문의 형태로 학습
    - d차원으로 인코딩된 각 단어를 하나의 스칼라로 맵핑하는 F.C Layer 활용
    - 각각을 모두 concat하여 하나의 벡터로 통합한 뒤 소프트맥스. 정단 단어의 첫 위치 인덱스가 정답 레이블

  - SQuAD 2.0: 기존의 1.1 버전에서 정답의 유무까지 예측

    - [CLS] 토큰을 활용하여 문서에 질문에 대한 정답이 있는지 없는지 판단

  - On SWAG: 다음에 등장할 문장 중 가장 적절한 문장을 예측

    ![OnSWAG](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\OnSWAG.jpg)



GPT-2

- 구조는 GPT-1과 크게 다르지 않지만 40GB에 달하는 텍스트 데이터를 학습 & 학습 데이터의 퀄리티 우수

  - 퀄리티 우수: 추천 수가 많은 레딧의 포스트 텍스트 데이터 활용, 데이터 처리 과정에 BPE 적용

- 파라미터 최적화 없이 down-stream 태스크 수행

- 모델 구조

  ![gpt2_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\gpt2_1.jpg)

  - residual connection, activation 이후 Layer Normalization를 취한 형태
  - 각 residual layer에 대해 전체 residual layer 개수 n으로 나누어 스케일링: 앞선 레이어의 영향력이 점차적으로 줄어들 수 있도록

- *'최적화 없이도 성능이 좋더라'* - 별다른 학습 없이 fine-tuing한 BERT 모델(89 F1 Score) 대비 3/4의 성능(55 F1 Score)을 얻음

- TL;DR(Too Long Didn't Read)의 문서 한줄 요약 태스크에도 활용

  - TL;DR - 한 아티클을 100개의 토큰으로 생성

GPT-3: 미친듯이 큰 모델

![gpt3_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\gpt3_1.jpg)

- 미친듯이 많은 파라미터: 모델 크기가 클수록 범용적인 활용이 가능(task-agnostic)

  - 175억 개 파라미터: 96개의 Attention 레이어, 320만 개의 배치 사이즈

- Few-shot

  ![few_shot](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\few_shot.jpg)

  - Fine-tuning을 거치지 않고, 맞춰야할 타깃에 앞서 힌트 문맥을 몇 개 제시하여 예측 수행

  - 다음과 같이 모델의 크기가 커질 수록 few-shot learning의 성능 향상 폭은 높아지고, 상승폭은 점점 더 커짐

    ![few_shot_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\few_shot_2.jpg)

    

ALBERT: 모델 사이즈를 줄여보자

- ALBERT 모델은 기존의 BERT 모델의 파라미터 수를 줄인 모델, 즉 BERT 모델을 경량화한 모델

- Factorized Embedding Parameterization

  - Remind: Transformer 구조의 입력 x에 대한 임베딩 벡터 차원은 모델의 flow 끝까지 이어짐 - output된 hidden 벡터의 차원까지

  - 따라서, 임베딩 벡터의 차원을 높여야만 높은 표현력을 얻을 수 있는데, 위 그림과 같이 임베딩 벡터의 차원을 높일 수록 파라미터 수가 급격히 증가

  - ALBERT 모델은 임베딩 벡터와 인코더 사이에 차원을 변경해주는 추가적인 Linear 레이어를 구성 => 임베딩 벡터 차원을 높이지 않고도 높은 표현력을 얻을 수 있게 됨(approximation)

    ![albert_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\albert_1.jpg)

- Cross-layer Parameter Sharing

  - Shared-FFN: feed-forward 네트워크 파라미터를 모든 레이어에 걸쳐 공유함
  - Shared-attention: 모든 attention 레이어가 같은 attention 파라미터를 공유
  - All-shared: FFN, attention 파라미터 모두 공유
  - 파라미터 수를 대폭 낮출 수 있는 반면 성능 하락 폭은 낮아 효율성 굿!

![albert_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\albert_2.jpg)

- Sentence Order Prediction, SOP
  - 기존 BERT의 학습 과정 중 하나인 NSP는 성능 향상에 별 도움이 되지 않음
  - 문장 간 연결 관계를 예측함에 있어서, 인위적으로 concat한 문장은 앞뒤문장의 사용 단어 빈도 수가 확연히 달라 구분이 쉽기 때문
  - 때문에, 이를 바꾸어 연결된 두 문장의 순서만을 바꿔 학습을 진행 - 두 문장에 사용되는 단어의 분포가 유사하여 더 어려운 학습 태스크

![albert_3](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\albert_3.jpg)

- GLUE 벤치마크

  ![albert_4](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\albert_4.jpg)

ELECTRA

- Advarsarial Learning을 활용한 모델

  ![ELECTRA_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\ELECTRA_1.jpg)

- Generator: BERT 모델의 Masking Prediction

- Discriminator: Generator의 output이 입력 데이터와 일치하는지, 그렇지 않은지 판별

- Pre-train을 걸쳐 Discriminator를 down-stream 태스크에 활용(ELECTRA)

  ![ELECTRA_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\ELECTRA_2.jpg)

  - 기존의 Masked 랭귀지 모델에 비해 학습을 반복할 수록 빠르게 성능이 향상됨

Light-weight Models

- BIG-size 모델들을 경량화 하여, 모바일 기기 등 엣지 디바이스에도 모델이 활용될 수 있도록 하는 기술
- DistillBERT, TinyBERT 등

Fusing Knowledge Graph into Language Model

- 모델의 '상식'을 높이는 연구
- ERNIE, KagNET 등



> Attitude & Tips

GLUE Benchmark: NLP 모델의 상대적 평가를 위해 QQP, QNLI, SST-2 등 여러 정형화된 데이터셋을 활용하여 벤치마크 진행

![GLUE](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\GLUE.jpg)

모델 크기에 따른 성능 향상은 멈출 줄을 모른다

- 다만, 컴퓨터 환경의 한계, 학습 속도의 한계는 있으니 적절한 지점을 찾아야지

![model_size](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week04\model_size.jpg)