# Day 20. Self-Supervised Pre-Training Models

> GPT-1, BERT

최근 동향

- Transformer와 Self-Attention 메커니즘은 NLP 분야의 큰손으로 자리 잡았음
- 최근 모델의 핵심 키워드는 'BIG'
  - Transformer 블록을 여러 층으로 쌓아올려 성능 향상을 이끌고 있고, 모델의 크기와 성능의 비례관계는 그칠 줄을 모름
  - 특정 태스크에 맞는 모델을 학습하는 것보다, 대용량 데이터를 모조리 활용하여 대형 모델을 학습한 뒤, fine-tuning을 통해 downstream task를 해결
    - 대형 모델의 말단 부분만 문제 목적에 맞게 수정하여 Fine-tuning
    - 새롭게 수정한 부분에는 비교적 높은 learning rate를, 기존 모델 부분에는 비교적 낮은 learning rate를 부여하여 효과적인 학습을 진행!
  - ALBERT, RoBERTa, Reformer, T5, ELECTRA 등!
  - 하지만, Self-attention 구조의 Greedy Encoder는 여전히 문제로 남아있음

GPT-1, OpenAI

- 일론 머스크의 