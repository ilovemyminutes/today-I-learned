# Day 24. 정점의 표현, 추천 시스템 | 신기정 교수님

> Node Embedding

Node Embedding: 그래프의 정점들을 벡터의 형태로 표현하는 것

![node_embedding_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\node_embedding_1.jpg)

- 벡터로 표현이 가능하면 머신러닝 등을 활용해 군집 분석, 정점 분류 등 다양한 문제를 해결할 수 있음

- 벡터로 '잘' 표현되도록 하기 위한 object: 정점 간 관계, 즉 '유사도'를 유지

  ![node_embedding_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\node_embedding_2.jpg)

Similarity

- 임베딩 공간에서의 유사도: 내적(inner product)를 유사도로써 사용: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+z_v%5E%7B%5Cintercal%7Dz_%7Bu%7D+%3D+%7C%7Cz_%7Bu%7D%7C%7C+%5Ccdot+%7C%7Cz_%7Bv%7D%7C%7C+%5Ccdot+cos%28%5Ctheta%29)
- 그래프 공간에서의 유사도: 여러 방법이 있음
  - 인접성 기반 유사도: *'두 정점이 인접해있을 경우 유사하다'*
  - 거리 기반 유사도: *'두 정점이 거리가 짧을 수록 유사하다'*
  - 경로 기반 유사도: *'두 정점 사이에 경로가 많을 수록 유사하다'*
  - 중첩 기반 유사도: *'두 정점이 많은 이웃을 공유할 수록 유사하다'*
  - 임의보행(random walk) 기반 유사도: *'한 정점에서 출발하여 다른 정점에 도달할 확률이 높을 수록 두 정점은 유사하다'*
- 그래프 공간에서의 유사도와 임베딩 공간에서의 그것을 최대한 일치하도록 학습하여 임베딩을 하게 되는 것!

그래프 공간에서의 유사도: 인접성 기반 접근법, Adjacency

- *'두 정점이 인접해있을 경우 유사하다'*''

- 인접 행렬(adjacency matrix): 정점이 인접한 경우 1, 그렇지 않은 경우 0 => 두 정점 간 유사도가 binary 값을 갖게 됨

  ![adj_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\adj_1.jpg)

- 임베딩 학습 시 활용하는 손실 함수: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum_%7B%28u%2C+v%29+%5Cin+V+%5Ctimes+V%7D+%7C%7Cz_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D+-+A_%7Bu%2C+v%7D%7C%7C%5E%7B2%7D)

- 한계

  - 그래프 내 두 정점 간 거리를 반영할 수 없음

  - 그래프 내 정점이 소속된 군집을 고려할 수 없음

    ![adj_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\adj_2.jpg)

그래프 공간에서의 유사도: 거리 기반 접근법

- *'두 정점이 거리가 짧을 수록 유사하다'*

- '충분히 가깝다'에 대한 기준(threshold)를 마련하여 기준 거리보다 거리가 가까운 경우 유사하다고 간주

![distance_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\distance_1.jpg)

그래프 공간에서의 유사도: 경로 기반 접근법

- *'두 정점 사이에 경로가 많을 수록 유사하다'*

- 경로: 정점 u, v에 대하여 다음 조건을 만족하는 정점들의 순열
  - u로 시작하여 v로 끝난다
  - 순열에서 연속된 정점은 간선으로 연결되어 있어야 한다
- 임베딩 학습 시 활용하는 손실 함수: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum_%7B%28u%2C+v%29+%5Cin+V+%5Ctimes+V%7D+%7C%7Cz_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D+-+A_%7Bu%2C+v%7D%5E%7Bk%7D%7C%7C%5E%7B2%7D) (![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+A_%7Bu%2C+v%7D%5E%7Bk%7D): 정점 u와 v의 경로 중 거리가 k인 경로의 수)

그래프 공간에서의 유사도: 중첩 기반 접근법

- 임베딩 학습 시 활용하는 손실 함수: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum_%7B%28u%2C+v%29+%5Cin+V+%5Ctimes+V%7D+%7C%7Cz_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D+-+S_%7Bu%2C+v%7D%7C%7C%5E%7B2%7D)(![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+S_%7Bu%2C+v%7D+%3D+%7CN%28u%29+%5Ccap+N%28v%29%7C+%3D+%5Csum_%7Bw+%5Cin+N%28u%29+%5Ccap+N%28v%29%7D+1): u의 이웃 집합을 N(u)라고 할 때 두 정점의 공통 이웃의 수)
- 공통 이웃 수(![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+S_%7Bu%2C+v%7D+%3D+%7CN%28u%29+%5Ccap+N%28v%29%7C+%3D+%5Csum_%7Bw+%5Cin+N%28u%29+%5Ccap+N%28v%29%7D+1)) 대신 자카드 유사도 또는 Adamic Adar 점수를 사용할 수 있음
  - 자카드 유사도: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Cfrac+%7BN_%7Bu%7D+%5Ccap+N_%7Bv%7D%7D+%7BN_%7Bu%7D+%5Ccup+N_%7Bv%7D%7D) (공통 이웃 수의 **비율**을 의미)
  - Adamic Adar: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Csum_%7Bw+%5Cin+N_%7Bu%7D+%5Ccap+N_%7Bv%7D%7D+%5Cfrac+%7B1%7D+%7Bd_%7Bw%7D%7D) (![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+d_%7Bw%7D): log(N(w)), 정점 w와 인접한 노드 수에 로그를 취한 값)

그래프 공간에서의 유사도: 임의보행 기반 접근법

![random_walk](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\random_walk.jpg)

- *'한 정점에서 출발하여 다른 정점에 도달할 확률이 높을 수록 두 정점은 유사하다'*

- Random Walk: 현재 정점의 이웃 중 하나를 균일 확률로 선택하는 이동 과정을 반복하는 것

  - 시작 정점 주변의 지역적 정보와 전역적 정보를 모두 고려한다는 특징

- 과정

  1. 각 정점에서 시작해 random walk(deep walk) 반복 수행

  2. random walk 중 도달한 정점 리스트 구성. 여러 번 도달한 경우, 여러 번 리스트업 됨

  3. 이를 바탕으로 다음의 손실 함수를 최소화

     ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum_%7Bu+%5Cin+V%7D+%5Csum_%7Bv+%5Cin+N_%7BR%7D%28v%29%7D+-+log%28P%28v+%7C+z_%7Bu%7D%29%29)(![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+N_%7BR%7D%28u%29): u가 random walk를 통해 도달한 정점 리스트)

     - 정점 u가 정점 v에 도달할 확률(![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28v%7Cz_%7Bu%7D%29))은 소프트맥스 활성화 함수를 거쳐 도출

Node2Vec

- 임의보행 접근법의 일종

- 현재 정점과 직전에 머물렀던 정점을 모두 고려하여 다음 도달할 정점을 선택

  ![node2vec_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\node2vec_1.jpg)

  - 직전 정점의 거리를 기준으로 경우를 구분해 확률을 차등적으로 부여

- 다음과 같이 확률 부여 기준에 따라 군집화 결과가 달라짐. 즉, 임베딩 결과가 다름

  ![node2vec_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\node2vec_2.jpg)

- 임베딩 학습 시 활용하는 손실 함수: ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum_%7Bu+%5Cin+V%7D+%5Csum_%7Bv+%5Cin+N_%7BR%7D%28u%29%7D+-+Softmax%28z_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D%29)

  - 중첩된 summation으로 인해 O(n^2)의 시간복잡도로, 정점 수 n이 커질 수록 현실적으로 연산이 어려워짐
  - 따라서, 네거티브 샘플링을 통해, 몇 개의 정점만을 추출하여 손실값을 측정
    - 샘플링 크기가 클 수록 안정적

Transductive Method

- 위 임베딩 방법들은 변환식(Transductive) 방법으로, 학습의 결과로 임베딩 자체, 즉 변환된 정점 벡터를 얻음

- 이와 반대로, 정점을 임베딩하는 함수를 얻는 귀납식(inductive) 방법이 있음

- Transductive 방법의 한계

  1. 추가된 정점에 대한 임베딩 불가

  2. 모든 정점에 대한 임베딩을 미리 계산 후 저장. 즉, 메모리 공간이 필요함

  3. 정점이 attribute를 지닌 경우 활용 불가

     

> Latent Factor Recommender System

사용자와 상품을 벡터 공간에 임베딩하여 상품을 추천하는 방법

- 룰베이스로 각 object를 fix된 지점에 맵핑하는 것이 아니라, 효과적인 factor를 찾아 맵핑하는 것이 핵심

- 임베딩의 기준: 사용자와 상품의 **내적이 평점과 최대한 유사**하도록!

  ![latent_factor_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week05\latent_factor_1.jpg)

손실 함수

- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Csum_%7B%28i%2C+x%29+%5Cin+R%7D+%28r_%7Bxi%7D+-+p_%7Bx%7D%5E%7B%5Cintercal%7Dq_%7Bi%7D%29%5E%7B2%7D)(![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+p_%7Bx%7D): 사용자 x의 latent factor, ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+q_%7Bi%7D): 상품 i의 latent factor)
  - 과적합의 위험성으로 regularization을 적용한 손실 함수를 사용
  - ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Csum_%7B%28i%2C+x%29+%5Cin+R%7D+%28r_%7Bxi%7D+-+p_%7Bx%7D%5E%7B%5Cintercal%7Dq_%7Bi%7D%29%5E%7B2%7D+%2B+%5B%5Clambda_%7B1%7D+%5Csum_%7Bx%7D+%7C%7Cp_%7Bx%7D%7C%7C%5E%7B2%7D+%2B+%5Clambda_%7B2%7D+%5Csum_%7Bi%7D+%7C%7Cq_%7Bi%7D%7C%7C%5E%7B2%7D+%5D)

Advanced Latent Factor Recommender System

- 사용자와 상품의 평점 값 자체를 사용하는 것이 아니라, 편향을 학습에 활용하는 방법

- 사용자의 편향: 해당 사용자가 매겼던 평점들의 평균과 전체 상품 평점 평균의 차

- 상품의 편향: 해당 상품에 매겨진 평점들의 평균과 전체 상품 평점 평균의 차

- 평점을 다음과 같이 분리하여 학습(bx: 사용자 편향, bi: 상품 편향, μ: 전체 상품 평균)

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+r_%7Bxi%7D+%3D+%5Cmu+%2B+b_%7Bx%7D+%2B++b_%7Bi%7D+%2B+p_%7Bx%7D%5E%7B%5Cintercal%7Dq_%7Bi%7D)

- 위 4개의 항을 모두 고려하여 손실항을 구성, 4개 항에 대한 regularization을 모두 부여한 손실함수를 학습에 활용

시간적 편향을 고려한 잠재 인수 모형

- 다음과 같이 각 편향이 시간에 따라 변할 수 있도록 평점을 모형화

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+r_%7Bxi%7D+%3D+%5Cmu+%2B+b_%7Bx%7D%28t%29+%2B++b_%7Bi%7D%28t%29+%2B+p_%7Bx%7D%5E%7B%5Cintercal%7Dq_%7Bi%7D)

> Attitude & Tips

- 확실히 스크래치 코딩이 필요하겠다는 생각이 빡 드네