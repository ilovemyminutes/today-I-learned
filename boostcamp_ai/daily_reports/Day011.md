# Day 11. 딥러닝 기초 | 임성빈, 최성준, 최성철 교수님

> 베이즈 통계학: *어떤 식으로 모델의 모수를 추정할까?*

베이즈 정리

- 조건부확률을 이용하여 정보를 갱신하는 방법
  $$
  P(B|A) = \frac {P(A \cap B)} {P(A)} = P(B) \frac {P(A|B)} {P(A)}
  $$

  - A의 새로운 정보가 주어졌을 때 P(B)로부터 P(B|A)를 계산하는 방법을 제공

  - 이런 식으로 적용해볼 수 있지
    $$
    P(\theta | D) = P(\theta) \frac{P(D | \theta)} {P(D)}
    $$

    - P(θ):사전확률, P(θ|D): 사후확률, P(D|θ): 가능도, P(D): Evidence
      - Evidence: 데이터 전체의 분포 <- [?] 좀더 찾아보자
        - Evidence: Marginal Likelihodd라고도 불림. θ와 독립이기 때문에 P(θ)(사전확률)을 Normalize하는 역할을 함
        - 사전확률 각각에 대한 가능도의 기댓값으로도 해석할 수 있음
    - Reference. Mathematics for Machine Learning
    - 사전확률은 데이터 분석 전, 가정/가설을 통해 우리가 알고 있는 확률
    - 사후확률은 모델링 결과 추정한 확률
    
  - 조건부 확률

$$
P(A \cap B) = P(B)P(A|B)
$$

- 예: 발병률이 10%인 COVID-99. 실제로 걸렸을 때 검진될 확률 99%, 걸리지 않았을 때 오검진될 확률이 1%일 떄, 어떤 사람이 질병에 걸렸다고 검진결과가 나왔을 때 실제로도 감염됐을 확률은?

  - 오탐율이 오르면 테스트의 정밀도(Precision)가 떨어진다!

    ![image-20210201111519495](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201111519495.png)

    ![image-20210201111825252](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201111825252.png)

    ![image-20210201111805887](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201111805887.png)

베이즈 정리를 통한 정보의 갱신

- 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 **사후 확률을 사전 확률로 사용**하여 갱신된 사후 확률을 계산

조건부확률 -> 인과관계?

- 조건부확률은 유용한 통계적 해석을 제공하나, 인과관계(causality)를 추론할 때 함부로 사용해서는 안 된다.

- 데이터가 아무리 많아져도 조건부확률만 가지고 인과관계를 파악하기 어렵다

- 인과관계는 데이터 분포의 변화에 robust한 예측 모델을 만들 때 중요

  ![image-20210201114126334](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201114126334.png)

  - [?] 인과관계 기반 모델이란?

- 인과관계를 알아내기 위해서는 중첩요인(confounding factor) 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 함

  ![image-20210201114257998](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201114257998.png)
  - 예: '키가 클 수록 지능이 높다'라는 분석 결과가 나오는 경우가 있는데, 이건 연령 요인에 대한 중첩요인을 배제하지 않았기 때문에 발생하는 문제임

  - 예: 치료법에 따른 완치율의 원인 결과 분석

    ![image-20210201114600071](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201114600071.png)

    - 전체적으로는 치료법 b가 더 좋은 모습을 보이나, 신장 결석 크기가 작은 경우와 큰 경우 모두 치료법 a가 더 좋은 모습(Simpson's paradox)

    - '신장 크기'의 중첩 효과를 제거하지 못해 발생! 조정(intervention) 효과를 통해 Z의 개입을 제거

      ![image-20210201115130191](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201115130191.png)

      ![image-20210201115141456](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201115141456.png)

      - 중첩효과를 제거하고 나면, 치료법 a가 더 효과가 좋다는 것을 알 수 있음



> Deep Learning Basic

Introduction

- 장님이 코끼리를 만지는 것과 같음
- What make you a good deep learner?
  - 구현 실력: 머릿 속으로 생각하는 것을 실제로 만들 줄 아는 것
  - 수학 실력: 선형대수, 확률론 등 근본!
  - 트렌드 파악: 연구 결과를 아는 것

- 딥러닝을 공부하는 것은 AI 전부를 공부하는 것과 동치가 아니다

![image-20210201115942392](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201115942392.png)

- 딥러닝의 핵심 요소
  - 데이터: 학습을 위한 대용량의 데이터가 필요하다
  - 모델: 데이터를 입력받아 학습할 모델이 필요
  - 손실함수: 모델 학습을 위한 loss를 측정하는 함수가 필요
  - 알고리즘(최적화): loss를 최소화하기 위한 최적화 방법이 필요 

Data

- 풀고자 하는 문제에 따라 필요한 데이터가 달라짐
  - Classification
  - Semantic Segmentation: 이미지의 픽셀별 어떤 클래스에 속하는지 분류
  - Detection: 이미지 내 물체 영역 인식
  - Pose Estimation: 사람이 취하고 있는 자세(스켈레톤 정보)를 추출
  - Visual QnA: 이미지와 질문이 주어졌을 때 답을 리턴

Model

- 문제를 해결하기 위한 모델
- GoogleNet, GAN, ...

Loss

- 모델을 어떻게 학습할 것인가. 학습의 기준을 제공
  - Regression Task: MSE
  - Classification Task: Cross Entropy
  - Probabilistic Task: MLE
- Loss는 달성하고자 하는 목표의 근사치에 불과함!
  - Loss Function이 감소하는 것만이 목표는 아님. 감소하고 있다고 해서 원하는 학습이 된다는 보장이 없음. 문제와 손실함수의 관계를 잘 고려하는 것이 중요
    - 회귀 문제를 풀 때, 데이터 노이즈가 많을 경우 몇몇의 이상치가 학습을 방해할 수 있음 => 이럴 때는 MAE를 사용하는 것이 좋음

Optimization Algorithm

- 최적화 방법을 통해 손실함수가 정해지고, 최적화를 어떻게 진행할 지 결정할 수 있음
  - SGD, Momentum, NAG, ...
- 최적화 방법별 특성을 파악하는 것도 매우 중요
- Dropout, Early Stopping 등 다양한 방법을 동원해서 테스트 데이터에 대한 예측이 잘 되도록 학습

Historical Review

- AlexNet(2012): 224X224 이미지 분류
  - ILSVRC: 이전부터 있었는데, AlexNet이 딥러닝을 활용해서 최초로 우승. 이후로 딥러닝 이외의 우승 모델이 나온 적이 없음(Black Magic)
  - 이 모델을 기점으로 머신러닝의 패러다임이 바뀌었음
- DQN(2013): 강화학습. 딥마인드가 통해 Atari 게임 학습에 활용한 강화학습 모델
- Encoder / Decoder(2014): NMT(neural machine translation) 문제 해결
  - 인코더: 주어진 단어 시퀀스를 특정 벡터로 인코딩
  - 디코더: 인코딩 벡터로부터 다른 언어의 시퀀스로 디코딩
- Adam Optimizer(2014)
  - Adam을 왜 죄다 사용할까? *학습이 잘 되니까!*
  - CIFAR10 데이터를 활용한 논문 썰
    - SGD 옵티마이저로 총 Epochs의 절반을 돌리다가 lr을 1/10로 줄이고 75% 학습이 됐을 때 lr을 또 1/10로 줄이면 논문 결과가 나옴
    - 보통 '왜 이 옵티마이저를 사용했는지'에 대해서는 설명이 나오지 않음. 다른 옵티마이저를 왜 채택하지 않았는지, 왜 이렇게 했는지 안나옴 => 이렇게 안하면 복원이 안되고, 안 좋은 성능이 나온다는 이야기
    - 하이퍼 파라이머 서치에 따라 성능 결과가 아주 달라짐(컴퓨터 리소스가 대량 요구됨)
    - 이럴 떄 Adam은 일반적으로 좋은 성능을 보장해줌!
- Generative Adversarial Network(GAN, 2015)
  - 아주 중요한 부분!
- Residual Network(ResNet, 2015)
  - 많은 의미를 남겨준 논문
  - 딥러닝을 딥러닝으로 부를 수 있게 한 논문이라고 볼 수도 있음: *네트워크를 깊게 쌓기 때문에 딥러닝*
  - 네트워크를 너무 깊게 쌓으면 테스트시 좋지 않은 결과가 나온다는 인식이 깔려있었는데, ResNet을 계기로 인식이 바뀜
- Transformer(2017)
  - Attention 구조는 매우 매우 중요한 구조임
  - 다른 RNN 구조를 거의 대체해버렸음
- BERT(2018)
  - Fine-tuned NLP 모델: NLP 분야의 다양한 문제를 pre-trained 된 네트워크를 소수의 데이터로만 재학습 시켜서 해결
- BIG Language Models(GPT-X, 2019)
  - Fine-tuned NLP 모델의 끝판왕
  - 1750억개의 파라미터로 구성
- Self Supervised Learning(2020)
  - SimCLR
  - 주어진 학습 데이터 이외에, 라벨이 부여되지 않은 데이터를 추가 활용
  - 비지도학습을 통해 데이터를 방대하게 활용
  - 문제가 구체적으로 정의되어 있을 때, 데이터를 추가 활용하는 것이 아닌, 오히려 데이터를 새롭게 만들어내는 논문도 있음(self supervised data sampling)

> PyTorch 시작하기

PyTorch, TensorFlow

- Facebook 출신 PyTorch, Google 출신 TensorFlow(feat. Keras)

  |                                 |           Keras           |           TensorFlow           |                PyTorch                |
  | :-----------------------------: | :-----------------------: | :----------------------------: | :-----------------------------------: |
  |            API Level            |        high-level         |     Both high & low level      |              lower-level              |
  |              Speed              |           Slow            |              High              |                 High                  |
  |          Architecture           | simple, readable, concise |      not very easy to use      |                complex                |
  |            Debugging            |     no need to debug      |           difficult            |           good capabilites            |
  |      Dataset Compatibility      |       slow & small        |          fast & large          |             fast & large              |
  |         Popularity Rank         |             1             |               2                |                   3                   |
  |           Uniqueness            | multiple back-end support | object detection functionality | flexibility & short training duration |
  |           Created By            |             -             |             Google             |               Facebook                |
  |           Ease of use           |       user-friendly       |      incomprehensive API       |    integrated with Python language    |
  | ***Computational graphs used*** |       static graphs       |         static graphs          |   ***dynamic computation graphs***    |

- TensorFlow 예전에는 말이야...

  - 세션을 열어서 함수가 구동될 수 있는 공간을 만들어야 함(텐서(tensor)가 흐를 수 있는(flow) 공간)

  - Define -> Run이 어려우며, 디버깅도 어렵다(논문 구현하는 사람=🐶잘하는 사람)

  - print문도 못썼음 ㅎ

    <img src="C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201133912454.png" alt="image-20210201133912454" style="zoom: 50%;" />

  - Dynamic Graph를 지원하는 PyTorch로 사용자들이 넘어감

  - TensorFlow 쓰다가 PyTorch를 사용하니 피부가 좋아지고 시력이 좋아졌다는 썰도 있음

  - 검색, 논문 구현 등 이제는 PyTorch가 TensorFlow를 따라 옴

  - 그래도 TensorFlow가 서비스 활용에 더 편해서 여전히 많이 사용

- PyTorch

  - *'NumPy + AutoGrad + Function'*
  - NumPy 구조를 갖는 Tensor 객체로 array 표현
  - 자동미분을 지원하여 DL 연산 지원
  - DL을 지원하는 다양한 형태의 함수와 모델
  - 튜토리얼 잘 보자! 'PyTorch Tutorial'
  - 'PyTorch로 시작하는 딥러닝 입문'

> 뉴럴 네트워크 - MLP

Neural Networks

- 꼭 인공신경망이 인간의 뇌를 본땃기 떄문에 성능이 좋은 것이 아니다
- 굳이 인간의 뇌를 모방하는 방법론이 아니라는 것
- *하늘을 날고 싶다고 해서 새처럼 날 필요가 없다. 현재 비행기는 새처럼 생기지 않았음*
- 인간의 뇌로부터 영감을 얻었을 수 있으나, 이미 너무 다른 형태가 되어버렸다

정의

- 'function approximators that stack affine transformations followed by nonlinear transformations'

Linear Neural Networks

![image-20210201174534387](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201174534387.png)

- Loss를 최소화하기 위한 w, b를 찾는 것이 목적.

  - 기본적인 선형모델로 다양한 탐색 방법이 존재하나, 모델이 linear하고 loss가 convex일 때 등 다양한 제약 조건 존재

  - 그래서 Backpropagation 활용!

    - Loss가 줄어들 수 있는 방향으로 파라미터가 움직이는 방향을 결정. 즉, 미분값의 반대 방향으로 학습 진행

      ![image-20210201174915982](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201174915982.png)

    - Gradient Descent

  - eta(learning rate): 적절한 학습이 될 수 있도록 적절한 값을 지정행야 함

  - transpose(W)와 x의 행렬곱의 의미: 두 벡터스페이스 간의 선형 변환

    - 선형성이 있는 변환은 행렬로 표현이 가능하기 떄문

  - 레이어를 여러개로 쌓으면?

    ![image-20210201175304691](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201175304691.png)

    - 여러 레이어를 쌓을 수 있지만, 이건 결국 두 행렬을 곱한 또 하나의 행렬을 곱한 것과 다를 바 없음

    ![image-20210201175402011](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201175402011.png)

    - 복잡한 학습이 가능하도록 *Nonlinear transform 함수*를 취하는 것!

Beyond Linear Neural Networks

- Activation Functions: ReLU, Sigmoid, Hyperbolic Tangent

  - 문제에 따라 어떤 활성화 함수가 좋을지는 알 수 없음. 분명한건 레이어를 깊게 쌓기 위해서는 이러한 nonlinear transform 함수를 끼워넣여야 한다는 것!

- Neural Network가 학습이 잘 되는 또다른 구체적인 이유: 히든레이어 하나가 포함된 인공신경망은 원하는 값을 거의 근사할 수 있다(Multilayer Feedforward Networks are Universal Approximators)

  - 하지만 존재성만 보장할 뿐, 어떻게 생겨먹어야되는 건지 알 수 없음

- Loss function이 뭘까?

  ![image-20210201181102002](C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210201181102002.png)

  - 회귀, 왜 하필 제곱 오차를 활용하는걸까? 
    - 당연히 제곱 오차를 사용하지 않아도 된다. 각 loss마다 갖고 있는 성질이 다르고, 데이터에 작용하는 효과가 다르기 떄문에, 상황에 맞는 선택이 필요하다.
  - 분류, 크로스 엔트로피 왜 사용하는데?
    - 식을 유심히 바라보면 그 답을 내려볼 수 있음! d차원의 true value 벡터는 한 원소만 1이고, 나머지는 0. 즉, 예측을 통해 나온 y_hat에 대해 정답에 해당하는 위치에 대해서만 값을 추출하는 것! 그렇게 되면 모델이 얼마나 잘 맞추고 있는지 시그널을 보내주는 것과 맥락이 같겠지
  - 확률적 접근, 왜 가능도를 사용하는가?
    - 출력값의 숫자 그대로가 아니라, 확률적인 해석을 하고자 할 때 사용.

> 데이터셋 다루기

.py 파일을 잘 다뤄보자(파일명으로 구조를 느껴보자)

- main.py: 파이썬을 실행하는 파일
- config.py: 환경을 설정하는데 활용되는 파일
- network.py: 모델이 모듈화되어 있는 파일
  - 모듈화를 하게 되면 모델을 블록 조립하듯이 연결할 수 있음
  - ipython 파일만 사용하게 되면 효율적인 모델 사용이 불가능

- 데이터를 다운 받는 것부터 학습까지 일괄적으로 작동하도록 코드를 짜봐라



> Attitude & Tips

- 1종오류/2종오류 확실하게 알아두자
- 수식을 눈으로만 보지 말고 손으로 써보니까 이해가 더 잘 된다.
- 베이지안 통계 좀더 찾아봐야겠다.
  - Causality
  - Cofounding Factor
  - Intervention
- Metric 정리해두기
- 추가적으로 보면 좋겠다고 조언 주시는 부분은 모두 확인해보자
- Denny Britz, Deep Learning's Most Important Ideas - A Brief Historical Review, 2020
- 파이토치 코드 리뷰가 활발히 필요한 시점!
  - 코드를 타고 들어가서 코드를 리뷰하는 식으로
- 코랩 자원이 회수되어 원격장치 연결이 끊길 수 있음
- `os.getcwd()`
- ubyte?
- \__getitem__?
- 데이터