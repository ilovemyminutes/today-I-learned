

# Day 31. Image Classification, Annotate Data Efficient Learning | 오태현 마스터

> Image Classification

###### Computer Vision

- Inverse Rendering: 3D 정보를 갖고 공간을 만드는 컴퓨터 그래픽과 달리, 컴퓨터비전은 형성된 공간으로부터 이미지를 추출해오는 것이기에 Inverse rendering으로 볼 수 있다

![img_clf_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_1.jpg?raw=true)

###### Image Classification

- 머신러닝 활용 이미지 분류: feature를 사용자가 직접 지정해주는 작업 필요 => 딥러닝의 경사하강법을 통한 feature extraction과 대조
- 딥러닝 활용 이미지 분류: 이미지를 입력받아 내부적으로 추상적인 변수를 추출(feature extraction)
  - 정답을 예측하는 과정에서 feature를 update하며, 머신러닝에 비해 성능이 매우 뛰어남

![img_clf_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_2.jpg?raw=true)

- 세상 모든 데이터를 갖고 있다면 인공신경망이 필요없다고 생각할지도 모른다. 단순 검색 엔진만 만들면 된다는.

  - 하지만, 한 번의 검색에 필요한 시간 복잡도는 O(N)으로, 데이터가 무한이 존재하면 한번의 검색 조차 힘들어진다.

  ![img_clf_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_3.jpg?raw=true)

- 현실적으로 모든 데이터를 다루는 것이 힘들기에, 이미지를 판별할 수 있는 일종의 함수를 찾아낸다! 바로 인공신경망을 활용해서

  - 귀납적 방법으로 볼 수 있는데, 입력된 이미지가 갖는 페턴을 함수 내부적으로 찾아내 사용자가 원하는 Task를 수행하는 셈

![img_clf_4](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_4.jpg?raw=true)

###### Convolutional Neural Network

- 단순히 Fully Connected NN을 활용해 이미지 분류 모델을 구축할 경우, 각 히든 노드는 모든 픽셀에 대한 정보가 응축되어 있기 때문에, 아래 그림과 같이 크롭과 같은 정보 손실 문제가 발생하면 제대로 예측해낼 수 없음
- 현실 속에는 깔끔한 이미지만 있는 것이 아니기 때문에, 현실적으로 활용하기 어려운 모델

![img_clf_5](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_5.jpg?raw=true)

- 따라서, 합성곱 연산을 통해 이미지로부터 지엽적인 패턴을 파악하는 CNN이 고안됨

- 각 노드는 이미지의 일부분을 맡아 정보가 응축되어, 위처럼 크롭의 이슈가 발생하더라도 지엽적 패턴 파악을 통한 이미지 분류 가능

  ![img_clf_6](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_6.jpg?raw=true)

###### AlexNet

- 2012년 ILSVRC에서 1위를 차지한 모델
- 합성곱 연산과 풀링 연산이 반복되는 구조
  - 합성곱 필터 크기/stride: 11×11, 5×5, 3×3 / 1
    - 이전의 LeNet보다 크기가 큰 이미지를 입력받아, 더 큰 필터를 사용
  - 풀링 필터 크기/stride: 2 × 2 / 2
- 7개의 레이어, 605K개의 노드, 60M개의 파라미터
- 1.2M개의 학습 데이터를 활용
- ReLU, Dropout 활용

![img_clf_7](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_7.jpg?raw=true)

###### Receptive Field

- 입력된 이미지 일부가 합성곱/풀링 과정을 거쳐 한 픽셀로 맵핑되었을 떄, 일부 입력의 크기를 의미
- 즉, 각 필터가 입력 이미지의 어느 부분만큼 인식하는 지를 의미

![img_clf_8](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/img_clf_8.jpg?raw=true)

###### 그 이후

- VGGNet, GoogLeNet 등 등장
- 과거의 모델이 SOTA가 되는 과정에서 활용된 기법과 더불어 새로운 구조를 활용
- 점점 더 작은 필터 사이즈를 채택, 레이어 수를 늘려서 receptive field를 넓히고 파라미터 수를 줄임
- GPU 환경이 나날이 발전하기 때문에 모델도 덩달아 빠르게 진화하는 중!

> Annotation Data Efficient Learning

###### Data Augmentation

- 양질의 이미지를 얻기는 어렵고 고비용 => 기존의 이미지를 변형하여 학습 데이터로 활용
- 이미지의 밝기 조절, 회전, 대칭, 크롭 등을 통해 이미지를 변형
- OpenCV, NumPy로 편하게 증강할 수 있음!

###### Affine Transformation

- 각 line 간 비율을 유지하면서 다른 line으로 변환
- 정사각형을 평행사변형으로 변형하는 상황을 떠올리면 편하다!
- 이렇게 이미지를 기하학적으로 변형하는 작업을 warping이라 부름

###### CutMix

- 이미지를 혼합하는 방법

- 혼합한 비율에 따라 레이블도 확률적으로 부여

  ![aug_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_1.jpg?raw=true)

###### Transfer Learning

- '이미지 데이터는 얻기 어렵다', 즉 각 태스트에 맞는 대량 이미지를 얻는 것이 현실적으로 어렵다
- Big Model: 사전 학습된 모델을 추가적으로 학습하는 방식
- 사전 학습된 모델의 학습 파라미터는 고정(freeze)해두고 특정 태스크를 위해 추가 구성한 부분만을 추가 학습

![aug_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_2.jpg?raw=true)

###### Fine Tuning

- Transfer Learning과 같이 사전 학습된 모델을 활용
- 사전 학습된 모델에 낮은 learning rate를, 추가 구성한 부분에는 높은 learning rate를 부여하여 추가 학습

![aug_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_3.jpg?raw=true)

###### Knowledge Distillation

- 경량화: 사전 학습한 모델과 비슷한 '가벼운 모델'을 만드는 기법
- 사전학습 모델을 Teacher 모델로, Student 모델의 '가벼운 모델'을 학습

- 레이블을 활용하지 않는 경우
  - Student 모델이 Teacher 모델의 Inference와 가까워지도록 학습
  - 손실함수: 쿨백-라이블러 발산

![aug_4](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_4.jpg?raw=true)

- 레이블을 활용할 경우

  - Teacher 모델의 Inference와 Ground Truth를 모두 참고하여 학습. 즉, Teacher 모델을 모사함과 동시에 정확성을 높이는 셈
  - Teacher 모델의 Inference와의 괴리와 Ground Truth와의 괴리를 가중합한 Loss를 바탕으로 역전파, Student 모델을 업데이트

  ![aug_5](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_5.jpg?raw=true)

  - Soft Label(T=t)

![aug_6](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_6.jpg?raw=true)

###### Semi-supervised Learning

- 레이블이 있는 데이터와 그렇지 않은 데이터를 모두 활용하는 학습 기법
- 레이블이 있는 데이터로 모델을 학습한 뒤, 학습한 모델을 통해 레이블이 없는 데이터를 추론하여 Pseudo Label을 부여
- Pseudo Label이 부여된 데이터를 모델에 다시 학습

![aug_7](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_7.jpg?raw=true)

###### Self-training

- Augmentation + Teacher-Student Network + Semi-supervised Learning

- 사전학습된 Teacher Model을 통해 Pseudo Labeling을 진행, 사용 가능한 모든 데이터를 통해 Student Model을 학습

- Knowledge Distillation의 학습 방식과 같이, Ground Truth와 Teacher Model의 Inference를 모두 고려하여 학습

- Student 모델이 Teacher Model의 성능을 넘을 경우, 해당 모델을 Teacher Model로 대체. 다시 위 과정을 반복!

  ![aug_8](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week07/aug_8.jpg?raw=true)