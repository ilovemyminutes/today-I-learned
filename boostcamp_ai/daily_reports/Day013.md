# Day 13. Convolutional Neural Network | 최성준, 최성철 마스터

> CNN은 무엇인가?

Convolution

- 연속형: <img src='https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%28f+%2A+g%29%28t%29+%3D++%5Cint+f%28%5Ctau%29+g%28t+-+%5Ctau%29d%5Ctau+%3D+%5Cint+f%28t+-+%5Ctau%29g%28t%29+d+%5Ctau'>

- 이산형: <img src='https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%28f+%2A+g%29%28t%29+%3D++%5Csum_%7Bi%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D+f%28i%29+g%28t+-+i%29+%3D+%5Csum_%7Bi%3D-%5Cinfty%7D%5E%7B%5Cinfty%7D+f%28t+-+i%29g%28i%29'>
- 2D Image Conv: <img src='https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%28I+%2A+K%29%28i%2C+j+%29+%3D+%5Csum_%7Bm%7D+%5Csum_%7Bn%7D+I%28m%2C+n%29K%28i+-+m%2C+j+-+n%29+%3D+%5Csum_%7Bm%7D+%5Csum_%7Bn%7D+I%28i+-+m%2C+i-n%29K%28m%2C+n%29'>

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/2d_conv.png?raw=true">

- 2D Conv in action: 필터의 가중치에 따라 convolution 결과가 크게 달라질 수 있다

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/2d_conv_in_action.png?raw=true">

RGB Image Convolution

- 일반적으로 다루는 이미지는 색깔이 포함된 3차원 데이터

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/conv_RGB.png?raw=true">

- 여러 개의 필터를 가지고 Conv 연산을 진행하면 그 갯수만큼이 채널인 feature map이 출력

  - 입력층의 shape에 맞게 필터 사이즈를 결정, 이후에도 필터의 Conv 연산에 따라 출력된 결과의 shape를 고려하여 필터의 크기 결정
  - 파라미터 수는 아래 그림 기준으로 (5×5×3)×4 + (5×5×4)×10  <- 3은 입력 데이터의 채널 수, 4는 히든레이어의 채널 수와 같지!

  ![image-20210203103542531](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/conv_RGB2.png?raw=true)

Convolutional Neural Networks

- CNN은 convolutional layer, pooling layer, fully connected layer
  - convolutional layer, pooling layer: 특징을 추출하기 위한 레이어
  - fully connected layer: 문제에 맞는 의사결정을 위한 레이어
  - 가장 고전적인 형태 - 현재는 fully connected layer를 없애거나 최소화 하는 분위기
    - 학습하고자 하는 파라미터 수가 많아질수록 generalization이 떨어짐
    - CNN의 발전: 최대한 모델을 Deep하게 가져가는 동시에 파라미터 수를 줄이기 위해 고군분투
    - 전체 파라미터 수가 몇 개인지 신경쓰면서 모델 짜자!

Stride

- 필터가 Conv 연산을 진행하면서 한번 이동할 때의 거리

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/stride.png?raw=true" />

Padding

- 출력값의 shape 조정을 위해 입력값의 데이터 근처에 공백을 추가하는 작업

- 적절한 패딩 계수를 적용하면 출력값의 shape를 입력값의 shape와 맞출 수 있음

- 제로패딩: 공백에 채워지는 값이 0

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/padding.png?raw=true" />

Stride? Padding?

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/stride_padding.png?raw=true" />

Convolution Arithmetic

- padding=1, stride=1, 3×3×128 필터에 대해 다음의 모델의 파라미터 수는?

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/conv_arith.png?raw=true" />

- 필터 하나당 파라미터 수는 3×3×128개, 출력값의 채널이 64개이기 때문에 필터는 총 64개 => 파라미터 수: (3×3×128)×64=73728

  - 오로지 필터가 쥐고 있는 파라미터만 생각하면 된다!
  - 네트워크 모양만 봐도 이게 1만 단위, 10만 단위인지 대충 감이 올 수 있도록 연습!

- Exercise: AlexNet의 파라미터를 세어봅시다

  ![image-20210203113452375](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/n_params_alexnet.jpg?raw=true)

  - 딱 봐도 Dense 레이어의 파라미터 수가 Conv 레이어에 비해 압도적으로 많음
    - Convolution operator가 이미지의 모든 영역에 대해 같은 가중치를 공유하기 때문에 가중치가 훨씬 적음
    - 인공신경망의 성능을 높이기 위해 파라미터를 줄이는 것이 중요한데, 때문에 fully connected 레이어를 최소화하려고 하는 것이 트렌드

1 × 1 Convolution

- 파라미터 수를 줄이기 위해 활용하는 것이 바로 1×1 Conv layer
- Dimension reduction을 위해, 즉 채널 수를 줄이기 위해 사용
  - Conv 레이어를 더 깊게 쌓을 수 있는 것과 동시에 파라미터 수를 줄일 수 있게 된다
  - 예) bottleneck architecture

> Modern CNN: 최근으로 거슬러 올라올 수록 네트워크의 depth는 깊어지고, 파라미터 수는 줄어들고, 성능은 올라간다

ILSVRC: ImageNet Large-Scale Visual Recognition Challenge

- Classification / Detection / Localization / Segmentation

- 1000개 카테고리

- 100만 장이 넘는 이미지

- 45,657개 학습 이미지

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/ILSVRC.png?raw=true" />

- 2015년도 이후 인간을 뛰어넘는 성능을 얻음

AlexNet

![image-20210203115430790](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/alex.png?raw=true)

- 이중 구조: 당시 GPU가 부족했기 때문에 모델 구조가 2개로 나뉘어져 있음
- input이 11×11: 11이라는 숫자는 그리 좋지 않음
  - 하나의 Conv 커널이 이미지로부터 볼 수 있는 영역은 커지지만 필요한 파라미터 수가 많아짐(채널이 1이어도 121개의 파라미터 요구)
- 총 8개의 레이어로 이루어짐
- 핵심 아이디어
  - ReLU 활성화 함수 채택: 이유가 뭐가 됐든 좋은 활성화 함수다! 레이어가 깊어졌을 떄 망칠 수 있는 요소가 별로 없음
  - GPU 사용(2개의 GPU 활용)
  - Local response normalization(LRN), Overlapping pooling
    - LRN: 어떤 입력 공간에서 reponse가 많이 나오면 몇 개를 죽임 <- [?] 좀더 찾아보자
  - Data augmentation
  - Dropout
  - 지금 보면 흔한 방법으로 보이나, 당시에는 당연하지 않았음. 이 패러다임을 열어준 멋진 조상님
- ReLU Activation
  - 선형 모델이 갖고 있는 좋은 성질을 가짐: 그래디언트가 0보다 크면 값을 그대로 갖고 있을 수 있고, 0보다 작으면 0을 리턴 
    - [?] 0보다 작은 그래디언트를 0으로 리턴해준다는게 구체적으로 느낌이 안와
  - 경사하강법을 활용한 학습(최적화)이 쉬움
  - Good generalization <- 결과론적
  - Vanishing Gradient 문제 극복
    - 기존의 활성화 함수는 0을 기점으로 값이 커지거나 작아지면 기울기가 0에 가까워지는 현상 발생 -> vanishing gradient 문제 발생
    - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/sigmoid_tanh.png?raw=true" />

VGGNet

- 핵심 아이디어
  - **3×3 Conv 필터만 사용**(stride는 1)하여 depth를 늘림
  - 1×1 Conv for fully connected layers
  - Dropout(0.5)
  - VGG16(16개 레이어), VGG19(19개 레이어)

- 왜 3×3 필터를 사용했을까?

  - 필터의 크기가 클 때 갖는 이점: 입력 데이터에 대해 하나의 필터가 더 많은 정보를 쥘 수 있음(receptive field)

    - [?] Receptive Field: 하나의 필터가 Convolution 연산을 진행함에 있어 고려할 수 있는 입력 데이터의 크기

  - 3×3 필터로 2번의 Convolution을 취하는 것과 입력 데이터의 5×5의 픽셀값이 합쳐진 것과 같음

    - 즉, 3×3 필터를 2번 사용하는 것과, 5×5 필터를 1번 사용하는 것의 receptive filter는 같음

    - 하지만! 파라미터의 개수를 세어보면 **3×3 필터를 사용했을 경우 파라미터 수가 현저히 적다**

      <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/vgg.png?raw=true" />

    - (3×3)×2 보다 5×5가 더 크니 당연한 결과

  - 즉, 3×3 필터를 여러 개 활용하는 것이 5×5 필터를 활용하는 것보다 파라미터 수가 적고 spatial receptive field는 같다

  - 이러한 파라미터 수의 이득에 따라 최근의 모델들은 필터 사이즈가 거의 7×7을 넘어가지 않음

GoogLeNet

- 1×1 Conv: Dimension reduction의 이점(채널 수)

  - VGGNet에서 3×3 필터를 통해 파라미터 수에 대해 이득을 취한 것처럼, 1×1 필터를 사용하면 파라미터 수에 대한 이득을 취할 수 있음

- VGGNet에 비해 늘어난 22개 레이어 활용

  ![image-20210203123029653](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/googlenet.png?raw=true)

  - 비슷해보이는 구조가 여러 번 반복(network in network, NIN)

- 핵심 아이디어: Inception blocks

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/inception.png?raw=true" />

  - Inception block의 이점

    - 퍼져서 학습이 진행되다가 Concatenation되는 구조

      - 하나의 입력에 대해서 여러 receptive field를 갖는 필터를 사용, 이후 concat을 통해 여러 reponse를 연결

    - 핵심은 중간중간 **1×1 Conv가 포함**됐다는 점!

      - 전체적인 파라미터 수를 줄이게 됨: channel-wise dimension reduction의 효과

      <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/n_params_inception.png?raw=true" />

      - 채널 방향으로 dimension을 줄이니 파라미터 수가 확연히 줄어듦(30% 가량 감소)

앞선 모델과 파라미터 수 비교

- AlexNet(8개 레이어): 60M
- VGGNet(19개 레이어): 110M
- GoogLeNet(22개 레이어): 4M (쫙 줄었네)

ResNet

- 파라미터 수가 많아지면 발생하는 문제

  - Overfitting: Training error는 줄어드는데 Test error는 증가하는 현상

  - Overfitting이 아닌 다음의 문제도 존재

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/many_params_problem.png?raw=true" />

    - overfitting은 아니지만, 파라미터 수를 늘리기 전보다 학습/테스트 성능이 떨어지는 문제 발생(=학습이 제대로 안됨)

- 핵심 아이디어

  - Identity map(=residual connection)를 추가한 것(skip connection)   

    - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/identity_map.png?raw=true" />
      - 레이어에 의해 출력된 f(x)에 입력 데이터 x를 더함으로써 입력 데이터와 출력값의 차이(residual)에 대한 학습을 진행하게 됨
    - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/identity_map2.png?raw=true" />
      - Identity map을 추가하지 않으면 레이어를 추가해도 성능 향상이 없으나, Identity map을 추가하면 레이어 추가에 따라 성능이 향상되는 모습
    - Connection & Batch normalization
    - ![image-20210203125856102](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/identity_map3.png?raw=true)
      - 입력 데이터를 더하기 위해서는 차원을 맞춰줘야 하기 때문에 1×1 Conv를 활용
      - Batch Norm을 ReLU 뒤에 넣는 것이 좋다, Conv 뒤에 넣는 것이 좋다 재밌는 논란이 많음. 여기서는 Conv 뒤에 삽입

  - Bottleneck architecture

    - 3×3 Conv 전에 1×1 필터를 통해 input 채널을 줄이고, 3×3 Conv 이후에 1×1 필터로 input 채널을 늘리는 형태(입력 데이터와의 채널 차원을 맞추기 위해)

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/bottleneck.png?raw=true" />

- 이러한 특성들로 파라미터 수는 줄고, 성능은 높아졌다.

DenseNet

- 핵심 아이디어

  - Dense Block: 더하지 말고 concatenation을 하자!

    - 입력 데이터의 spatial 채널과 출력 데이터의 spatial 채널이 같으니 concat도 무리가 없음
    - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/densenet.png?raw=true" />
    - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/densenet2.png?raw=true" />
    - 문제: concat의 특성상 모델이 깊어질 수록 채널이 점점 커짐 -> Convolutional feature map도 커져서 파라미터가 많이 요구됨 => 차원을 중간중간 줄여줘서 해결!

  - Transition Block: concat으로 늘어난 차원을 줄이자!

    - Dimension Reduction: Batch Norm -> 1×1 Conv -> 2×2 AvgPooling

  - 아래 그림과 같이 Dense block, transition block을 반복하며 학습 진행

    ![image-20210203131003652](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/densenet3.png?raw=true)

요약하자면?

- VGG: 3×3 필터의 활용
- GoogLeNet: 1×1 Convolution 활용
- ResNet: skip-connection
- DenseNet: concatenation

> Computer Vision Applications: Semantic Segmentation and Detection

Semantic Segmentation

- 픽셀 단위 분류 문제
- Dense Classification, per-pixel classification 등으로도 불림
  - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/semantic_segmentation.png?raw=true" />

- 자율주행 문제에 많이 활용됨
  - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/auto_driving.png?raw=true" />

Fully Convolutional Network

- 이건 일반적인 CNN 구조

  - <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/fully_conv_net.png?raw=true" />

- Dense 레이어를 없앤 Fully Convolutional Network는 다음과 같은 구조

  - ![image-20210203151255734](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/fully_conv_net2.png?raw=true)

- 장점: Dense 레이어가 없어짐 

  - 근데 사실, 파라미터수는 Conv 레이어로 바꾸나 Dense로 하나 똑같음

    - Dense: 4×4×16인 입력에 대해 의 경우 flatten한 뒤 길이가 256인 Dense레이어를 거쳐 길이가 10인 출력층 도달
      => 파라미터 수: 2560개
    - Conv: flatten하지 않고 4×4×16인 커널을 만들어서 길이가 10인 출력층 도달
      => 파라미터 수: 2560개
    - 이러한 작업을 convolutionalization이라고 부름

  - 그럼 왜! convolutionalization을 하는 걸까?

    - classfication net에서 Fully connected 레이어를 convolution  레이어로 변환하는 것은 **결과값을 히트맵으로 바라볼 수 있는 효과**가 있음

      <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/fully_conv_net3.png?raw=true" />

    - 더 큰 이미지, 즉 입력 데이터의 크기에 관계 없이 모델을 돌릴 수 있음

    - output의 크기가 커지게 되면 convolution의 shared paramter의 특성 상 덩달아 레이어의 크기도 커지게 됨(입력 데이터의 채널 수에 따라 함께 커지는 커널 사이즈를 떠올리면 편해)

    - 주의해야할 점: 원래 입력했던 이미지에 비해 출력 이미지의 크기는 현저히 작음.

      - 하지만, 히트맵과 같은 결과값을 얻을 수 있는 것은 Semantic Segmentation의 가능성을 제공해주는 데에서 의미가 있지

    - Fully Convolution Network는 어떠한 input에 대해서도 학습할 수 있으나, output dimension이 일반적으로 감소함

      - 따라서, 이러한 output dimension을 보강할 방법이 필요하다 이말이야.

        <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/fully_conv_net4.png?raw=true" />

      - Deconvolution, Unpooling 등

Deconvolution: conv transpose

- Convolution의 역연산

  ![image-20210203153251951](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/deconvolution.png?raw=true)

- 근데 사실, convolution의 역연산은 존재할 수가 없다.

  - convolution은 어떤 여러 값을 연산을 통해 통합하는 과정인데, 이미 통합된 값을 여러 값으로 쪼갤 수는 없겠지
  - 근데 이렇게 생각하면 네트워크 아키텍쳐를 짤 때, convolution의 역이라고 생각하면 편함

- <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/deconvolution2.png?raw=true" />

  - 입력값에 대해 dilation과 padding을 씌우고 연산하여 shape를 키우는 것(convolution의 엄밀한 역이 아님)

  - 결과가 꽤 멋지게 나옴!

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/deconvolution3.png?raw=true" />

  - [?] 이와 관련해서 좀더 알아봐야겠다.

Detection: R-CNN

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rcnn.png?raw=true" />

- 학습 원리

  1. 입력 이미지를 받아 이미지 내 2000개의 패치(patch, proposal)를 추출(selective search 방법을 활용)

     - 각 이미지 패치의 크기는 랜덤임

  2. 각 패치를 일정한 크기로 통일

  3. AlexNet에 학습

  4. SVM을 활용한 분류

     - 바운딩 박스에 대한 regression을 포함

     - [?] 분류를 위한 레이블링은 어떻게?

  - 단순무식한 방법의 냄새

- 단점: 하나의 이미지 안에서 2000개의 패치를 뽑아 CNN에 넣어야 한다는 점. 이미지 하나에 대한 detection을 하기까지 1분이 걸려버림

Detection: SPPNet

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/sppnet.png?raw=true" />

- R-CNN과 비슷한 맥락인데, CNN을 단 한번만 학습
- 이미지 전체에 대한 convolutional feature map을 만들고 이미지 내 뽑은 패치의 바운더리에 해당하는 곳의 텐서만 뽑아오자
- Sub-Tensor를 뽑는 것이 관건 -> 한 번의 CNN 모델 구동으로 detection 속도가 더 빠름
- *Spatial pyramid pooling*이 핵심

Detection: Fast R-CNN

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/fast_rcnn.png?raw=true" />

- 학습 원리
  1. 한 이미지로부터 2000개의 바운딩 박스를 추출
  2. 이미지 전체에 대한 Covolutional feature map을 생성
  3. 각각의 region에 대해 fixed lenght feature를 ROI 풀링을 통해 추출(SPPNet과 유사)
  4. 바운딩 박스에 대한 레이블과 바운딩박스 regressor의 두가지 출력값을 얻을 수 있음

Detection: Faster R-CNN

<img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/faster_rcnn.png?raw=true" />

- Region Proposal Network(RPN) + Fast R-CNN

  - RPN: 바운딩 박스를 뽑아내는 것 또한 학습시켜버리자!

- Region Proposal Network

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/region_proposal_net.png?raw=true" />

  - 임의의 패치가 바운딩 박스로서 의미가 있는지, 즉 물체가 있을 것인지를 판별해주는 역할(region proposal)

  - anchor box: 미리 정해놓은 바운딩 박스의 크기. 템플릿을 만들어두는거지

    - 이러한 anchor box에 대한 offset이 얼만큼인지를 찾고, 이 박스를 미리 고정해두는 것이 목적

  - RPN 과젱에서 Fully Conv 레이어가 활용됨

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/region_proposal_net2.png?raw=true" />

    - 9: 사전 정의된 anchore 박스 9개 - 한 변의 길이가 (128, 256, 512), (1:1, 1:2, 2:1)의 각각 다른 비율
    - 4: 바운딩 박스를 얼마나 키우고 줄일지를 조절하는 bounding box regression parameter 4개
      - width, heigt, x방향/y방향 평행이동값(offset)의 총 4개 파라미터
    - 2: 바운딩 박스가 쓸모있는지 없는지 분류하는 yes/no의 2개 짜리 파라미터

Detection: YOLO - *You Only Look Once*

- 매우 빠른 object detection 알고리즘(basline - 45fps, smaller - 155fps)
  
- R-CNN 계열의 모델과 달리, 이미지를 한번 입력받아 바로 결과를 내주는 식이기 때문에 아주 빠름
  
- 학습원리

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/yolo.png?raw=true" />

  1. 이미지를 S×S 그리드로 분할
     - 오브젝트의 중앙이 특정 그리드 내에 놓여져 있을 경우, 해당 그리드는 디텍션을 맡게됨(바운딩 박스가 어떻고, 물체가 무엇인지)
  2. B개의 바운딩 박스 각각에 대한 예측을 진행(바운딩 박스의 결정 + 박스에 대한 가치판단)
     - 바운딩 박스 각각은 width, height, offset x, offset y를 찾아 box refinement 작업을 진행 & 오브젝트가 있는지 없는지(=해당 바운딩 박스가 쓸모있는지 없는지)에 대한 confidence 리턴
  3. 각각의 셀은 C개의 클래스에 대한 확률을 예측
  4. 2와 3의 결과를 취합하여 바운딩 박스 내에 오브젝트를 분류하여 예측해주는 것!

  - 모두 취합하고나면 크기가 S×S×(B*5+C)인 텐서가 완성됨
    - S×S: 그리드 내 셀의 갯수
    - B*5: (x, y, width, height)의 B개의 바운딩 박스와 confidence
    - C: 클래스 갯수

- [?] 레이블링은 어떻게 하는거지? 이미 수천 개를 다 해놓는 건가?



> CNN - 강아지 종류 분류하기

- [Stanford Dogs Dataset](http://vision.stanford.edu/aditya86/ImageNetDogs/main.html)
- Notebook - [Collect Stanford Dog Dataset](https://colab.research.google.com/drive/1fyNSK4PSFIXDbo6HTf1xnjhq7BUVpwW4?usp=sharing)
  - 데이터를 수집하는 과정에서 함수들은 자주 사용하면 손에 익을 것 같으니, 흐름을 기억해두자!
- Notebook - [Build CNN based classification for dog dataset](https://colab.research.google.com/drive/10HBNfen3hy2zyYvIvojFiIhOHXszDaLr?usp=sharing)
  - torch를 손에 익을 필요가 있음
  - 계속 다른 데이터에 적용해보면서 클래스를 구성하는 요령을 익히자.

> Attitude & Tips

- 인공신경망의 파라미터 수를 잘 세어보자!
- [?] Receptive Field

- 모델 발전의 흐름을 아는 것이 중요함!
- 앞서 나온 것들을 짬뽕해서 더 나은 모델이 나오는 경우가 많음
- 기존의 방법을 활용하여 *'이 방향으로 가면 더 문제 해결이 쉽지않을까?'*의 의문을 통해 발전된 모델이 나온 셈