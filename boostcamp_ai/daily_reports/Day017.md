# Day 17. Recurrent Neural Network | 주재걸 마스터

> Basic of RNN

![rolled, unrolled](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/rolled,%20unrolled.jpg?raw=true)

- 일반적인 Neural Network와 달리, 모델 내부에서 재귀적으로 갱신되는 hidden state가 존재
- Unrolled diagram을 보면 내부에 입력 시퀀스 수만큼 W가 있는 것으로 보이나, 사실은 하나의 W를 공유하는 형태(rolled diagram)

Hidden state 계산 방법?

- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+h_%7Bt-1%7D): 이전 step의 hidden state. 첫 번째 hidden state는 영벡터
- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+x_%7Bt%7D): 현재 step의 입력 벡터
- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+h_%7Bt%7D): 현재 step의 hidden state
- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+f_W): 파라미터 W에 대한 RNN의 함수
- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+y_t): 현재 step의 출력 벡터

![rnn_update_h](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/rnn_update_h.jpg?raw=true)

- 위 그림과 같이 hidden state의 업데이트 과정에서 크게 W(hh), W(xh)의 두 종류의 weight가 필요. 행렬 연산의 관점에서 봤을 때, 두 weight 행렬을 concat한 형태로 연산이 가능!

RNN의 종류

- One-to-one: 단일 입력에 대해 단일 출력 발생. 시퀀스의 개념이 들어가지 않아 일반적인 Neural Network와 같음
- One-to-many: 단일 입력에 대해 다중 출력 발생. 이미지 캡셔닝 기술이 이에 해당. 특정 데이터가 입력되었을 때, 시퀀스 형태로 결과값을 출력하는 것.
- Many-to-one: 다중 입력에 대해 단일 출력 발생. 텍스트 기반의 감성 분석이 이에 해당. 시퀀스 데이터로부터 패턴을 파악하여 분류/회귀 문제 등을 해결
- Many-to-many - 입력 후 출력: 다중 입력에 대해 다중 출력 발생. Machine Translation이 이에 해당. 시퀀스 데이터를 입력받아 차례대로 시퀀스 데이터를 출력하는 방법
- Many-to-many - 입력과 동시에 출력: 다중 입력에 대해 다중 출력 발생. 프레임 단위의 영상 캡셔닝이 이에 해당. 입력이 발생할 때마다 딜레이 없이 출력값을 내놓는 방법

Character-level Language Model

- 예) 'hello' 시퀀스 데이터에 대한 학습과 추론

  - 학습

  1. 'h', 'e', 'l', 'o'의 4가지 문자를 원핫인코딩: h-[1,0,0,0], e-[0,1,0,0], l-[0,0,1,0], o-[0,0,0,1]

  2. ![hello_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/hello_1.jpg?raw=true)

     W_hh는 이전 hidden state를 h_t로 귀속, W_xh는 현재 x_t를 h_t로 귀속시키는 역할을 수행

  3. ![hello_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/hello_2.jpg?raw=true)

     위 그림과 같이 hidden layer와 output layer를 거쳐 output을 내놓고, loss를 계산하여 weight를 업데이트하게 됨. 입력값의 'l'가 2번 연속으로 입력되었음에도 각각 'l', 'o'로 ground truth가 다른데, 재귀적으로 update되는 hidden state가 이러한 부분에서 올바른 예측을 유도하게 됨

  - 추론

    ![hello_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/hello_3.jpg?raw=true)

    inference 단계에서는 위와 같이 단일 결과값을 다음 step의 input으로 활용함으로서 다중 output을 만들게 됨. 하나의 output을 가지고서 장기간의 미래 시점까지 예측을 할 수 있게 됨

Backpropagation Through Time: BPTT

![bptt_truncation_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/bptt_truncation_1.jpg?raw=true)

- 위 그림과 같이 시퀀스의 길이가 지나치게 길어질 경우 모든 loss를 계산하고 weight를 업데이트하는 것이 컴퓨터 메모리 한계로 인해 어렵거나 불가능할 수 있음

![bptt_truncation_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/bptt_truncation_2.jpg?raw=true)

- 그럴 때 활용하는 것이 **Truncation**. 시퀀스의 모든 step에 대해 loss값을 계산하는 것이 아닌, 여러 개의 구간으로 나누어 loss를 구간별로 계산 및 weight update 진행

Interpretable Cells

![interpretation_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/interpretation_1.jpg?raw=true)

- RNN 모델이 일반적인 NN과 다른 점은 재귀적으로 작동하는 hidden state의 존재
- hidden state가 어떻게 업데이트 되는 지 패턴을 파악함으로써 hidden state의 어떤 차원이 시퀀스 데이터의 어떤 특징을 파악하고 있는지 해석할 수 있음

![interpretation_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/interpretation_2.jpg?raw=true)

- 위 그림은 hidden state의 특정 차원에 대한 value를 시각화한 것인데, 큰따옴표를 기준으로 값의 폭이 크게 달라지는 것으로 보아 큰따옴표의 시작과 끝에 주목하는 차원이라는 점을 알 수 있음

Gradient Vanishing/Exploding: RNN의 고질적인 문제

![gradient_vanishing](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/gradient_vanishing.jpg?raw=true)

- Hidden state가 과거의 정보를 기억하는 것 까지는 좋다 이말이야
- 근데, 학습하는 과정에서, 특히 BPTT의 과정에서 gradient가 지나치게 왜곡되는 문제가 발생
  - Gradient Vanishing: 가중치의 각 value의 절댓값이 0과 1사이일 경우, BPTT가 진행됨에 따라 0으로 수렴하여 기울기가 소실되는 문제
  - Gradient Exploding: 가중치의 각 value의 절댓값이 1보다 클 경우, BPTT가 진행됨에 따라 무한대로 발산하여 기울기가 폭발하는 문제
- 이러한 고질적인 문제로 인해 Vanilla RNN 모델은 거의 사용하지 않고, 해당 문제를 해결한 LSTM, GRU 모델 등을 활용한다.

LSTM: Cell state면 만사 오케이!

![LSTM_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/LSTM_1.jpg?raw=true)

- Cell state를 도입함으로써 gradient vanishing 문제 해결
- input gate, forget gate, output gate, gate gate의 4가지 gate로 구성
  - Input gate: 무엇을 기억할 것인 지 결정
  - Forget gate: 무엇을 잊을 지 결정
  - Output gate: input gate와 forget gate로 도출된 값을 최종적으로 얼마나 드러낼 것인지 결정
  - Gate gate: 새로운 cell state에 얼만큼의 정보를 넣을 것인지 결정
- 핵심의 cell state 갱신 파트
  - ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+C_%7Bt%7D+%3D+f_%7Bt%7DC_%7Bt-1%7D+%2B+i_%7Bt%7D+%5Ctilde%7BC%7D_%7Bt%7D) 
  - f_t를 통해 이전 cell state로부터 무엇을 잊을지가 결정됨
  - i_t를 통해 현재의 어떤 정보를 현재 cell state에 반영할지 결정됨
  - 과거 정보와 현재 정보를 더함으로써 과거 정보를 연달아 전달할 수 있는 것!
  - 이러한 **덧셈 연산**이 Gradient Vanishing 문제를 해결하는, 즉 Gradient값을 유지하는 핵심!

GRU: Cell state도 필요 없어. Hidden state한테 다 맡겨!

![GRU](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/GRU.jpg?raw=true)

- 핵심은 hidden state가 LSTM의 cell state 역할까지 수행한다는 점
- (1-z_t), z_t를 활용한 가중합으로 hidden state가 갱신되는데, (1-z_t)h_t-1은 이전 hidden state로부터 어떤 값을 가져갈지를 결정, z_th^~t는 현재 정보로부터 어떤 것을 가져갈지를 결정
- 현재 가져가지 않을 정보의 빈 자리에 이전 hidden state를 채워넣는 것과 같은 느낌으로 이해하면 좋겠다



> Attitude & Tips

[?] LSTM torch 내부 구조

[?] Truncation, Chunk