# Day 32. Image Classification II, Semantic Segmentation | 오태현 마스터

> Image Classification

##### Deeper Model

- 좋은 성능: Receptive Field가 커지고, 비선형성이 높아짐에 따라 수용력(capacity)이 높아지기 때문
- 그렇다면, 항상 좋은 것이냐? *NO!*
  - 모델 구조가 깊어질 수록 Gradient Vanishing/Exploding 문제 발생 가능성 ↑
  - 높은 계산복잡도
  - Degradation problem: 아래 그림처럼 특정 시점부터 성능이 더이상 좋아지지 않는 구간 발생
    - NOTE. Test 에러가 특정 시점 이후로 '늘어나는 것이 아니므로' overfitting은 아님!
    - 모델의 깊이가 깊어질 수록 degradation의 위험이 높아지는데, 이를 잘 핸들링할 필요가 있음

![img_clf_II_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_1.jpg)

##### GoogLeNet

- 1세대 이미지 분류 딥러닝 모델들인 AlexNet, VGGNet을 넘어 등장한 모델
- 효율적인 컴퓨터 연산을 통해 더 깊은 네트워크를 구축(degradation 에러 해결)

Inception Module

- 아래 그림과 같이, 여러 합성곱 및 풀링 레이어에 대한 병렬적 연산을 수행
- 수행한 결과를 채널을 주축으로 concat

![img_clf_II_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_2.jpg)

- *'여러 레이어에 대해 연산을 수행하면... 컴퓨터 자원 너무 많이 드는거 아냐?'*

- 이를 해결하기 위해 1×1 필터를 채택한 Conv레이어를 중간 중간 추가

  ![img_clf_II_3](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_3.jpg)

  - 다음처럼 채널을 주축으로 Conv 연산을 수행. 활용한 필터 수가 출력의 채널이 되어, 채널을 줄일 수 있게 됨! => Bottleneck의 역할을 수행

![img_clf_II_4](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_4.jpg)

Inception Modules

- 아래 그림과 같이 GoogLeNet은 여러 개의 Inception Module이 스택된 형태
- *'스택만 잔뜩 되어 있으면 Gradient Vanishing 문제가 발생할 것 같은데..'*  => Auxiliary Classifier를 추가 구성하여 이러한 문제를 방지!

![img_clf_II_5](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_5.jpg)

Auxiliary Classifier

![img_clf_II_6](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_6.jpg)

- 각 Inception 블록마다 단계별로 loss값을 구하는, 즉 단계별로 inference를 해놓는 경로를 마련
- 최종 Output에 대한 loss와 Auxiliary Classifier로부터 얻은 loss를 모두 종합하여 역전파 수행 => Gradient Vanishing 문제 해결
- Gradient Vanishing 문제를 해결했기 때문에 깊은 모델로써 강점을 발휘할 수 있는 것

##### ResNet

- *'깊을 수록 모델 성능은 좋아진다'*

  - 2016년 SOTA ResNet은 깊숙한 모델 구조가 특징인데, 모델 구조가 깊을 수록 좋은 성능을 보였고, 152개 레이어를 쌓은 ResNet은 심지어 인간의 오차율을 넘음 

  ![img_clf_II_10](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_10.jpg)

- 과거 적은 레이어로 학습한 모델과 달리, ResNet은 훨씬 깊은 모습!

  ![img_clf_II_8](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_8.jpg)

Residual Connection

- 가설: 'x를 입력해 H(x)를 얻었을 때, x와 H(x) 간 관계를 파악하는 것은 어렵다'
- 출력된 H(x)를 레이어를 거쳐 나온 F(x)와 x의 합, 즉 H(x) = F(x) + x로 정의하면,
  - Target Func: H(x) = F(x) + x 에 대하여
  - Residual Func: F(x) = H(x) - x 라고 할 수 있음
  - 즉, Residual Connection에 포함된 레이어는 입력 x와 H(x)의 '잔차'의 관계에 대한 학습을 진행
  - Gradient Vanishing 문제 해결

![img_clf_II_9](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_9.jpg)

- 왜 잘될까? 왜 Gradient Vanishing 문제를 해결할까?

  ![img_clf_II_11](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_11.jpg)

  - 어떤 분석 논문에 따르면, Skip-Connection을 통해 Gradient가 전파될 수 있는 경로가 많아지기 떄문에 학습이 잘 이루어진다고 함. Skip-Connection을 하나 추가할 때마다 Gradient 전파경로의 경우의 수가 2배 증가하여, Gradient 전파에 대한 시간 복잡도는 O(2^n)

##### Overview: DenseNet

![img_clf_II_12](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_12.jpg)

- 각 레이어마다 출력된 Hidden State를 채널 축을 기준으로 모두 Concat하며 학습
  - Gradient Vanishing 문제 방지
  - 추상화 작업을 강화할 수 있음
  - feature의 재사용을 강화(초반 단계의 출력도 참조하므로)

##### Overview: SENet

![img_clf_II_13](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_13.jpg)

- Depth를 높이거나 별도의 Connection을 활용하는 것이 아닌, 주어진 Activation(Feature Map) 간의 관계를 더 뚜렷하게 파악할 수 있도록 설계한 것이 특징
- Attention 구조를 활용하여 각 채널에 대해 어떻게 집중해야할 지 정보를 파악
  - Squeeze: Activation의 공간적 정보를 없앤 뒤 각 채널의 분포를 얻는 과정
  - Excitation: 기존 Activation과 Squeeze결과의 곱을 통해 Activation의 채널에 가중치를 부여. 즉, Activation의 채널 간 관계성을 파악하는 과정

##### Overview: EfficientNet

![img_clf_II_14](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_14.jpg)

- 깊이를 달리하거나, 너비를 달리하거나, 입력 이미지의 해상도를 달리하거나 등 성능을 높이는 방법에 따라 Saturation Point가 다름

- 이러한 여러 방법들을 적절히 모두 활용한 것이 EfficientNet

  ![img_clf_II_15](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_15.jpg)

##### Overview: Deformable Convolution

![img_clf_II_16](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_16.jpg)

- 정적인 그리드가 아닌, 비정형 그리드를 활용하여 객체를 유연하게 파악

![img_clf_II_17](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\img_clf_II_17.jpg)

- 과거의 모델에 비해 현대 모델들은 파라미터 수가 적어 연산량이 적으나, 효율적인 구조를 구성하여 정확도는 높다

> Semantic Segmentation

##### Fully Convolutional Networks, FCN

- 입력부터 출력까지 모두 인공신경망으로만 구성된 엔드투엔드 구조
- 어떤 사이즈의 이미지도 입력할 수 있고, 입력 이미지와 동일한 크기의 Segmentation 결과를 얻을 수 있음
- Fully Connected 구조를 사용하지 않고 업샘플링을 적용하여 저해상도 문제를 어느 정도 해결했다는 특징이 있음

![seg_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_1.jpg)

1×1 Convolution

- FC 레이어는 근본적으로 공간적인 정보를 고려할 수 없음
- 때문에 FC 레이어를 사용하지 않고, 1×1 Conv 레이어를 활용하여 Spatial Information을 유지
- 덕분에 출력 결과로 아래와 같은 히트맵을 얻음

![seg_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_2.jpg)

- 1×1 Conv 레이어를 활용하면 아래와 같이 채널 간 압축 과정이 일어나는 것!

  - 이는 사실, 기존의 Feature Map을 채널을 주축으로 Flatten하여 FC 레이어를 적용하는 것과 같음

    ![seg_3](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_3.jpg)

  - 하지만 1×1 Conv 레이어를 적용할 경우 출력 해상도가 떨어지는 문제가 발생 => 업샘플링을 통해 해결

    ![seg_4](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_4.jpg)

Upsampling

- 마지막 Conv 레이어를 걷어내고, 입력 이미지의 크기와 같도록 출력 해상도를 높이는 작업
- Conv, Pooling 레이어를 걷어낼 수록 Receptive Field가 줄어들기 떄문에 이미지에 대한 컨텍스트 파악력이 떨어질 수 있음
- 트레이드 오프가 일정 부분 있기 때문에, 현명한 작업 필요! => Unpooling, Transposed Convolution, Upsample and Convolution

![seg_5](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_5.jpg)

- Transposed Convolution
  - 입력 이미지에 필터를 적용하여 사이즈를 크게 변환하는 방법
  - 엄밀한 의미에서 Deconvolution은 아니나, 느슨하게는 Deconvolution 작업이라 볼 수 있음
  - 연산 과정에서 중첩 문제가 발생하는데, 때문에 필터 사이즈와 Stride에 대한 튜닝이 필수적
  - 이러한 오버랩 이슈를 방지하기 위해, 학습 가능한 레이어를 마련하여 interpolation을 진행
    - 전통적인 interpolation 방식은 constant한 형태

Skipping Connection

![seg_6](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_6.jpg)

- 얕은 단계에서부터 깊은 단계까지 각각 Conv, Pooling 과정에서 얻은 Activation을 채널을 기준 축으로 Concat한 뒤 업샘플링을 진행
  - [?] 뇌피셜이지만, 여러 Activation을 활용함으로써 업샘플링의 한계를 최대한 보완할 수 있는듯?

##### UNet

![seg_7](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_7.jpg)

- 압축과정을 통해 추상적 특징을 파악하는 Contracting 단계와 함축된 정보의 해상도를 높이는 Expanding 단계로 구성

Contracting Path

![seg_8](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_8.jpg)

- 각 블록을 거칠 때마다 이미지의 길이가 절반으로 줄어듦
- 가장 마지막 블록의 출력에 잠재 특성이 응축된 것으로 가정

Expanding Path

![seg_9](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_9.jpg)

- 채널 수를 줄이면서 해상도를 복구해나가는 형태
- 블록을 거듭할 때마다 이미지 사이즈가 2배씩 늘어남
- 또한, Contracting 과정에서 대응되는 Activation을 Concat

Overall

![seg_10](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week07\seg_10.jpg)

- 중요한 것은, 이미지를 입력받아 압축과 팽창의 과정이 원활하게 진행될 수 있도록 각 이미지 사이즈를 모두 짝수로 유지해야 함
- 얕은 지점에서의 정보를 깊은 지점에 Concat함으로써 경계선 등 추상화 과정에서 놓칠 수 있는 Local한 정보를 되새겨줄 수 있음