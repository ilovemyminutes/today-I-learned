# Day 37. Timespace for ML, Compression

##### Space

- 모델, 또는 더 넓은 범주에서 문제를 효율적으로 해결하기 위해, Space를 따질 필요가 있음

- Search Space: 어떠한 현상을 탐색할 공간. 최종적인 의사결정을 위해 현상을 관찰하고 문제를 도출

- Problem Space: 문제를 정의할 공간. 어떤 문제가 있고, 어떻게 해결해야할 지

- Solution Space: Problem Space로서 국한된 공간 안에서, 최선의 해결책을 찾기 위한 바운더리 

  ![space_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\space_1.jpg)

  - 가령, 1 < x < 10, 2 < y < 10에 대하여 x + y를 최소화한다고 할 때, 가능한 x와 y가 Solution Space(=feasible space)를 이룸

    ![space_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\space_2.jpg)

  - 문제 해결을 위한 경우의 수는 매우 다양

- 이러한 Solution Space 속에서 문제를 효율적으로 풀어나가는 것을 모델 경량화와 연결지을 수 있지

- 최적의 해를 구할 수 있는 일반적인 방법이 있는데, 더 적은 파라미터 수를 가지고 해당 해를 좇을 수 있다면, 그것이 더 효율적인 방법이 되는 셈

##### Entropy

- 무질서한 정도를 나타냄. 좀더 자세하게 말하자면, 어떤 분포가 어느 정도의 무질서도를 갖고 있는지, 이에 따라 얼만큼의 정보를 획득할 수 있는지를 나타내는 것

  ![entropy_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\entropy_1.jpg)

- '모델이 데이터를 입력받아 학습을 한다'라는 상황에 적용해보자

  - 모델을 처음 학습할 때, 즉 initial state에서는 그 출력이 매우 무질서함. 랜덤한 weight를 바탕으로 선형/비선형 변환을 거쳐 출력값을 내놓았기 때문에 정보를 충분히 얻지 못한 것

  - 학습을 거듭할 수록 점점 질서정연한, 정보가 풍부한 출력을 내뱉음. 데이터에 기반해 weight는 점점 튜닝되었기 때문에, 규정된 패턴에 입각해 일정한 output을 내뱉게 됨

    ![entropy_2](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\entropy_2.jpg)

##### Hyper Parameter Search

- 사용자가 지정할 수 있는 파라미터셋에 대한 튜닝
- 모델 학습을 마치고 성능을 측정하기 때문에 Cost가 많이 들게 되므로 효율적인 탐색이 중요

Manual Search, Grid Search, Random Search

![hyper_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\hyper_1.png)

Surrogate Model

- 메인 모델의 파라미터 서치를 대신할(surrogate) 모델을 마련. 이 녀석을 학습한 뒤, 찾은 파라미터를 토대로 메인 모델을 학습

  ![hyper_2.png](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\hyper_2.png.jpg)

  

Neural Architecture Search, NAS

- 인공신경망 구조를 자동화 파리프라인을 통해 찾는 것

- 구성한 아키텍쳐를 학습/평가 이후 새로운 아키텍쳐 구축에 반영하기를 반복

- 딱봐도 인간이 설계했다고 보기 어려운 모델 구조를 얻음

  ![hyper_3](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\hyper_3.jpg)

##### NAS for edge devices

MnasNet

- 이전에는 하이퍼파라미터 서치 과정에서 하드웨어적 요소를 고려하지 않았다면, MnasNet은 엣지 디바이스의 하드웨어를 고려하여 서치를 진행

- 모델을 샘플링하는 과정에서 모델 Accuracy와 더불어 엣지 디바이스에 대한 latency를 고려

  ![nas_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\nas_1.jpg)

PROXYLESSNAS

- 이전에는 모델의 일부 레이어만을 추출하여 하이퍼파라미터 서치를 진행했으나, 해당 모델은 모델의 모든 구조를 고려

##### MobileNet

- Conv 레이어를 변형하여 엣지 디바이스에서 작동 가능한 모델
- Depth-wise Separable Convolution
  - 기존 Conv 방법은 (C, W, H)의 입력으로부터 채널이 C'인 출력을 얻고자 할 때, (C, W0, H0)의 필터를 C'개 활용
    - C' * C * M * N의 연산량이 발생(M, N는 각 커널이 가로 세로 방향으로 이동한 횟수)
  - DSC의 경우, 입력의 각 채널에 대해 (1, W, H)의 커널이 각 채널에 대응하여 Conv연산을 수행하고, 연산을 통해 얻은 (C, W', H') 이미지에 대해 C'개 (C, 1, 1) 커널로 Point-wise Conv를 적용
    - C * M * N + C' * C의 연산이 발생
  - 기존 Conv 대비 연산량이 획기적으로 줄어듦
  - 모델의 크기가 작을 때는 학습이 잘 되지 않을 수 있음

##### Compression

- 손실압축: 압축 이후 복원이 불가능한 경우(mp3, mp4 등)
- 비손실압축: 압축 이후 복원이 가능한 경우(zip, 7z 등)

"압축이 되었다"

- 압축을 위해 인코딩을 하는데, 인코딩 결과의 길이가 입력의 길이보다 짧을 경우 압축되었다고 함

- 디코딩을 통해 인코딩 결과를 기존 입력 형태로 복원

- 이러한 관점에서, 머신러닝 모델은 데이터를 입력받아 정보가 응축된 특정 결과로 인코딩하는 것이라고 볼 수 있음

  ![compression_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\compression_1.jpg)

##### Information Entropy

> *이러한 관점에서, 머신러닝 모델은 데이터를 입력받아 정보가 응축된 특정 결과로 인코딩하는 것이라고 볼 수 있음*

- 그렇다면, 정보는 어떻게 얻어내는 것일까?

- 무질서했던 분포를 질서정연하게 만들어내는 과정을 통해 정보를 얻는다고 볼 수 있음. 이러한 질서를 맞추는 과정은 역전파를 통해 실현

- KL Divergence Loss를 떠올려보자

  ![info_1](C:\Users\iloveslowfood\Documents\workspace\iloveTIL\boostcamp_ai\etc\images\week08\info_1.png)

  - 느슨한 의미에서, KL Divergence는 두 분포간의 거리를 의미한다고 볼 수 있음
  - KL Divergence는 Cross Entropy와 Entropy의 두 가지 항으로 구성
  - Cross Entropy와 Entropy의 괴리를 측정하는 것. 즉, 실제 세상이 지닌 무질서도(정보량)와 모델이 뱉은 가상의 세상이 지닌 무질서도를 비교하는 것
  - KL Divergence를 Loss로 채택하는 것은 곧 KL Divergence를 최소화하는 것을 의미하고, 이는 실제 세상의 무질서도와 가상 세상의 무질서도를 최대한 비슷하게 만든다는 것을 의미
  - 데이터 또한 실제 세상으로부터 얻은 표본이기 때문에, 엄밀하게는 Entropy 또한 Constant한 것으로 볼 수 없으나, 데이터가 실제 세상에 근사하다는 가정하에 Entropy를 Constant하다고 간주할 수 있음
  - 이에 따라 KL Divergence를 최소화하는 것은 Cross Entropy를 최소화하는 것과 같아짐
    - Cross Entropy의 최소화하는 것은 실제 레이블 P에 대해 Q 또한 같은 값을 갖도록 유도하는 것