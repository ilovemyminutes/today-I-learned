# Day 25. Graph Neural Network | 신기정 교수님

Graph Neural Network, GNN

- 귀납식 임베딩
  - 변환식 임베딩의 한계
    - 학습 이후, 새로운 정점에 대해서는 임베딩을 얻을 수 없음
    - 모든 정점에 대한 임베딩을 사전 계산해야 함
    - 정점의 속성 활용 불가
  - 귀납식: 변환식의 한계 극복
    - 새로운 정점도 임베딩 가능
    - '임베딩 함수'를 추출 => 모든 임베딩 사전 계산 필요 없음
    - 정점의 속성 활용 가능
    - Downstream task를 위한 학습의 경우 변환식 임베딩 방식보다 좋은 성능!

GNN의 구조

- 입력: 그래프, 정점의 속성
  - 그래프: 인접 행렬, ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+A_%7B%7CV%7C+%5Ctimes+%7CV%7C%7D)(|V|: 정점 수)
  - 정점의 속성: 차원이 속성 수인 벡터, ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X_%7Bu%7D)(u: 정점)
    - 예: 지역, 성별, 논문에 사용된 원핫 벡터, 각 정점의 중심성 등

임베딩을 얻는 과정

- 계산 그래프(computation graph): 이웃 정점들의 정보를 집계, 응축하는 과정을 반복

  ![gnn_basic_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/gnn_basic_1.jpg?raw=true)

  - 각 정점에서의 이웃 정점들을 활용하여 속성을 집계

    ![gnn_basic_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/gnn_basic_2.jpg?raw=true)

    - 따라서, 단계마다 정점별 집계되는 값이 다르게 측정, 즉 해당 정점에서의 이웃 정점 간 관계성을 반영할 수 있음
    - 몇 번의 집계 과정을 거칠지는 설정할 수 있음(하이퍼 파라이터)

  ![gnn_basic_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/gnn_basic_3.jpg?raw=true)

  - 층별 집계 함수의 학습 파라미터는 모두 공유됨

집계 함수

- 다음의 과정을 거침

  1. 이웃들 정보의 평균 계산
  2. 신경망에 적용

- k번째 레이어에서 정점 v의 임베딩은 다음과 같음

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+h_%7Bv%7D%5E%7Bk%7D+%3D+%5Csigma+%28W_%7Bk%7D+%5Csum_%7Bu+%5Cin+N%28v%29%7D+%5Cfrac+%7Bh_%7Bv%7D%5E%7Bk-1%7D%7D+%7B%7CN%28v%29%7C%7D+%2B+B_%7Bk%7Dh_%7Bv%7D%5E%7Bk-1%7D%29%2C+h_%7Bv%7D%5E0+%3D+x_%7Bv%7D)

  - W, B: 학습 파라미터
  - 이전 레이어에서의 임베딩 정보를 바탕으로 다음 레이어의 임베딩 벡터를 구하는 식
  - 초기화: 각 정점이 지닌 속성 벡터
  - bias에 포함되는 항은 residual connection의 역할을 하는 것 아닌가...(뇌피셜)

- 집계 함수는 여러 방법이 있는데, 어떤 집계 방법을 활용하느냐에 따라 다르게 학습됨(GCN, GraphSAGE 등)

손실 함수

![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+L+%3D+%5Csum+_%7B%28u%2C+v%29+%5Cin+V+%5Ctimes+V%7D+%7C%7C+z_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D+-+A_%7Bu%2C+v%7D%7C%7C%5E%7B2%7D)

- ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+z_%7Bu%7D%5E%7B%5Cintercal%7Dz_%7Bv%7D): 임베딩된 정점 u, v의 임베딩 벡터 간 유사도
- 역전파를 통해 손실 함수 최소화하는 방향으로 학습

Downstream task

- 적절한 손실함수를 활용하여 downstream task를 위한 학습 가능
- Classification: Cross Entropy 사용 등

Graph Convolutional Network, GNN

- 기본적인 집계 함수를 변형한 GNN

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+h_%7Bv%7D%5E%7Bk%7D+%3D+%5Csigma%28W_%7Bk%7D+%5Csum_%7Bu+%5Cin+N%28v%29+%5Ccup+v%7D+%5Cfrac+%7Bh_%7Bu%7D%5E%7Bk-1%7D%7D+%7B%5Csqrt+%7B%7CN%28u%29%7C%7CN%28v%29%7C%7D+%7D%29%2C+h_%7Bv%7D%5E%7B0%7D+%3D+x_%7Bv%7D)

  - 기존의 기준 정점에 대한 이웃 수만을 활용하여 정규화를 해준 것과 달리, 이웃 정점의 이웃 정점 수와 기준 정점의 이웃 정점 수의 기하 평균을 활용 => 정점 간 관계를 더 interactive하게 반영할 수 있음

GraphSAGE

- 기본적인 집계 함수를 변형한 GNN

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+h_%7Bv%7D%5E%7Bk%7D+%3D+%5Csigma%28%5BW_%7Bk%7D+%5Ccdot+AGG%28%5C%7Bh_%7Bu%7D%5E%7Bk-1%7D%2C+%5Cforall+%7Bu%7D+%5Cin+N%28v%29%5C%7D%29%5D%29)

  - AGG: 평균, 풀링, LSTM 등의 aggregation을 활용
  - 정규화 파라미터를 학습 가능하도록 하여 더욱 interactive한 학습이 가능해지는 것!

*'GNN은 음... CNN쓰면 되는거 아냐?'*

- 결론적으로, NO
- CNN은 합성곱 과정에서 주변 픽셀과의 관계성을 파악하게 되는데, 그래프에 활용되는 인접 행렬은 값의 '위치'는 별 의미가 없음 => CNN으로는 제대로 학습될 수가 없는 데이터 형태
  - 각 정점별 이웃 정점 수가 다르고, 이웃 정점의 이웃 정점들도 관계가 상이

Graph Attention Network, GAT

- Attention 메커니즘을 활용한 GNN

- h_k를 정규화하는 부분에 대해 Attention 메커니즘 기반의 학습 파라미터를 도입

- 과정: 정점 i의 정점 j에 대한 Self-attention을 구해보자

  1. ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctilde%7Bh_%7Bi%7D%7D+%3D+h_%7Bi%7DW): 해당 레이어의 정점 i의 임베딩에 Weight를 곱해 새로운 임베딩을 얻음

  2. 기준 정점 i와 각 정점의 임베딩 벡터를 concatenate한 뒤, 학습 가능한 attention 벡터 a와 내적. 다음은 정점 i와 j의 임베딩 벡터를 내적한 결과

     ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+e_%7Bij%7D+%3D+a%5E%7B%5Cintercal%7D%5BCONAT%28%5Ctilde%7Bh_%7Bi%7D%7D%2C++%5Ctilde%7Bh_%7Bj%7D%7D%29%5D)

  3. 구한 e 벡터에 소프트맥스를 취하면, 해당 벡터의 j번째 값(확률)이 정점 i와 j의 attention 값이 됨!

     ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Calpha_%7Bij%7D+%3D+softmax_%7Bj%7D%28e_%7Bij%7D%29+%3D+%5Cfrac+%7Bexp%28e_%7Bij%7D%29%7D+%7B%5Csum_%7Bk+%5Cin+N_%7Bi%7D%7D+exp%28e_%7Bik%7D%29%7D)

- 여러 어텐션 벡터를 학습에 활용(multi-head attention)
- 기존의 GCN 대비 향상된 성능!

Graph Embedding

- 정점 임베딩과 같은 맥락으로, 그래프 임베딩은 그래프 전체를 벡터 공간으로 임베딩하는 것을 의미

  ![graph_embedding_1](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_1.jpg?raw=true)

- Graph Pooling

  - 미분가능한 풀링(differentiable pooling, DiffPool)을 활용할 경우, 단순통계량을 활용할 경우보다 일반적으로 좋은 성능을 보임



지나친 획일화 문제

- 그래프 신경망의 레이어 수가 증가하면서 각 정점의 임베딩이 서로 유사해지는 현상

  ![graph_embedding_2](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_2.jpg?raw=true)

- 작은 세상 효과와 관련: 가령, 두 정점이 거리가 5로 떨어져 있는데도 작은 세상 효과와 같은 맥락으로 관계성이 높아지는, 즉 유사한 노드로 인식되는 경우

- 지나친 획일화 문제 발생시 Downstream task 수행 과정에서 정확도가 감소하는 문제 발생

  

지나친 획일화 문제 해결 #1 - Jumping Knowledge Network, JK 네트워크

- 마지막 레이어의 임베딩만 활용하지 않고, 모든 레이어에서 구해진 임베딩을 함께 사용

  

지나친 획일화 문제 해결 #2 - APPNP

- 0번째 층을 제외하고는 신경망 없이 집계 함수 단순화

  ![graph_embedding_3](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_3.jpg?raw=true)

- 레이어 수 증가에 따른 정확도 감소 문제 X

  ![graph_embedding_4](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_4.jpg?raw=true)

Graph Data Augmentation

- 데이터 내 누락되거나 부정확한 간선이 있을 수 있음

- 이러한 문제를 데이터 증강을 통해 해결

  ![graph_embedding_5](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_5.jpg?raw=true)

  1. 정점 간 유사도 측정
  2. 유사도가 높은 정점을 추출
  3. 유사도를 바탕으로 해당 정점 간 간선을 임의로 추가 부여

- 확실하게 정확도가 개선됨!

  ![graph_embedding_6](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week05/graph_embedding_6.jpg?raw=true)