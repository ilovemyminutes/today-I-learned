# 경사하강법 | 임성빈 교수님

> 경사하강법 기초

미분
$$[f'(x) = lim_{h→0} {\frac {f(x+ h) - f(x)} h}]$$

- 함수의 임의의 점에서의 접선의 기울기

  <img src="C:\Users\iloveslowfood\AppData\Roaming\Typora\typora-user-images\image-20210126100844313.png" alt="image-20210126100844313" style="zoom:67%;" />

  - 접선의 기울기를 통해 함수의 증감을 파악할 수 있다
  - 미분값이 음수: 특정 값에 미분값을 더해주면 함수값은 증가하게 됨
  - 미분값이 양수: 특정 값에 미분값을 더해주면 함수값은 증가하게 됨
  - 경사상승법(gradient ascend): 함수가 증가하는 방향으로 이동하도록 미분값을 더하는 것
  - 미분값을 뺀다면? 함수값이 감소하는 방향으로 x값이 이동하게 되겠지! => 경사하강법(graident descent)

- `sympy`(symbolic python) 라이브러리를 통해 미분을 진행할 수 있음

  ```python
  import sympy as sym
  from sympy.abc import x
  
  sym.diff(sym.poly(x**2 + 2*x + 3), x) # Poly(2*x + 2, x, domain='ZZ')
  ```

경사하강법 알고리즘

- 경사하강법을 위해서는 다음과 같은 요소가 필요할 것!

```python
var = init # 시작점
grad = gradient(var)

while abs(grad) > eps: # 실제로 미분값이 0이 되는 경우는 잘 없기 때문에 종료 조건을 걸어둬야 함
    var = var - lr * grad # 학습률을 통해 학습 속도를 조절
    grad = gradient(var) # 미분값 update
```

- 변수가 벡터(다변수)인 경우 편미분을 사용 => 특정 변수에 대한 기울기를 계산, *그레디언트 벡터*
  $$
  \nabla f = (\partial_{x1}f, \partial_{x2}f,\,... ,\,\partial_{xd}f)
  $$

  $$
  \partial_{x_{i}}f(\textrm{x}) = lim_{h→0} {\frac {f(\textrm{x}+ h\textrm{e}_{i}) - f(\textrm{x})} h}
  $$

  - 그레디언트 벡터는 각 점에서 가장 빨리 감소하게 되는 방향과 같다

- 벡터의 경사하강법은 다음과 같이 진행

```python
var = init # 시작점
grad = gradient(var)

while norm(grad) > eps: # 벡터이기 때문에 앞서 미분값의 크기를 측정한 절댓값 대신 노름(norm)을 사용함
    var = var - lr * grad # 학습률을 통해 학습 속도를 조절
    grad = gradient(var) # 미분값 update
```



> 경사하강법 심화

경사하강법으로 선형회귀 계수 구하기

- 선형회귀 목적식: 
  $$
  \lVert \textrm {y} - \textrm X\beta \rVert_{2}
  $$
  이를 최소화하는 β를 찾아야 함!

$$
\nabla_{\beta} \lVert \textrm {y} - \textrm X\beta \rVert_{2} \&= (\partial_{\beta_{1}}\lVert \textrm {y} - \textrm X\beta \rVert_{2},\,... \,, \partial_{\beta_{d}}\lVert \textrm {y} - \textrm X\beta \rVert_{2})
$$

- 적절한 유도를 통해 다음을 알 수 있음
  $$
  \nabla_{\beta_{k}} \lVert \textrm {y} - \textrm X\beta \rVert_{2} = \partial_{\beta_{k}}\{\frac 1 n \sum_{i=1}^{n} (y_{i} - \sum_{j=1}^{d} X_{ij}\beta_{j})^2 \} = - \frac{\textrm{X}^\intercal_{·k}(\textrm y - \textrm X \beta)}{n \lVert \textrm y - \textrm X \beta\rVert_{2}}
  $$

- 그러면!
  $$
  \nabla_{\beta} \lVert \textrm {y} - \textrm X\beta \rVert_{2} = (- \frac{\textrm{X}^\intercal_{·1}(\textrm y - \textrm X \beta)}{n \lVert \textrm y - \textrm X \beta\rVert_{2}},\, ...\, ,- \frac{\textrm{X}^\intercal_{·d}(\textrm y - \textrm X \beta)}{n \lVert \textrm y - \textrm X \beta\rVert_{2}})
  $$

  - 복잡해보이지만 사실 Xβ의 계수인 β를 미분한 결과가 X^T에 곱해진 꼴

- 목적식을 최소화하는 β를 구하는 경사하강법 알고리즘
  $$
  \beta^{\,t +1 } ← \beta^{\,t} + \frac {\lambda} {n} \frac {\textrm{X}^{\intercal}(\textrm y - \textrm X \beta^{\,t})}{\lVert \textrm y - \textrm X\beta^{\,t} \rVert}
  $$

  - Norm을 최소로하는 β를 찾나 Norm의 제곱값을 최소화하는 β를 찾나 같기 때문에, 계산의 편의를 위해 Norm을 제곱한 값을 최소화 하는 β를 찾는다.
    $$
    \beta^{\,t +1 } ← \beta^{\,t} + \frac {2\lambda} {n} \frac {\textrm{X}^{\intercal}(\textrm y - \textrm X \beta^{\,t})}{\lVert \textrm y - \textrm X\beta^{\,t} \rVert}
    $$

- 코드로 구현하면 다음과 같음

  - [?] 왜 이렇게 되지?

  ```python
  for t in range(T): # 지정한 루프에서만 학습을 진행(while문도 당연히 가능)
      error = y - X @ beta # Error Trem: 오차값
      grad = - transpose(X) @ error # 그레디언트
      beta -= lr*grad # 상수배를 통해 이동 거리를 조절
  ```

경사하강법의 한계

- 미분가능하고 볼록(convx)한 함수에 대해 적절한 학습률과 학습횟수를 선택했을 때 수렴 보장
- 선형회귀 문제는 회귀계수 β에 대해 볼록함수기 때문에 수렴이 보장
- 비선형회귀 문제는 목적식이 볼록하지 않아 수렴이 항상 보장되지는 않음

확률적 경사하강법(Stochastic Gradient Descent, SGD)

- 데이터를 1개 또는 일부(mini-batch)만을 활용하여 업데이트
- 볼록이 아닌(non-convex) 목적식은 SGD를 통해 최적화
- 일부 데이터를 통해 구한 그레디언트 벡터의 기댓값과 모든 데이터를 통해 구한 그레디언트 벡터의 기댓값이 유사함이 확률적으로 보장
  - 딥러닝의 경우 경사하강법보다 SGD가 실질적으로 낫다도 검증됨
- 연산량이 O(d^2·n) -> O(d^2·b) 만큼 감소하여 연산 자원을 효율적으로 활용할 수 있음

SGD의 원리: 미니배치 연산

- 경사하강법은 전체 데이터 D = (X, y)를 갖고 목적식의 그레디언트 벡터 ∇L(D, θ)를 계산
- SGD는 미니배치 D(b) = (X(b), y(b))를 가지고 그레디언트 벡터 계산. 미니배치는 확률적으로 선택하기 때문에 목적식의 모양이 바뀜
  - 미니배치를 뽑을 때마다 목적식이 다름
    - 미분이 불가능한 non-convex 함수라고 하더라도 최소 지점을 찾을 수 있게 됨
      - 경사하강법으로는 미분계수가 0인 지점이라도, SGD를 진행하면 그 지점을 탈출할 수 있게 되고, 실질적인 최소 지점을 발견할 수 있음
  - 학습 루프가 진행됨에 따라 경사하강법과 유사한 방향으로 이동
  - 볼록이 아닌 목적식에도 사용 가능하므로 머신러닝 학습에 더 효율적!
  - SGD가 학습 요율이 더 좋으나, 미니배치 사이즈를 너무 작게 잡으면 경사하강법보다 학습 속도가 느려짐!

SGD의 원리: 하드웨어

- 딥러닝 학습에는 대용량 데이터가 사용되는데, 일반적인 경사하강법을 활용하면 out-of-memory 문제가 발생할 수 있음
  - 학습 효율이 떨어질 뿐 아니라 컴퓨터 연산 자체가 불가능하다는 단점
- 대용량 데이터를 미니배치로 쪼개 SGD를 사용하면 병렬 연산을 사용하여 빠른 학습이 가능, 하드웨어 한계 극복
  - 학습률과 학습 횟수의 결정이 관건!

> Attitude & Tips

- ∇: nabla, 델 연산자

- 그레디언트 유도 과정
  $$
  \nabla_{\beta_{k}} \lVert \textrm {y} - \textrm X\beta \rVert_{2} = \partial_{\beta_{k}}\{\frac 1 n \sum_{i=1}^{n} (y_{i} - \sum_{j=1}^{d} X_{ij}\beta_{j})^2 \} = - \frac{\textrm{X}^\intercal_{·k}(\textrm y - \textrm X \beta)}{n \lVert \textrm y - \textrm X \beta\rVert_{2}}
  $$
  

