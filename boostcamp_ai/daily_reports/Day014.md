# Day 14. Recurrent Neural Networks | 임성빈, 최성준 교수님

> RNN Basic

시퀀스 데이터

- 소리, 문자열, 주가 등 데이터를 시퀀스(sequence) 데이터라고 부름
- 시계열(time-series) 데이터도 시간 순서에 따라 나열된 데이터이므로 시퀀스 데이터에 포함

- '(1)개가 (2)사람을 물었다' vs '(2)사람이 (1)개를 물었다': 순서를 뒤바꾸게 되면, 해당 사건에 대한 데이터가 현저히 줄어들거나, 맥락이 맞지 않는 예측을 하게 될 수 있음

시퀀스 데이터를 어떻게 다룰까?

- 조건부 확률: 이전 시퀀스 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부 확률을 이용

  - 다음과 같이 베이즈 법칙을 활용해 T=1의 시점부터 T=t-1의 시점으로 T=1의 시점부터 T=t 시점까지의 결합확률분포를 표현할 수 있음

    ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28X_%7B1%7D%2C+...+%2C+X_%7Bt%7D%29+%3D+P%28X_%7Bt%7D%5C%2C%7C%5C%2C+X_%7B1%7D%2C+...+%2CX_%7Bt-1%7D%29P%28X_%7B1%7D%2C+...+%2C+X_%7Bt-1%7D%29)

    ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28X_%7B1%7D%2C+...+%2C+X_%7Bt%7D%29+%3D+P%28X_%7Bt%7D%5C%2C%7C%5C%2C+X_%7B1%7D%2C+...+%2CX_%7Bt-1%7D%29P%28X_%7Bt-1%7D%5C%2C%7C%5C%2C+X_%7B1%7D%2C+...+%2CX_%7Bt-2%7D%29P%28X_%7B1%7D%2C+...+%2C+X_%7Bt-2%7D%29)

    ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28X_%7B1%7D%2C+...+%2C+X_%7Bt%7D%29+%3D+%5CPi_%7Bs%3D1%7D%5E%7Bt%7DP%28X_%7Bs%7D%5C%2C%7C%5C%2CX_%7Bs-1%7D%2C+...%2C+X_%7B1%7D%29)

    즉, ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+X_%7Bt%7D+%5Csim+P%28X_%7Bt%7D+%5C%2C+%7C+%5C%2C+X_%7Bt-1%7D%2C+...+%2C+X_%7B1%7D%29) 의 조건부 확률 분포를 모델링하는 것이 시퀀스 데이터를 다루는 기본적인 방법

  - 이론상으로는 모든 과거의 데이터를 활용하나, 실제로는 비교적 최근 정보를 갖고 모델링을 진행하는 편

- 시퀀스 데이터를 다루기 위해서는 조건부 확률을 모델링한다는 특성상 가변적인 데이터를 다룰 수 있는 모델이 필요

  - AR(τ), 자기회귀 모델(autoregressive model)

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_basic.png?raw=true" />

    - 고정된 길이 τ만큼의 시퀀스만 사용하는 경우
    - 하지만 고정된 길이 τ를 설정하거나 기타 하이퍼 파라미터를 선택하기 위한 까다로움이 있음 -> 도메인 지식 요구

  - 위와 다른 방법은, 잠재변수를 활용한 잠재 AR 모델

    - RNN 모델의 모델링 방법이 이에 해당함

    - 직전 정보를 제외한 나머지 정보들을 H_{t}의 잠재변수로 인코딩해서 활용

      <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_basic2.png?raw=true" />

    - 이렇게 모델링을 하면, t-1 시점의 직전 정보와 이를 제외한 정보에 대한 잠재변수만 고려하면 되기 때문에 길이가 가변적이지 않은 형태로 시퀀스 데이터에 대한 모델링을 진행할 수 있음

Recurrent Neural Network 이해하기

- 가장 기본적인 RNN 모형은 MLP와 유사

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_configuration.png?raw=true" />

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctextbf+%7B%5Ctextrm+%7BO%7D%7D+%3D+%5Ctextbf+%7B%5Ctextrm+%7BHW%7D%7D%5E%7B%282%29%7D+%2B+%5Ctextbf+%7B%5Ctextrm+%7Bb%7D%7D%5E%7B%282%29%7D)

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctextbf+%7B%5Ctextrm+%7BH%7D%7D+%3D+%5Csigma+%28%5Ctextbf+%7B%5Ctextrm+%7BXW%7D%7D%5E%7B%281%29%7D+%2B+%5Ctextbf+%7B%5Ctextrm+%7Bb%7D%7D%5E%7B%281%29%7D%29)

  - O: 출력 행렬, H: 잠재변수, σ: 활성화 함수, W: 가중치 행렬, b: bias
  - W(1), W(2)는 시퀀스과 관계 없이 불변인 행렬(가중치가 전체 시점에 대해)
  - 슬프게도 이러한 모델은 t시점에 대한 예측을 위해 t시점의 정보만 다루기 떄문에(=과거 시점을 담은 잠재변수를 다룰 수 없음) 과거 시점의 데이터를 활용할 수 없음

- 잠재변수를 제대로 다룰 수 있는 모델 구조는 다음과 같음 => 찐 RNN 모델! 

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_configuration2.png?raw=true" />

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctextbf+%7B%5Ctextrm+%7BO%7D%7D_%7Bt%7D+%3D+%5Ctextbf+%7B%5Ctextrm+%7BH%7D%7D_%7Bt%7D+%5Ctextbf+%7B%5Ctextrm+%7BW%7D%7D%5E%7B%282%29%7D%2B+%5Ctextbf+%7B%5Ctextrm+%7Bb%7D%7D%5E%7B%282%29%7D)

  ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctextbf+%7B%5Ctextrm+%7BH%7D%7D_%7Bt%7D+%3D+%0A%5Csigma+%28+%0A%5Ctextbf+%7B%5Ctextrm+%7BX%7D%7D_%7Bt%7D+%5Ctextbf+%7B%5Ctextrm+%7BW%7D%7D_%7BX%7D%5E%7B%281%29%7D%2B+%5Ctextbf+%7B%5Ctextrm+%7BH%7D%7D_%7Bt-1%7D+%5Ctextbf+%7B%5Ctextrm+%7BW%7D%7D_%7BH%7D%5E%7B%281%29%7D+%2B+%5Ctextbf+%7B%5Ctextrm+b%7D%5E%7B%281%29%7D%29+)

  - Wx(1): 입력 데이터에 대한 가중치행렬, WH(1): 이전 잠재변수(과거 정보)에 대한 가중치 행렬, W(2): 잠재변수를 통해 만들어진 값을 출력값에 맞게 변환해주는 가중치 행렬 
  - 현재 시점인 τ=t에 대한 입력 데이터 X와 τ=t-1 시점까지의 잠재변수 H(t-1)를 가지고 잠재변수 H(t)를 생성한 뒤, 이를 통해 O(t)의 예측이 진행
  - H(t+1)의 다음 시점 잠재변수를 만들기 위해서 H(t)를 복제하는 인코딩 과정을 거침
  - Wx(1), WH(1), W(2)의 3가지 가중치 행렬이 존재
    - 주의! 가중치는 시점 τ에 따라 변하지 않는(공유되는) 가중치 행렬임. 시점 τ에 따라 변하는 것은 잠재변수!

- RNN의 역전파

  - 잠재변수의 연결그래프에 따라 순차적으로 계산

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_backprop.png?raw=true" />

  - 그래프가 흘렀던 방향의 반대 방향으로 역전파가 이루어짐

  - RNN의 역전파는 특히, Backpropagation Through Time(BPTT)라고 부름

    - 모든 시점까지 예측이 진행된 뒤, 가장 마지막 시점의 그래디언트가 하나하나씩 과거로 거슬러 올라가면서 가중치를 업데이트

Backpropagation Trough Time, BPTT

- 시퀀스 길이가 길어질 수록 일부 항이 불안정해짐

  ![image-20210204113029054](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_backprop2.png?raw=true)

Vanishing Gradient를 해결하자

- Vanishing Gradient 문제: 그래디언트가 0에 수렴하는 문제

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/vanishing_grad.png?raw=true" />

  - 미래 시점에 대한 데이터만 중요하게 간주되고 과거 시점은 유실하게 되면, 예측 과정에서 핵심 정보를 놓치게 될 수 있음
  - Vanilla RNN 모델은 이러한 고질적인 문제가 있음

- Truncated BPTT: 시퀀스 길이가 길어지는 경우 BPTT를 통한 역전파 알고리즘 계산이 불안정하기 때문에 길이를 끊는 것이 중요
  - 역전파 과정에서 H(t)에 오로지 O(t)에 대한 그래디언트만 전달하는 방식
  - [?] 좀더 찾아봐야겠다
  - GRU, LSTM: 이러한 역전파 방법을 활용

> Sequential Models - RNN

Sequential Model

- Naive Sequence Model

  - 시퀀스 데이터가 뭘까?

    - 일상에서 접하기 가장 쉬운 순서가 있는 데이터. 비디오, 소리 등

  - 시퀀셜 모델의 가장 어려운건 뭘까?

    - 결국 대부분의 문제는 하나의 답을 찾는 것이 목적인데, 입력 데이터의 크기를 정의하기가 어려움
    - 단어를 몇개 입력할 건데? 비디오 얼만큼 입력할 건데?
    - 입력 데이터의 길이에 관계없이 모델은 돌아가야 한다는 것이 핵심

    ![image-20210204115831171](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/sequential_model.png?raw=true)

    - 시간이 흘러갈 수록 고려할 과거가 많아져

- Autoregressive Model

  - 과거의 고정된 길이만을 관찰하는 것

    ![image-20210204120016367](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/sequential_model2.png?raw=true)

  - Markov model(first-order autoregressive model)

    ![image-20210204120113066](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/sequential_model3.png?raw=true)

    - 현재는 직전의 과거에만 의존한다고 가정
      - 장점: 결합확률분포를 표현하기가 쉬워짐
      - 단점: 너무 많은 양의 과거를 버리게 됨
        - 현실적으로는 말이 안되지: 수능 전날에만 공부한다고 점수가 잘나오는 건 아니니

- Latent Autoregressive Model

  - Naive sequence model과 autoregressive model의 단점: 과거의 특정 시점을 다루는 것에 한계가 명확
  
  - 위 단점을 극복하자!: *Hidden State*(과거의 정보를 요약)
  
    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/latent_ar.png?raw=true" />
  
    - Hidden State(latent state)는 요약하는 방식에 따라 값이 달라질 수 있으나, 결정적으로 모델은 입력데이터 X와 hidden state만 고려하면 됨

Recurrent Neural Network

- 자기 자신으로 돌아오는 구조가 존재하는 것이 MLP와의 차이

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_configuration3.png?raw=true" />

  - 자기 자신으로 돌아오는 구조이기 때문에 고려해야할 state가 적은 것으로 보이지만, 사실상 구조를 위 그림과 같이 풀어서 바라보면 입력값이 굉장히 많다는 것을 알 수 있음

- RNN의 최대 단점: 먼 과거의 정보를 활용하기 어려움

  - Short-term dependencies: 가까운 과거는 미래 예측에 더 많이 활용됨

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_short_term.png?raw=true" />

  - Long-term dependencies: 먼 과거는 미래 예측이 사용되기 어려움

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_long_term.png?raw=true" />

  - 모델이 학습되면서 특징이 hidden state에 요약되는데, 이 과정에서 먼 과거의 state는 살아남기가 어려움

  - 이 문제를 구체적으로 다시 보자

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/rnn_vanishing_grad.png?raw=true" />

    - Activation function Φ에 의해 출력된 값은 점점 range가 제한되고, 여러번 합성될 수록 그래디언트 값은 0에 가까워지거나 지나치게 증폭됨
      - Sigmoid, tanh: vanishing gradient의 위험
      - ReLU: exploding gradient의 위험(그래디언트 값이 양수라면) -> 자꾸 상수배를 하게 되니까
    - 이러한 문제 때문에 LSTM, GRU가 등장

Long Short Term Memory, LSTM

- LSTM의 구조

  - Cell State라는 녀석이 컨베이어 벨트 위를 지나가면서 중요한 가중치는 살리고, 불필요한 가중치는 버리는 방향으로 학습을 진행함 => Vanilla RNN이 갖고 있던 vanishing gradient 문제 극복
  - Forget Gate, Input Gate, Output Gate의 3가지 게이트가 포함
  - 모델 내부에서만 업데이트되는 Cell State와 출력에 활용되는 Hidden State를 신경써서 봐보면 되겠다 
  - 모델 내 활성화 함수는 시그모이드와 tanh가 사용되었는데, 시그모이드는 각 가중치에 대한 중요도를 반영하기 위해, tanh는 가중치를 (-1, +1)로 normalize하기 위해 사용되었다고 직관적으로 이해하면 좋다.

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/lstm_configuration.jpg.png?raw=true" />

  1. Forget Gate

  ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/lstm_forget.jpg?raw=true)

  - 어떤 정보를 버릴 지 판별하는 부분. 이전의 hidden state와 현재 데이터 x를 입력 받아 f(t)를 출력
  - f(t)는 이전 Cell State의 각 성분별 중요도를 매기기 위해 사용됨

  2. Input Gate

  ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/lstm_input.jpg?raw=true)

  - 어떤 정보를 쥐고갈 지 판별하는 부분. 이전의 hidden state와 현재 데이터 x를 입력 받아 i(t)와 후보 Cell State를 출력
  - i(t)는 시그모이드 함수를 거쳐 후보 Cell State의 각 성분별 중요도를 매기는 역할을 수행
  - 후보 Cell State는 tanh 함수에 의해 normalize됨

  3. Update cell

  ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/lstm_update.jpg?raw=true)

  - Input 게이트와 Forget 게이트의 결과값을 반영하여 Cell State 업데이트
  - 무엇을 잊을지에 대한 f(t) 가중치와 이전 Cell State를, 무엇을 가질지에 대한 i(t)의 가중치와 후보 Cell State를 곱한 값을 더하여 새로운 Cell State로 활용

  4. Output Gate

  ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/lstm_output.jpg?raw=true)

  - 앞서 Update cell 단계에서 구한 Cell State에 대해 중요한 정보를 중심으로 새로운 hidden state에 반영
  - Cell State를 tanh 함수를 통해 noramlize하고 성분별 중요도를 파악한 o(t)와 곱하여 중요한 성분을 선별하게 됨

Gated Recurrent Unit, GRU

- 게이트가 3개였던 LSTM과 달리 GRU는 2개의 게이트만을 가짐

- LSTM의 핵심 요소 중 하나인 Cell State가 포함되어 있지 않으며, 이 역할을 Hidden State가 모두 수행
  
  - Hidden State의 가중 평균을 활용함으로써 해결!
  
- LSTM보다 구조가 단순하나, 오히려 성능이 더 좋은 경우도 많음 

- 네트워크의 파라미터가 적어서 generalization이 잘 된거 아닐까 추측

- GRU의 구조

  1. Reset Gate

     ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/gru_reset.jpg?raw=true)

     - 이전의 Hidden State와 현재 데이터 x를 입력 받아 시그모이드 함수를 거쳐 r(t)를 생성
     - r(t)는 시그모이드 함수를 거쳐 이후 Candidate에게 어떤 정보가 중요한지 중요도를 전달하는 역할을 수행
  
  2. Update Gate
  
     ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/gru_update.jpg?raw=true)
  
     - 이전의 Hidden State와 현재 데이터 x를 입력 받아 시그모이드 함수를 거쳐 z(t)를 생성
     - Hidden State를 업데이트하는 데 가중 평균을 하기 위해 활용됨
  
  3. Candidate
  
     ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/gru_candi.jpg?raw=true)
  
     - 이전 Hidden State에 중요도를 매긴 것과 데이터 x를 입력받아 후보 Hidden State를 출력
  
  4. Next Hidden State
  
     ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/gru_next_h.jpg?raw=true)
  
     - 앞선 과정동안 마련해놓은 가중치 z(t)와 후보 Hidden State를 활용하여 최종 Hidden State의 업데이트
     - 위 그림의 수식과 같이 z(t)를 활용하여 이전 HIdden State와 후보 Hidden State의 가중 평균을 통해 다음 Hidden State가 매겨짐



> Transformer

- 시퀀셜 모델링을 어렵게 하는 요소는 무엇이 있을까
  - 사람의 언어를 생각해보면 쉬움
  - 같은 의미를 가진 말을 하더라도 어순이 다를 수 있고, 말을 축약해서 할 수 도 있고 늘려서 할 수도 있으니까 어려운 것
  - 즉, 시퀀스의 길이가 가변적이고 순서가 달라질 수 있다는 것이지
- Transformer: 기존의 RNN 계열 모델과 달리 재귀적인(recurrent) 구조가 포함되어 있지 않고, *'Attention'*이라 불리는 구조를 활용한 것이 핵심!
- 기계 번역에 자주 사용
  - 본질적으로, 시퀀셜 데이터를 입력받아 인코딩하는 방식이기 때문에 단순히 기계번역(NMT) 문제에만 국한되지 않음

핵심 작동 원리

- 시퀀스를 시퀀스로 변형하는 것이 주 목적

  - 입력 데이터와 출력 데이터의 시퀀스 길이는 달라도 상관없다

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/transformer_enc_dec.png?raw=true" />

- 기존 RNN 모델은 가령 3개의 단어가 입력되면 모델이 3번 작동하는데, Transformer는 여러 개의 단어가 입력되어도 단 한번만 작동(단 generate시에는 한 단어씩 찍어냄. 내부적으로 학습할 때 한번만 작동한다는 것!)

  <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/transformer_enc_dec2.png?raw=true" />

  - 알아야 하는 의문들
    - N개의 단어가 어떻게 한번에 처리되는 것인가?
    - 디코더와 인코더 사이에 어떤 정보를 주고 받는가?
    - 디코더가 어떻게 generate를 하는 것인가?

- `왜 이게 잘 될까?: *Self-attention*

  ![image-20210204230037252](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/transformer_enc_dec3.png?raw=true)

  - Transformer는 크게 인코더와 디코더로 구성되어 있는데, 위 그림과 같이 Encoder에서는 각 단어 간의 관계를 판별할 수 있는 Self-attention 과정을 거침

  ![self_attention](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/self_attention.png?raw=true)

  - 각 단어는 임베딩을 통해 고정된 차원으로 맵핑

    - 임베딩 벡터의 차원은 하이퍼 파라미터로 사용자가 지정할 수 있음

  - 특정 단어의 attention 계산 과정은 다음과 같음

    1. 입력받은 문장에 대해 Query, Key, Value 행렬를 생성

       ![qkv](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/qkv.png?raw=true)

       - 위와 같이 Query, Key, Value의 가중치 행렬을 곱하여 생성
         - Query, Key, Value 벡터의 크기는 (논문에서는) 임베딩 벡터에 비해 훨씬 낮은 차원으로 설정
       - 가중치 행렬은 모든 단어에 대해 파라미터를 공유함!

    2. Score 계산

       - 특정 단어의 Score를 구하기 위해, 해당 단어의 Query 벡터와 나머지 단어들의 Key 벡터를 내적하여 Score값을 산출

    3. Key 벡터 사이즈의 제곱근으로 Score를 나눔

       - 그래디언트를 계산할 떄 더 잘 학습될 수 있도록 하는 효과가 있음!

    4. Softmax 적용

       - 각 단어(자기 자신 포함)에 대하여 구한 normalize된 score값에 대해 Softmax 함수를 적용하여 각 값을 확률화 

    5. Value 벡터에 Softmax가 적용된 score를 곱한다

       - 각 단어마다 갖고 있는 Value 벡터에 확률값을 가중치로서 상수배해줌으로써 집중할 단어는 그대로 남기고, 관련 없는 단어는 비중을 낮추기 위함

    6. 위에서 구한 Value 벡터들을 싸그리 더한다

       - 각 단어로부터 구한 weighted Value 벡터들을 싹다 더하면 특정 단어에 대한 self-attention 값이 되는 것!

- Multi-headed attention

  - 아직 개념들을 완전히 이해하지 못했지만.... 결론적으로 컨셉은 다음 그림으로 이해할 수 있다 이말이야!

    - 러프하게 생각해본다면, 단순히 하나의 self-attention에만 의존하는 것이 아니라, 여러 self-attention을 고려하여 문맥을 더욱 입체적으로 판단할 수 있는 것!
    - 가령, 이렇게만 문맥을 파악할 수 있지만

    ![image-20210204232129732](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/multi_headed_attention.png?raw=true)

    - 이렇게 본다면 좀더 다차원적인 접근(=일반화된 예측)이 가능해지는 것

      ![image-20210204232308770](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week03/multi_headed_attention2.png?raw=true)

> Attitude & Tips

- 손으로 쓰는 것만큼 이해를 잘 할 수 있은 방법은 없다

- Transformer는 아직 정복하지 못한 것 같다. 더 더 더 봐야겠다.

  - 한 번 이해해두면 큰 자산이 될 거라고 교수님께서 말씀하셨으니, 주의 깊게 봐보자.

  - 읽어봤는데 아직 이해를 못했음... 다시 봐야할 글들

    - [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

    - [\[번역]The Illustrated Transformer](https://nlpinkorean.github.io/illustrated-transformer/)

      