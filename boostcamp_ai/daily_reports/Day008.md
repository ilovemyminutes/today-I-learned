# Day 8. Pandas I / 딥러닝 학습방법 이해하기 | 최성철, 임성빈 교수님

> pandas I

pandas(<u>pan</u>el <u>da</u>ta)

- 구조화된 데이터 처리

- numpy와 통합하여 강력한 스프레드시트 처리 기능 제공
- 인덱싱, 연산 함수, 전처리 함수 제공
- 데이터 처리, 통계 분석

|      | Column1    | Column2    | Column3    |
| ---- | ---------- | ---------- | ---------- |
| row1 | data[1, 1] | data[1, 2] | data[1, 3] |
| row2 | data[2, 1] | data[2, 2] | data[2, 3] |

- row = instance = tuple
- column = feature = field = attribute
- column vector = feature vector

Series

- 데이터 테이블의 하나의 컬럼 또는 feature vector가 Series object
- 데이터, 인덱스, 데이터 타입으로 구성

- Funtions
  - `pd.Series(data, dtype)`
  - `pd.Series().index`: 인덱스를 담은 넘파이 배열
    - list를 수정하듯이 인덱스 수정 가능함
    - `pd.Series().index.name`: 인덱스의 이름 지정 가능(멀티인덱싱에 활용 가능)
  - `pd.Series().values`: value를 담은 넘파이 배열
  - `pd.Series().astype()`: 타입 변경
  - `pd.Series().name`: Series의 이름 지정

DataFrame

- Series의 모음
- 일반적으로 딕셔너리 형태로 데이터를 입력하여 생성
- 인덱싱
  - `loc`: 인덱스 이름을 기준으로 접근
  - `iloc`: 인덱스 위치값을 기준으로 접근
  - Fancy Indexing
    - `data[data['CHAS']!=0].head()` 등의 boolean 인덱싱 가능
- Funtions
  - `pd.DataFrame().head()`: 상위 데이터 리턴
    - 데이터를 일부만 볼 때는 `pd.DataFrame().head().T`를 자주 사용
  - ` pd.DataFrame().dtypes`: 컬럼별 속성 리턴
  - `pd.DataFrame()['col_name']`: 하나의 컬럼에 접근. Series 타입으로 리턴
    - `pd.DataFrame().col_name`으로도 접근 가능
    - `pd.DataFrame()[column_list]`: list 형태로 복수의 컬럼에 접근도 가능
  - `pd.DataFrame().loc[row_idx, col_idx]`: 행과 열의 이름을 통해 데이터에 접근
    - 인덱스 이름을 명시해야 함
  - `pd.DataFrame().iloc[row_idx, col_idx]`: 행과 열의 위치를 통해 데이터에 접근
  - `pd.DataFrame().T`: transpose
  - `pd.DataFrame().to_csv()`: csv파일로 데이터 저장
  - `del pd.DataFrame['col_name']`: 특정 컬럼 제거
    - `pd.DataFrame().drop('col_name', axis)`로도 사용 가능
      - `axis=1` - 행 제거, `axis=0` - 열 제거
  - `pd.DataFrame().reset_index(drop=)`: 행 인덱스를 초기화 하는 경우
  - `pd.DataFrame().describe()`: 요약된 통계치를 보여줌
  - `pd.DataFrame()['col_name'].unique()`: 컬럼의 unique 값을 반환
  - `pd.DataFrame().isnull()`: 결측값이면 True, 결측값이 아니면 False
    - `pd.DataFrame().isnull().sum()`: 결측값의 총 개수를 보여줌
  - ` pd.DataFrame()['col_name1'].corr(pd.DataFrame()['col_name2'])`: 상관계수 계산
    - ` pd.DataFrame().corr()`: 상관계수 행렬
  - ` pd.DataFrame()['col_name1'].cov(pd.DataFrame()['col_name2'])`: 공분산 계산
  - ` pd.DataFrame().corrwith(pd.DataFrame()['col_name2'])`: 하나의 열과 데이터프레임 열 각각의 상관계수 계산

Series Operation

- `Series1.add/mul/...(Series2)`: Series 간 덧셈, 곱셈 등 연산 수행
  - 인덱스 이름이 서로 맞지 않을 경우 `NaN`로 채워짐

DataFrame Operation

- `pd.DataFrame().add(pd.Series(), axis)`: 데이터프레임에 시리즈를 더함. axis를 올바르게 설정해야 의도한 연산이 발생!

lambda, map, apply

- map
  - `df_data['CHAS'].map({0:'changed'})`: 열의 특정 값을 지정값으로 맵핑

    ```python
    srs = pd.Series(np.arange(10, 15))
    srs.map({i:j for i, j in zip(range(10, 16), list('abcde'))}) # 딕셔너리 형태로 입력하여 맵핑
    '''
    0    a
    1    b
    2    c
    3    d
    4    e
    dtype: object
    '''
    ```

    - `pd.DataFrame()['col_name'].replace(dict)`와 같이 사용할 수도 있음

- apply

  ```python
  data.apply(sum)
  '''
  CRIM         1828.44292
  ZN           5750.00000
  INDUS        5635.21000
  CHAS           35.00000
  NOX           280.67570
  RM           3180.02500
  AGE         34698.90000
  DIS          1920.29160
  RAD          4832.00000
  TAX        206568.00000
  PTRATIO      9338.50000
  B          180477.06000
  LSTAT        6402.45000
  MEDV        11401.60000
  dtype: float64
  '''
  ```

  ```python
  def foo(x):
      return pd.Series([x.min(), x.max()], index=['min', 'max'])
  data.apply(foo)
  
  '''
           CRIM     ZN  INDUS  CHAS    NOX     RM    AGE      DIS  RAD    TAX  \
  min   0.00632    0.0   0.46     0  0.385  3.561    2.9   1.1296    1  187.0   
  max  88.97620  100.0  27.74     1  0.871  8.780  100.0  12.1265   24  711.0   
  
       PTRATIO       B  LSTAT  MEDV  
  min     12.6    0.32   1.73   5.0  
  max     22.0  396.90  37.97  50.0  
  '''
  ```

  - `applymap`: 모든 성분에 맵핑을 적용할 때

- lambda

  - `pd.DataFrame().apply(lambda x: ...)`: apply 함수와 병행하여 사용 

> 딥러닝 학습방법 이해하기

신경망을 수식을 분해해보자

- 선형모델만을 가지고 여러 문제를 해결하기는 어렵다

  <img src="C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\neural_network.png" alt="image-20210127113324961" style="zoom:80%;" />

- 데이터 X와 가중치 W의 곱에 y절편에 해당하는 b 벡터를 더하여 O가 구해진다고 가정해보자

  - 각 행렬의 shape에 주목!

  

- 다음과 같이 d개의 변수를 가지고 p개의 잠재변수를 설명하는 것! 화살표가 결국은 W가 되는 셈이지

  ![image-20210127113613033](C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\fully_connected_nodes.png)

- 알고가자! 소프트맥스
  $$
  \textrm {softmax(o)} = ({\frac {\textrm {exp}(o_{1})}{\sum_{k=1}^{p}\textrm {exp}(o_{k})}},\,...\,, {\frac {\textrm {exp}(o_{p})}{\sum_{k=1}^{p}\textrm {exp}(o_{k})}})
  $$

  - 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산
  - 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측
  - 넘파이를 활용하여 구현 가능

  ```python
  def softmax(vec):
      denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True))
      	# 지수함수를 통해 너무 큰 값이 입력될 경우 overflow 문제가 발생할 수 있어 방지하고자 np.max() 사용
          
      numerator = np.sum(denumerator, axis=-1, keepdims=True)
      val = denumerator / numerator
      return val
  ```

  - 추론을 할 때는 원핫(one-hot)벡터를 통해 최댓값을 가진 주소만 1로 출력하지, softmax를 사용하지는 않음
    - [?] 추론이 뭘 의미하지?

- 신경망은 선형모델과 활성함수(activation function)을 합성한 함수

  - 활성함수: 비선형함수로, 잠재벡터의 각 노드에 개별적으로 적용하여 새로운 잠재벡터를 만든다 => 비선형 문제 해결!
    - 소프트맥스는 모든 입력값을 고려하여 출력을 한다면, 활성함수는 오로지 하나의 입력값에 대하여만 변환/출력

활성함수(activation function)

- 활성함수는 실수 공간 위에 정의된 비선형(non-linear) 함수로, 딥러닝에 ㅎ괄용

- 활성함수를 사용하지 않으면 딥러닝은 선형모형과 차이가 없음

- 전통적으로 시그모이드 함수나 tanh함수를 활성함수로 사용하나, 현재 딥러닝에서는 ReLU함수를 많이 사용한다.

  <img src="C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\activation_functions.png" alt="image-20210127115650657" style="zoom:80%;" />

  - 과거 퍼센트론 발전에 시그모이드, tanh 함수가 쭉 함께함
  - ReLU: 활성함수로서 좋은 성질을 갖고 있음
    - 

- 다음과 같이 선형모델의 중간 중간에 활성함수(σ)를 포함하는 식의 레이어를 구성!

  ![image-20210127115949306](C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\add_actfuncs_1.png)

  - 이런 층들이 모여 다층 구조를 이룰 경우, 이를 다층 퍼셉트론(multi-layer perceptron, MLP)이라 부름

    ![image-20210127120259281](C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\add_actfuncs_2.png)

    - MLP의 파라미터는 L개의 가중치 행렬 W(1), ... , W(L)과 b(1), ... . b(L)로 이루어져 있다
    - l = 1, ... , L 까지 순차적인 신경망 계산을 순전파(forward propagation)라고 부른다.

층을 여러개 쌓는 이유?

- 이론적으로 2층 신경망으로도 임의 연속함수를 근사 가능

- 하지만 층이 깊을 수록 목적함수를 근사하는 데 필요한 노드 수가 훨씬 빨리 줄어들어 좀더 효율적인 학습이 가능!

  <img src="C:\Users\iloveslowfood\Documents\workspace\iloveCookBook\boostcamp_ai\daily_reports\wide_layer_problem.png" alt="image-20210127120623821" style="zoom:67%;" />

  - 2층 신경망이라면 위 그림과 같이 넓은 신경망이 되어야겠지
  - BUT! 층이 깊다고 해서 최적화가 쉽다고는 말할 수 없다. 층이 깊어질 수록, 즉 복잡한 함수가 될 수록, 딥러닝 모델은 점점 어려워짐

딥러닝 학습 원리: 역전파 알고리즘

- 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 파라미터를 학습.

  - 각 층에 사용된 {W(l), b(l)}(l=1, ..., L)를 학습

- 선형모델의 경우 1개의 층으로 계산하는 원리라서, 그레디언트 벡터를 동시적으로 계산할 수 있으나, 다층 구조의 딥러닝 모델의 경우 각 층에 대해 순차적으로 그레디언트를 계산하고 역순으로 이 값을 업데이트해주어야 함

- 각 층 파라미터의 그레디언트 벡터는 윗층부터 역순으로 계산

  - 합성함수 미분법인 연쇄법칙(chain-rule) 기반의 자동미분(auto-differentiation)을 사용
    $$
    z = (x + y) ^{2}
    $$

    $$
    \frac {\partial{z}} {\partial{w}} = 2w
    $$

    $$
    \frac {\partial{w}} {\partial{x}} = 1
    $$

    $$
    \frac {\partial{x}} {\partial{z}} = \frac {\partial{z}} {\partial{w}} \frac {\partial{w}} {\partial{x}} = 2w ·1 = 2(x + y)
    $$

  - 각 텐서(뉴런)의 값을 메모리에 저장해줘야만 역전파를 원활하게 진행하기 때문에(연쇄법칙을 사용하므로), 순전파에 비해 메모리 사용량이 크다.

>Attitude & Tips

- `pd.options.display.max_rows = 최대 출력 행 수`
- 컴퓨터 연산의 입장에서 코드를 생각하는게 되게 중요하구나👍 어제 피어세션에서도 그렇고 오늘 수업도 그렇고 컴퓨터 연산을 효율적으로 또는 의도한 대로 진행하기 위해 트릭을 사용하는 게 인상적이다
- 프레임워크를 활용하기에 앞서서 원리를 이해하고 들어가는 것이 중요해 보인다.