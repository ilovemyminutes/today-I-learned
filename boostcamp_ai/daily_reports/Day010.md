# Day 10. 시각화 / 통계학 | 최성철, 임성빈 교수님

> 시각화 도구

matplotlib

- pyplot 객체를 사용하여 데이터를 표시하는 방식

  - `plt.plot(X, Y, color: str, linestyle: str)`
  - pyplot 객체에 그래프를 **쌓은** 뒤 flush(주피터 상에서는 알아서 출력)

  ```python
  X = range(100)
  Y = range(100)
  plt.plot(X, Y)
  ```

  

  ![plot1](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/matplotlib_line.png?raw=true)

- 하나의 이미지로 보이나, pyplot 객체와 figure 객체의 2가지로 구성

- 단점: argument를 kwargs로 받아 docstring으로 파라미터 확인이 어려움

subplots

- figure 객체를 먼저 생성한 뒤 plot을 추가해나가는 형태

```python
fig = plt.figure()
fig.set_size_inches(10, 5)

ax1 = fig.add_subplot(1,2,1) # row, col, index
ax2 = fig.add_subplot(1,2,2)

ax1.plot(x1, y1, c='b')
ax2.plot(x2, y2, c='g')
plt.show()
```

set color: color 속성을 활용 - RGB값을 활용하거나 미리 지정된 색상 문자열을 입력

set title: `plt.title()`

- Latex을 활용할 수 있음 - `'$\frac {1} {2}$'`

set legend: `plt.legend()`

- `plt.legend(shadow: bool, fancybox: bool, loc: str)`

annotate

- `plt.text(x, y, 텍스트내용)`: 그래프 내 원하는 위치에 텍스트 삽입

- ```python
  plt.annotate(
      텍스트내용, 
      xy=(50, 150), 
      xytext=(20, 175), 
      arrowprops=dict(facecolor='black', shrink) # 화살표
              )
  ```

grid

- ```python
  plt.grid(
      bool, 
      lw=0.4, 
      ls='--', 
      c='.90'
  )
  ```

savefig

- `plt.savefig('file.png', c='a')`
- 주의! `plt.show()`를 진행하면 메모리 상에서 그래프가 없어지므로(?) `plt.show()` 이전에 figure를 저장해야 함

> matplotlib graph

Scatter plot

- `plt.scatter()`

  ```python
  n = 50
  x = np.random.rand(n)
  y = np.random.rand(n)
  colors = np.random.rand(n)
  area = np.pi * (15 * np.random.rand(n))**2
  plt.scatter(
      x, y, 
      s=area, # 데이터 크기 지정 
      c=colors, 
      alpha=.5 # # 투명도
  )
  plt.show()
  ```

Bar chart

- `plt.bar()`

  ```python
  data = [[5, 25, 50, 20],
  [4, 23, 51, 17],
  [6, 22, 52, 19]]
  
  x = np.arange(4)
  plt.bar(x, data[0], color='b', width=.25)
  plt.bar(x + 0.25, data[1], color='c', width=.25)
  plt.bar(x + 0.5, data[2], color='g', width=.25)
  plt.xticks(x+.25, ['A','B','C','D']) # x축 tick 값 설정
  
  plt.show()
  ```

  ```python
  plt.bar(x + 0.5, data[i], 
          bottom=np.sum(data[:i], axis=0) # 막대를 쌓아 올릴 때
         )
  ```

- `plt.barh()`: 가로 형태의 bar chart

Box plot

- `plt.boxplot()`

  ```python
  data = np.random.randn(100, 5)
  plt.boxplot(data)
  plt.show()
  ```

> Seaborn

Seaborn

- 통계 시각화를 위한 도구
- matplotlib를 더 쉽게 사용할 수 있는 느낌
- Docstring 확인이 편하다
- `hue` 파라미터를 통해 카테고리별 시각화 가능
- `data` 파라미터를 통해 데이터프레임를 입력하고, 각 축에는 컬럼명을 입력하여 편리한 시각화 가능

`sns.regplot()`: Scatter + Regression

`sns.countplot()`: 빈도를 세주는 plot

`sns.barplot(x, y, data, estimator)`: 시각화 결과 평균값의 bar를 출력 + 분포 가이드 선. estimator를 통해 평균값이 아닌 다른 추정값을 사용할 수 있음.

`sns.vilolinplot`: Boxplot + KDE 분포 확인

`sns.swarmplot`: Boxpot + Scatter. 데이터 적을 때 사용하면 좋음

`sns.FacetGrid`: 특정 기준으로 그래프를 여러 개로 나누고, `sns.map()`을 통해 그래프를 채워넣는 식

- 결합 분포 그려볼 때 편함

- ```python
  g = sns.FacetGrid(tips, cols=, rows=)
  g.map(sns.scatterplot, 'total_bill', 'tip')
  ```

> 통계학 맛보기

모수

- 통계적 모델링은 적절한 가정 위에 확률분포를 추정하는 것이 목표, 기계학습 또한 그러하다!
- 유한한 데이터만 관찰하여 모집단의 분포를 정확하게 알아내는 것은 불가능 => 근사적으로 확률분포를 '추정'

- 데이터가 특정 확률분포를 따른다고 가정하고, 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적 방법론(parametric method)
- 특정 확률분포를 가정하지 않고, 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수 방법론(nonparametric method)

확률분포 가정하기

- 우선 히스토그램을 통해 모양 관찰
  - 데이터가 2개의 값만 가짐 -> 베르누이 분포 가정
  - 데이터가  [0, 1] 사이에서 값을 가짐 -> 베타분포 가정
  - 데이터가 0 이상의 값만 가짐 -> 감마분포, 로그정규분포 가정
  - 데이터가 실수공간 전체에서 값을 가짐 -> 정규분포, 라플라스 분포 등 가정
- 기계적으로 확률분포를 가정하면 안됨! 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙

모수 추정

- 정규분포: 평균과 분산으로 추정하는 통계랑은 다음과 같음
  $$
  \bar {X} = \frac {1} {N} \sum_{i=1}^{N}X_{i},\,\, \textrm E[\bar{X}] = \mu
  $$

  $$
  {S^2} = \frac {1} {N-1} \sum_{i=1}^{N}({X_{i} - \bar{X}})^2, \,\, \textrm E[S^2] = \sigma^2
  $$

  - 표본분산 계산 시 불편 추정량을 구하기 위해 N-1로 나눔

- 통계량의 확률분포를 표집분표(sampling distribution)이라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 정규분포 N(μ, σ^2 / 2)를 따름(중심극한정리)

  - 표집분포(sampling distribution)와 표본분포(sample distribution)는 다른 개념! <- [?] 어떻게 다르지?
    - 표집분포: 통계량의 분포(표본분산의 분포, 표본평균의 분포 등)
      - 중심극한정리: 모집단의 분포와 관계 없이 표본 크기가 커질수록 표본평균의 분포는 정규 분포에 가까워진다
    - 표본분포: 표본들의 분포, 당연하게도 정규분포를 따르지 않을 가능성 존재

- 최대가능도 추정법

  - 표본평균이나 표본분산은 중요한 통계량이나, 확률분포마다 사용하는 모수가 달라 적절한 통계량은 그때그때 다름

  - 이론적으로 가장 가능성 높은 모수를 추정하는 방법 중 하나는 최대가능도추정법(MLE)임

  - 가능도 함수: 모수 θ를 변수로 갖는 함수!

  - 데이터 집합 X가 독립적으로 추출되었을 경우 로그가능도를 최적화
    $$
    L(\theta; \textrm X) = \Pi_{i=1}^{n} P(\textrm x_{i} | \theta) \Rightarrow logL(\theta; \textrm X) = \sum_{i=1}^{n}logP(\textrm x_{i} | \theta)
    $$

    - 확률질량/밀도함수가 곱셈의 형태로 표현되므로, 계산 편의를 위해 로그를 취해 사용한다.
    - 로그 가능도, 왜?
      - 로그를 취하더라도 최댓값이 발생하는 지점은 변하지 않음
      - 데이터의 숫자가 수억 단위가 되면 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능
      - 데이터가 **독립**인 경우 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있어 컴퓨터 연산이 가능
      - 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하는데, 로그가능도를 사용하면 O(n^2)에서 O(n)으로 연산량이 감소
      - 손실함수는 대개 경사하강법을 사용하므로 negative log-likelihood를 최적화하게 됨
        - 최소화 하는 목적식을 만들기 위해 음의 부호를 붙이는 것

  - 예: 정규분포

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/MLE_normal_dist.png?raw=true" alt="image-20210129131056042" style="zoom:67%;" />

    <img src="https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/MLE_normal_dist2.png?raw=true" alt="image-20210129131326760" style="zoom:67%;" />

    - 표본분산에 대한 추정의 경우, 항을 n-1이 아닌 n으로 나누어 앞서 표본분산의 수식과 다름 => 기존의 표본분산과 다른 추정량
      - MLE는 불편추정량을 보장하지 않으나, consistency를 보장하는 것이 장점 <- [?] consistency가 뭐지?

  - 예: 카테고리 분포

    ![카테고리 분포](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/note_%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EB%B6%84%ED%8F%AC_MLE.jpg?raw=true)


딥러닝에서의 MLE

- 최대가능도 추정법을 이용하여 기계학습 모델을 학습할 수 있음

- 딥러닝 모델의 가중치를 θ = (W(1), ... , W(L))이라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 (p1, ..., p_K)를 모델링

- 원핫벡터로 표현한 정답레이블 y = (y_1, ... , y_K)를 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도 최적화
  $$
  \hat {\theta}_{\textrm MLE} = \textrm {argmax}_{\theta} \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i, k} log(\textrm{MLP}_{\theta}(x_{i})_{k})
  $$


확률분포의 거리

- 기계학습에 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도
- 데이터공간에 P(x), Q(x)가 있을 경우 두 확률분포 사이의 거리(distance)를 계산할 때 다음과 같은 함수를 이용
  - 총변동 거리(Total Variation Distance, CV)
  - 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL)
  - 바슈타인 거리(Wasserstein Distance)

> Attitude & Tips

- 수학은 그때그때 개념을 알아가는 것이 효율적!

- 수학 따로 코드 따로 공부하는 것보다 한번에 병행하는 것이 좋다.

- *엔지니어로서 이론적 지식을 활용한 문제 해결을 넘어, 내가 가진 전부를 갖고 문제를 해결하려는 태도가 중요하다!*

- Matplotlib은 [Documentation](https://matplotlib.org/) 확인이 속 시원하다.

- `plt.style.use('ggplot')`

  - 더 많은 스타일 레퍼런스는 [여기](https://matplotlib.org/3.1.1/gallery/style_sheets/style_sheets_reference.html)를 확인

- `sns.set(style='darkgrid')`

- 추가 정리 내용

  - KL Divergence

    ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/note_KL%20Divergence.jpg?raw=true)

  - 카테고리 분포의 정의와 성질

    ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/note_%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC%EB%B6%84%ED%8F%AC_def.jpg?raw=true)

  - 다항분포의 정의와 성질, 카테고리 분포를 활용한 유도 과정

    ![](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/note_%EB%8B%A4%ED%95%AD%EB%B6%84%ED%8F%AC_%EC%9C%A0%EB%8F%84%EA%B3%BC%EC%A0%95.jpg?raw=true)

    

  - Reference

    - [Information Theory](http://norman3.github.io/prml/docs/chapter01/6.html)  
    - [카테고리분포와 다항분포]([https://gem763.github.io/probability%20theory/%EC%B9%B4%ED%85%8C%EA%B3%A0%EB%A6%AC-%EB%B6%84%ED%8F%AC%EC%99%80-%EB%8B%A4%ED%95%AD%EB%B6%84%ED%8F%AC.html](https://gem763.github.io/probability theory/카테고리-분포와-다항분포.html))  
    - [Kullback–Leibler divergence]([https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence))  

- 추가적으로 보면 좋을 것

  - 측도(measure)
  - 절대연속(absolutely continuous)
  - 확률 측도(probability measure)

- 이렇게 정리하면 배웠던 개념이 눈에 잘 안들어올 것 같아. 내용을 그룹핑해서 새로 정리하는 게 필요할 듯!