# Day 9. Pandas II / 확률론 맛보기 | 최성철, 임성빈 마스터

> Pandas II

Groupby - Basic

- SQL groupby 명령어와 같음

- split → apply → combine 과정을 거쳐 연산

- 엑셀의 피벗 테이블과 같은 개념

  ```python
  h_index = df.groupby(["Team", "Year"])["Points"].sum()
  # groupby(as_index=False): 그룹화된 인덱스를 인덱스로 넣을지(True) 넣지 않을지(False) 결정
  '''
  Team    Year
  Devils  2014    863
          2015    673
  Kings   2014    741
          2016    756
          2017    788
  Name: Points, dtype: int64
  '''
  ```

- Hierarchical Index

  - groupby 이후 인덱스가 다중으로 생성될 수 있음

    ```python
    h_index.index
    '''
    MultiIndex([('Devils', 2014),
                ('Devils', 2015),
                ( 'Kings', 2014),
                ( 'Kings', 2016),
                ( 'Kings', 2017)],
               names=['Team', 'Year'])
    '''
    ```

  - `unstack()`: 다중 인덱스를 풀어 줌. `stack()`을 통해 다시 다중 인덱스로 변환 가능

    ```python
    h_index.unstack() # stack()을 통해 원상복구 가능
    '''
    Year	2014	2015	2016	2017
    Team				
    Devils	863.0	673.0	NaN		NaN
    Kings	741.0	NaN		756.0	788.0
    '''
    ```

  - `reset_index()`: 인덱스를 초기화

    ```python
    h_index.reset_index()
    '''
    	Team	Year	Points
    0	Devils	2014	863
    1	Devils	2015	673
    2	Kings	2014	741
    3	Kings	2016	756
    4	Kings	2017	788
    '''
    ```

  - `swaplevel(i, j)`: level i와 j 인덱스 위치를 swap

    ```python
    h_index.swaplevel(i=0, j=1)
    '''
    Year  Team  
    2014  Devils    863
    2015  Devils    673
    2014  Kings     741
    2016  Kings     756
    2017  Kings     788
    '''
    ```

  - `sort_index(level=k)`: level k의 인덱스를 정렬

Groupby - grouped

- grouped 객체는 그룹핑이 완료된 제너레이터 형태. split된 상태를 추출할 수 있음

  ```python
  grouped = df.groupby("Team")
  for name, group in grouped:
      print(name)
      print(group)
  '''
  Devils
       Team  Rank  Year  Points
  2  Devils     2  2014     863
  3  Devils     3  2015     673
  Kings
      Team  Rank  Year  Points
  4  Kings     3  2014     741
  6  Kings     1  2016     756
  7  Kings     1  2017     788
  '''
  ```

- 추출된 group 정보에는 3가지 유형의 apply가 가능: aggregation, transformation, filtration

  - Aggregation

    - `agg()`를 통해 요약된 통계정보 추출

    ```python
    grouped.agg([max, sum]) # aggregation
    '''
    		Rank	Year			Points
    		max	sum	max		sum		max	sum
    Team						
    Devils	3	5	2015	4029	863	1536
    Kings	3	5	2017	6047	788	2285
    Riders	2	7	2017	8062	876	3049
    Royals	4	5	2015	4029	804	1505
    kings	4	4	2015	2015	812	812
    '''
    ```

    - `describe()`: grouped 객체에 대한 요약 통계값을 출력

    ```python
    grouped.describe().T
    '''
    		Team	Devils		Kings		Riders		Royals		
    Rank	count	2.000000	3.000000	4.000000	2.000000
    		mean	2.500000	1.666667	1.750000	2.500000
    		std		0.707107	1.154701	0.500000	2.121320
    		min		2.000000	1.000000	1.000000	1.000000
    		25%		2.250000	1.000000	1.750000	1.750000
    		50%		2.500000	1.000000	2.000000	2.500000
    		75%		2.750000	2.000000	2.000000	3.250000
    		max		3.000000	3.000000	2.000000	4.000000
    ...
    '''
    ```

  - Transformation: `transform()`을 통해 데이터 각각에 대한 변환

    ```python
    score = lambda x: (x - x.mean()) / x.std()
    grouped.transform(score) # 그룹별로 각 데이터에 대해 score 함수를 적용
    
    '''
    	Rank		Year		Points
    0	-1.500000	-1.161895	1.284327
    1	0.500000	-0.387298	0.302029
    2	-0.707107	-0.707107	0.707107
    3	0.707107	0.707107	-0.707107
    4	1.154701	-1.091089	-0.860862
    5	NaN			NaN			NaN
    6	-0.577350	0.218218	-0.236043
    7	-0.577350	0.872872	1.096905
    8	0.500000	0.387298	-0.770596
    9	0.707107	-0.707107	-0.707107
    10	-0.707107	0.707107	0.707107
    11	0.500000	1.161895	-0.815759
    '''
    ```

  - Filtration: grouped 객체에 대해 특정 조건에 맞는 데이터를 검색할 때 사용

    - `filter(func=)`: func 파라미터에는 리턴값이 boolean인 함수가 들어가야 함

    ```python
    grouped.filter(lambda x: x["Points"].mean() > 700)
    '''
    	Team	Rank	Year	Points
    0	Riders	1		2014	876
    1	Riders	2		2015	789
    2	Devils	2		2014	863
    3	Devils	3		2015	673
    '''
    ```

- 직접 해봅시다

  - `dateutil.parser.parse(timestr=)`: 시간에 대한 문자열을 입력 받아 파싱해주는 함수

  - 월별 통화 시간 집계

    ```python
    df_phone.groupby("month")["duration"].sum()
    '''
    month
    2014-11    26639.441
    2014-12    14641.870
    2015-01    18223.299
    2015-02    15522.299
    2015-03    22750.441
    Name: duration, dtype: float64
    '''
    
    df_phone.groupby("month")["duration"].sum().plot() # plot()을 통해 시각화
    ```

  - Pivot Table

    ```python
    df_phone.pivot_table(
        values=["duration"], # 테이블에 채워질 값
        index=[df_phone.month, df_phone.item], # row 인덱스 구성
        columns=df_phone.network, # column 인덱스 구성
        aggfunc="sum", # aggregation func
        fill_value=0, # NaN일 경우 채울 값
    )
    '''
    		duration
    		network	Meteor	Tesco	Three	Vodafone	data	landline	special	
    month	item									
    2014-11	call	1521	4045	12458	4316		0.000	2906		0	
    		data	0		0		0		0			998.441	0			0	
    		sms		10		3		25		55			0.000	0			1
    2014-12	call	2010	1819	6316	1302		0.000	1424		0
    ...
    '''
    ```

Merge & Concat

- `pd.merge()`

  ```python
  pd.merge(df_a, df_b, on="subject_id")
  '''
  	subject_id	test_score	first_name	last_name
  0	4			61			Billy		Bonder
  1	5			16			Brian		Black
  2	7			14			Bryce		Brice
  3	8			15			Betty		Btisan
  '''
  
  pd.merge(df_a, df_b, left_on="subject_id", right_on='subject_id') # 컬럼명이 다를 경우 left, right 별도 지정
  '''
  	subject_id	test_score	first_name	last_name
  0	4			61			Billy		Bonder
  1	5			16			Brian		Black
  2	7			14			Bryce		Brice
  3	8			15			Betty		Btisan
  '''
  
  pd.merge(df_a, df_b, on="subject_id", how="left") # how: merge할 방향, lefter, right, outer, inner
  ''' 왼쪽 테이블을 기준으로 합침. 빈공간은 NaN 처리
  	subject_id	test_score	first_name	last_name
  0	1			51			NaN			NaN
  1	2			15			NaN			NaN
  2	3			15			NaN			NaN
  '''
  
  pd.merge(df_a, df_b, right_index=True, left_index=True) # index를 기준으로 합칠 경우
  ```

- `pd.concat()`

  ```python
  df_new = pd.concat([df_a, df_b], ignore_index=True)
  '''
  	subject_id	first_name	last_name
  0	1			Alex		Anderson
  1	2			Amy			Ackerman
  2	3			Allen		Ali
  3	4			Alice		Aoni
  4	5			Ayoung		Atiches
  '''
  ```

persistence

- 데이터베이스에 있는 데이터를 추출하여 가공

```python
import sqlite3  # pymysql <- 설치

conn = sqlite3.connect("./data/flights.db")
cur = conn.cursor() # 이건 모르겠네
cur.execute("select * from airlines limit 5;") # 쿼리
results = cur.fetchall() # fetch할 객체 할당

df_airplines = pd.read_sql_query("select * from airlines;", conn) # 쿼리문으로 가져올 수 있음
```

```python
# conda install openpyxl
# conda install XlsxWriter

writer = pd.ExcelWriter("./data/df_routes.xlsx", engine="xlsxwriter") # load excel
df_routes.to_excel(writer, sheet_name="Sheet1") # save as excel
df_routes.to_pickle("./data/df_routes.pickle") # save as pickle
df_routes_pickle = pd.read_pickle("./data/df_routes.pickle") # load pickle
```

> 확률론 기초

딥러닝에서의 확률론

- 딥러닝은 확률론 기반의 기계학습 이론에 밑바탕
- 손실함수의 작동 원리: 데이터 공간을 통계적으로 해석해서 유도
  - 회귀분석의 손실함수(L2 Norm): **예측 오차의 분산을 최소화**하는 방향으로 학습
  - 분류문제의 크로스 엔트로피는 모델 **예측의 불확실성을 최소화**하는 방향으로 학습
  - 분산과 불확실성을 최소화하기 위해서는 측정하는 방법을 알 필요가 있음

이산확률변수, 연속확률변수

- 확률변수는 확률분포 D에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분

- 이산형확률변수: 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해 모델링
  $$
  P(X \in \textrm A) = \sum_{\textrm x \in A} P(X = \textrm x)
  $$

- 연속형확률변수: 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링
  $$
  P(X \in A) = \int_{A} P(\textrm x) \textrm {dx}
  $$

  - density는 누적확률분포의 도함수의 형태이며 확률로 해석할 수 없음

확률분포는 데이터의 초상화

- 지도학습의 가정 하, X × Y: 데이터 공간, D: 데이터 공간에서 데이터를 추출하는 분포
- 데이터는 확률변수로서 (x, *y*) ~ D로 표기

- 결합분포(joint distribution) P(x, *y*)는 D를 모델링

  ![image-20210128120852227](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/joint%20distribution.png?raw=true)

  - D는 사전에 알 수 있는 분포가 아님. 결합분포가 이산형이냐 연속형이냐에 관계 없이 모델링 방법에 따라 D를 연속형으로 바라볼 수도, 이산형으로 바라볼 수도 있음

- 주변확률분포(marginal distribution)

  - 가령, P(x)는 입력 x에 대한 주변확률분포로, *y*에 대한 정보를 주지는 않음

  $$
  P(\textrm x) = \sum_{y}P(\textrm x, y) \,\, or \,\, P(\textrm x) = \int_{y}P(\textrm x, y) \textrm dy
  $$

  ![image-20210128121420305](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/marginal%20dist.png?raw=true)

  

- 조건부분포(conditional distribution)

  - P(x | *y*)는 데이터 공간에서 입력 x와 출력 *y* 사이의 관계를 모델링
    - 특정 y에 대한 x의 분포를 관찰할 수 있음
  - 통계적 관계를 모델링하거나, 예측 모델을 구현할 때 활용
  - 통계적 관계를 명확하게 밝혀낼 수 있다는 특징

기대값

- 확률분포가 주어지면 데이터 분석에 사용 가능한 여러 종류의 통계적 범함수(statistical functional)를 계산 가능

- 기대값: 데이터를 대표하는 통계랑이자 확률분포를 통해 다른 통계적 범함수를 계산하는 데 사용

- 이산확률분포는 급수, 연속확률분포는 적분을 활용하여 구함

- 분산, 첨도, 공분산 등 다양한 통계량 계산 가능

  ![image-20210128123157926](https://github.com/iloveslowfood/iloveCookBook/blob/main/boostcamp_ai/etc/images/week02/var,%20cov,%20skew.png?raw=true)

조건부확률과 기계학습

- P(*y* | x)는 입력변수 x에 대해 정답이 *y*일 확률을 의미(연속확률분포의 경우 P(*y* | x)는 밀도로 해석)

- 로지스틱 회귀의 선형모델과 소프트맥스 함수의 결합: 데이터에서 추출된 패턴을 바탕으로 확률을 해석

- 분류문제

  - softmax(Wφ + b)는 데이터 x로부터 추출된 패턴 φ(x)와 가중치행렬 W를 통해 조건부확률 P(y | x)를 계산
    - *데이터가 이렇게 생겼을 때 정답이 A일 확률은 B정도 될거야*

- 회귀문제

  - 조건부기대값 E[y | x]를 추정
    $$
    \textrm E_{y ~ P(y|\textrm x)}[y|\textrm x] = \int_{y} yP(y|\textrm x) \textrm dy
    $$

    - 조건부기대값은 E||y - f(x)||를 최소화하는 함수 f(x)와 일치 <- [?] 왜 일치?

- 딥러닝은 다층신경망을 사용해 데이터로부터 특징패턴 φ를 추출
  
  - 손실 함수는 기계학습 문제와 모델에 의해 결정됨

몬테카를로 샘플링

- 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분

- 몬테카를로 샘플링 방법: 확률분포를 모를 때 데이터를 이용하여 기대값을 계산할 때 사용
  $$
  \textrm E_{\textrm x \sim P(\textrm x)}[f(\textrm x)] \approx \frac {1}{N} \sum_{i=1}^{N}f(\textrm x^(i)), \textrm x^{i} \sim P(\textrm x)
  $$

  - 샘플링 데이터를 바탕으로 임의의 함수 f의 함수값의 산술 평균이 기대값에 근사

- 이산형, 연속형 관계 없이 성립

- 독립추출만 보장된다면 대수의 법칙에 의해 수렴이 보장됨

> Attitute & Tips

- 판다스 인덱스의 level 기준: 가장 바깥부터 0, 1, 2, ...
- groupby에 대해 모르고 있던 내용들을 많이 알게 되었다
  - grouped 객체가 제너레이터니까 제너레이터 상태로 전처리를 진행하면 훨씬 메모리 비용을 아낄 수 있겠다.
- `unstack(level=-1)`: row 인덱스를 컬럼으로 보낸다고 생각하면 편함. 가로로 두꺼웠던 테이블을 세로로 길게 바꾸는 셈 
- `DataFrame.columns.droplevel(level=0)`: 멀티 인덱스를 가진 데이터프레임의 컬럼의 특정 level을 제거
- `DataFrame.add_prefix(prefix: str)`: Series는 row 인덱스에, DataFrame은 column 인덱스에 접두사를 더함

- sqlite3 알아두자!