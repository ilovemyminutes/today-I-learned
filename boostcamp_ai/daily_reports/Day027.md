# Special Lecture - Tips for Kaggle, Fullstack ML Engineer | 김상훈, 이준엽

#### Heuristic

- 이번 두 강의에서 가장 크게 느낀 점은, '경험의 중요성'이다. 캐글에 입상하기 위한 여려 가지 팁들, 풀스택 머신러닝 엔지니어가 되기 위해 하면 좋은 것들은 근본적으로 '다양한 경험'이 필요할 것으로 보였다.
  - 캐글에 대한 꿀팁, 풀스택 머신러닝 엔지니어가 되기 위해 하면 좋은 것 등은 모두 마스터분들의 경험에 의존했다. 충분히 꿀팁으로 다가오는 것이 많았으나, 한편으로는 저 경지에 도달했기에 꿀팁이라 말할 수 있는 것이고, 저 경지에 도달하지 못해본 사람은 꿀팁으로서 '체감'하지 못할지도 모르겠다는 생각이 들었다.
  - 다시 생각해보자면, 다행히도 '경험'은 누구나 할 수 있는 것이다. 얼만큼의 시간동안 어느 정도의 경험을 할지는 자신의 전략적 노력에 달려있으니, 꾸준히 스스로를 돌아보고 가장 효율적인 방향성을 찾아가야겠다. 약간 경사하강법스러운 표현이다.
- 다양한 경험
  - 사실 '다양한 경험을 쌓으세요'만큼 진부한 말도 없다. 이러한 막연함은 마스터 클래스에서 '주니어에게 기대하는 역량과 태도'에 대한 코멘트로부터 해소해볼 수 있었다.  '다양성'에는 자신만의 깊이, 나아가서는 자신만의 철학이 있는 것이 중요한 것으로 보였다.
  - '캐글은 과정에 의미가 있다', '작은 것부터 시작해서 완성하는 경험을 쌓는 것이 중요하다' 등의 조언은 이러한 '다양한 경험'과 연결지을 수 있다. 풀스택 머신러닝 개발자는 특정 분야만 공부한 사람보다 역량의 깊이가 떨어질 수 있다는 단점, 캐글에서 메달을 따지 못하면 스펙에 기재하기에는 애매하다는 단점 등은 '자신만의 다양성'과는 약간 다른 '객관적 성과'의 문제인 것 같다. 자신이 어떤 문제를 접했고, 해결을 위해 어떤 구체적 행동을 했으며, 이것이 어떤 결과를 낳았고, 최종적으로 자신은 지금 어떤 사람이며 어떤 사람이 되고 싶은지 등, 자기 자신을 명확하게 인지하고 있을 때 '다양한 경험'을 해봤다고 말할 자격이 있는 것 같다.
  - 마스터 클래스를 들으면서 '메타 인지'가 떠오르기도 했다. 메타 인지는 대략 자기 자신이 무엇을 잘 하고 있고, 잘 못하고 있는지 자각하고 있는 것을 의미하는데, 이러한 자각으로부터 방향성있는 성장을 얻을 수 있다. 캐글에서 수상을 할지 안할지 알 수는 없지만 매주 수십 시간을 투자한 것, 특정 분야에 대한 깊이가 떨어질 수 있지만 풀스택 머신러닝 엔지니어를 향해 움직인 것이 모두 자신이 어떤 사람인지 잘 알고 있기에 할 수 있었으리라 생각한다. 선택에 확신이 있으니, 선택하지 않은 것들을 미련없이 보내줄 수 있었겠다는 생각 또한 들었다.
  - '논문 구현만 해서는 차별성이 없는 것 같아요', '여기서 배우는 수준만으로는 안될 것 같아요' 등의 여러 조언들이 있다. 예상했던 것보다 선택해야할 것들이 많다고 생각이 들면서, 갈팡질팡 혼란을 겪기도 했다. 약간의 불안감을 가져다주기도 했지만, 되려 내가 지금 어떤 선택을 하면 좋을지, 그리고 지금 이 순간에 내가 무엇을 하고 있고, 해야만 하고, 더 할 수 있는지 끊임 없이 고민하게 하는 기폭제가 되었다.
- 마스터 클래스를 듣다보면, '마스터'의 이름에 걸맞게 분야별 최고 수준의 전문가분들이 코멘트를 하신다. 그래서 그런지, 때로는 나 자신이 교육 동안 느끼고 있는 어려움과 그들의 엄청난 전문성 사이에 괴리감이 느껴질 때가 있다. 이것마저 나는 어려워하는 건가-싶을 때도, 이건 아무것도 아니니 잘 해보자-싶을 때도 있다. 이러한 괴리감은 곧잘 사라지는데, 이건 아무래도 마스터분들이 압도적인 빌런들의 느낌보다, 듬직한 어벤저스이기 때문에 그렇지 않나 싶다.
- 잘 해오고 있고, 더 잘 하고 싶고, 더 잘 할 것으로 생각한다. 후회없이 잘 해보고싶다. 시간은 되돌릴 수 없으니, 매순간 현명하게, 최선을 다하자!

#### Ongoing

- 이번주 동안 하고 있는 것은 크게 두 가지이다. Transformer를 밑바닥부터 구현해보는 것과 추천 시스템을 구축하는 것. 우선 Transformer 구현이 집중하고 있고, 꽤나 많은 시행착오를 겪었으며, 꽤나 PyTorch와 친해진 느낌이 들었다.
  - Transformer 구현
    - 직접 모델 아키텍쳐를 참고하며 구현해보면서 느낀 것은, 각 블록 간의 Input과 Output을 파악하는 것이 매우 중요하다는 점이다. 가령, Transformer는 크게 Encoder와 Decoder로 구성되어 있는데, 막연히 '인코더에서 디코더로 넘겨주면서 학습이 진행되는구나!'라고 생각하면 구체적으로 구현할 수 없다. Train Phase와 Inference Phase의 상황을 머릿 속에 그려보면서, 각 블록별로 출력되는 state는 어떻게 생겼는지, 입력될 떄는 어떻게 되어야하는지, 좀더 깊이 들어가자면 Multi-head Attention이 진행되는 과정에서 헤드 수에 따라 reshape되고 난 뒤 Transpose를 취하는 이유가 무엇인지 등. 별개 다 있다. 그때 그때 학습/추론 상황을 상상하면서 구현하니까, 갖게 되는 의문들이 해소되었다. 이러한 문제는 아키텍쳐를 막론하고 발생할 것으로 보이고, 자주 접할 수록 좋은 자산이 될 것 같다
    - 각종 상상을 해보면서 알게된 건, PyTorch의 연산은 브로드캐스팅과 행렬곱이 거의 대부분이라는 점이다. 당연한 이야기일 수도 있지만, 위에서 설명한 Multi-head Attention의 Transpose부분이 도무지 이해가 가지 않아 파고들면서 갑작스럽게 깨닫게된 부분이었다. 행렬곱 연산을 하기 위해서는, 행렬곱 연산을 취할 차원을 행과 열의 차원으로 옮겨주는 작업이 필요고, 이를 위해 활용하는 것이 바로 Transpose의 역할이었다. 때문에 원활하게 Multi-head Attention이 헤드 단위로 연산이 진행될 수 있는 것이다. 또한, 브로드캐스팅도 처음에는 복잡했는데, 결국 연산 가능한 시나리오는 1xN과 NxN의 상황 두 가지뿐이다. 나머지 상황에서는 모조리 에러가 발생한다.
    - 지금까지 인코더와 Positional Encoding 부분을 구현했고, 디코더부분을 구현하는 중이다. 모두 완성해서 원활히 모델이 학습되는 모습을 보면 꽤나 가슴이 벅찰 것 같다.