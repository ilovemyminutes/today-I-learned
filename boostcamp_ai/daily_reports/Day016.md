# Day 16. NLP | 주재걸 마스터

> Intro to NLP, Bag-of-Words

NLP의 주된 분야

- Natural Language Understanding(NLU): 글에 대해 이해
- Natural Language Generation(NLG): 문맥에 맞게 텍스트를 생성
- Language modeling, Machine translation, Question answering, Document classification

분야별 톺아보기 - Natural Language Processing(main conference: ACL, EMNLP, NAACL)

- 딥러닝 기술 발전 선도
- Low-level parsing
  - Tokenization: 문장을 단어 단위로 쪼개는 작업. 쪼개진 문장은 시퀀스로 바라볼 수 있게 됨
  - Stemming: 단어의 어미가 다양하게 변할 수 있는데, 형태 변화를 없애고 단어의 의미(어근)만을 추출하는 작업
- Word, phrase level
  - Named entity recognition(NER): 여러 단어로 이루어진 고유명사를 체크하는 기술
  - Part-of-speech(POS) tagging: 문장 내 각 단어의 품사/성분을 판별하는 기술
  - Noun-phrase chunking
  - Dependency parsing
  - Coreference resolution
- Sentence level
  - Sentiment analysis: 어떤 글이 부정적인지 긍정적인지 판별
  - Machine translation: 기계번역
- Multi-sentence and paragraph level
  - Entailment prediction: 두 문장의 내포된 논리적 의미, 모순을 파악
  - Quetion answering: 독해 기반 질의 응답
  - Dialog systems: 대화를 할 수 있는 기술(챗봇)
  - Summarization: 한 줄 요약

분야별 톺아보기 - Text mining(major conferences: KDD, The WebConf(WWW), WSDM, CIKM, ICWSM)

- 텍스트 데이터로부터 유용한 정보/인사이트 추출
- 도규먼트 클러스터링(토픽 모델링)
- 사회 과학(computational social science)과 밀접한 관련

분야별 톺아보기 - Information retrieval(SIGIR, WSDM, CIKM, RecSys), 정보 검색

- 검색 기술이 고도화되면서, 성숙화 단계에 접어듦 => 기술 발전이 이제는 느린 편
- 추천 시스템(recommendation system)은 여전히 활발히 연구 진행!
  - 개인화된 광고, 상품 추천
  - 자동화된 새로운 검색 시스템

NLP 기술 동향

1. Word embedding: 텍스트 데이터를 단어로 구성된 시퀀스로 바라보고, 각각의 단어는 벡터로 표현
2. 시퀀스 데이터를 잘 다룰 수 있는 RNN 계열 모델(LSTM, GRU 등)을 NLP 문제의 main architecture로 활용
3. self-attention 모듈을 겸비한 Transformer 모델을 통해 RNN 모델 대체
4. Transformer 모델 기반의 고도화된 NLP 모델로 기계 번역 성능 향상
   - 영상 처리, 시계열 예측, 신약 개발 등 다양한 분야에도 성능 향상
5. Transformer 출범 이후, self-attention 구조를 스태킹한 모델을 구축하고 self-supervised learning을 통해 범용적 모델을 학습, pretrained 모델을 특정 task에 맞게 transfer learning하는 방식으로 변화
   - 특정 task만을 위해 설계된 모델보다 월등한 성능
   - BERT, GPT-3

- 한정된 GPU 리소스로 인해 난항을 겪는 중
  - 전기세만 수십 억원
  - 구글 등 IT 기업과 대용량 데이터를 보유한 소수 기관이 협력하여 학습을 진행한다고 함

Bag-of-Words

- Bag-of-Words representation 과정

  1. 단어 사전 구축. 단어 사전에는 단어가 유니크하게 담겨져 있어야 함

     <img src="https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/example1.png?raw=true" />

  2. 단어 사전의 각 단어를 원핫 벡터로 표현

     <img src="https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/example2.png?raw=true" />

     - 각 단어 간 유클리드 거리는 ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Csqrt+%7B2%7D), 코사인 유사도는 0 => 단어의 의미와 관계 없음

  3. 문장/도큐먼트를 원핫벡터의 합으로 표현(bag-of-words vector)

     <img src="https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/example3.png?raw=true" />

- NaiveBayes Classifier for Document Classification

  - MAP(maximum a posterior): 가장 있음직한 클래스를 판별하는 방법

    ![MAP_class](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/MAP_class.jpg?raw=true)

  - 유도된 식 P(d|c)P(c)를 좀더 살펴보자.

    ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28d%7Cc%29P%28c%29+%3D+P%28w_%7B1%7D%2C+...%2C+w_%7Bn%7D%7Cc%29P%28c%29)

    독립을 가정하면 다음과 같이 확률의 곱으로 표현할 수 있음. 계산이 훨씬 수월해지는 셈! 실제로는 해당 값에 로그를 취해 덧셈 연산을 함

    ![](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+P%28c%29%5CPi_%7Bw_%7Bi%7D+%5Cin+W%7D+P%28w_%7Bi%7D%7Cc%29)

    바르게 알고 갑시다. MLE와 MAP

    ![MLE vs MAP](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/MLE%20vs%20MAP.jpg?raw=true)

> Word Embedding

Word Embedding?

- 단어를 벡터로 표현하는 기술
- 기존의 BoW 방법으로는 단어 간 관계를 담아낼 수 없었는데, 임베딩 방법을 통해 단어 간 관계를 파악할 수 있음
  *유사한 단어 간 거리는 짧고 관련 없는 단어 간 거리는 멀다*

Word2Vec

- 워드 임베딩 기법의 일종
- Continuous Bag of Words(CBOW), Skip-gram의 2가지 방법이 있음

Word2Vec의 학습 원리

![word2vec_work](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/word2vec_work.jpg?raw=true)

- input 레이어, hidden layer(=embedding layer), output 레이어로 구성
- input layer와 output layer의 차원은 동일하게 설정, hidden layer의 노드 수는 하이퍼파라미터로 사용자가 조정
- CBOW: 중심 단어를 입력하고, 주변 단어를 레이블로써 맞추도록 설계
- Skip-gram: 주변 단어를 입력하고, 중심 단어를 레이블로써 맞추도록 설계 <- 더 자주 사용한다!

> Glove

Glove?

- 워드 임베딩 방법의 일종
- Word2Vec 방법보다 효율적인 임베딩 방법

![glove_formula](https://github.com/iloveslowfood/iloveTIL/blob/main/boostcamp_ai/etc/images/week04/glove_formula.jpg?raw=true)

- (1)윈도우 사이즈 내 두 단어가 동시에 등장한 횟수의 로그를 취한 값과 (2)입력 단어의 임베딩 벡터와 출력 단어의 임베딩 벡터의 행렬곱 값이 서로 비슷해지도록 하는 것을 목표로 학습
- 중심 단어와 주변 단어를 활용한 학습 과정에서 중복 학습이 발생할 가능성이 있는 Word2Vec 방법과 달리 중복되는 학습이 없어 학습 속도가 Word2Vec보다 빠르며, 데이터가 적더라도 비교적 좋은 성능을 보임

> Attitude & Tips

- [?] Word2Vec: 특정 단어만을 공개, 나머지 단어를 숨긴 채 나머지 단어를 예측하도록 학습
- [?] Word2Vec: 여러 단어에 대해 유사한 단어는 벡터의 합으로 찾아내는 것?
- [?] Word2Vec: 어떤 레이블을 사용하는지 이해가 잘 안가

- Laplace Smoothing